<!DOCTYPE html>
<!-- saved from url=(0027)https://www.gwern.net/Faces -->
<html lang="en-us"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

    <!-- CSS+JS inline for speed -->
    <style id="inlined-styles-colors">
:root {
    /*  General.
        */
    --GW-body-background-color: #fff;
    --GW-body-text-color: #000;
    --GW-body-link-color: #272727;
    --GW-body-link-hover-color: #888;

    /*  Selection.
        */
    --GW-text-selection-background-color: #333;
    --GW-text-selection-color: #fff;

    /*  Link underlining.
        */
    --GW-link-underline-background-color: var(--GW-body-background-color);
    --GW-link-underline-gradient-line-color: #333;
    --GW-link-underline-gradient-line-color-hover: #999;

    /*  Blockquotes.
        */
    --GW-blockquote-border-color-level-one: #ccc;
    --GW-blockquote-border-color-level-two: #c4c4c4;
    --GW-blockquote-border-color-level-three: #b3b3b3;
    --GW-blockquote-border-color-level-four: #a6a6a6;
    --GW-blockquote-background-color-level-one: #f8f8f8;
    --GW-blockquote-background-color-level-two: #e6e6e6;
    --GW-blockquote-background-color-level-three: #d8d8d8;

    /*  Abstracts.
        */
    --GW-abstract-border-color: #bbb;
    --GW-abstract-background-color: var(--GW-blockquote-background-color-level-one);

    /*  Sidebar.
        */
    --GW-sidebar-horizontal-rule-color: #aaa;
    --GW-sidebar-mobile-link-border-color: #000;

    /*  Table of contents.
        */
    --GW-TOC-border-color: #ccc;
    --GW-TOC-background-color: #f8f8f8;
    --GW-TOC-link-hover-background-color: #ececec;
    --GW-TOC-link-hover-color: #000;
    --GW-TOC-link-hover-indicator-bar-color: #ccc;
    --GW-TOC-number-color: #909090;
    --GW-TOC-number-hover-color: #313131;

    /*  Collapse blocks.
        */
	--GW-collapse-summary-blockquote-hover-color: #eee;
	--GW-collapse-disclosure-button-color: #eee;
	--GW-collapse-disclosure-button-hover-color: #ddd;
	--GW-collapse-disclosure-button-top-part-text-color: #bbb;
	--GW-collapse-disclosure-button-bottom-part-text-color: #aaa;
	--GW-collapse-disclosure-button-text-hover-color: #fff;

    /*  Headings.
        */
    --GW-H1-box-shadow-background-color: var(--GW-body-background-color);
    --GW-H1-box-shadow-line-color: #888;
    --GW-H2-bottom-border-color: #888;

    /*  Comments.
        */
    --GW-comment-section-top-border-color: #999;

    /*  Lists.
        */
    --GW-bulleted-list-marker-color: #808080;

    /*  Figures.
        */
    --GW-figure-outline-color: #888;
    --GW-figure-caption-outline-color: #888;

    /*  Epigraphs.
        */
    --GW-epigraph-quotation-mark-color: #808080;

    /*  Footnotes.
        */
    --GW-footnote-border-color: #aaa;
    --GW-footnote-highlighted-border-color: #aaa;
    --GW-footnotes-section-top-rule-color: #ccc;
    --GW-footnote-backlink-border-color: #000;
    --GW-footnote-backlink-border-hover-color: #999;
    --GW-footnote-backlink-after-bordered-block-background-color: var(--GW-body-background-color);

    /*  Footnote references.
        */
    --GW-footnote-ref-highlight-box-shadow-color: #aaa;
    --GW-footnote-ref-highlight-background-color: rgba(255,255,255,0.75);

    /*  Sidenotes.
        */
    --GW-sidenote-highlight-background-color: var(--GW-body-background-color);
    --GW-sidenote-highlight-box-shadow-color: #aaa;
    --GW-sidenote-border-color: #aaa;
    --GW-sidenote-scrollbar-track-color: var(--GW-body-background-color);
    --GW-sidenote-scrollbar-thumb-color: #aaa;
    --GW-sidenote-scrollbar-thumb-hover-color: #999;
    --GW-sidenote-self-link-border-color: #aaa;

    /*  Tables.
        */
    --GW-table-border-color: #000;
    --GW-table-full-width-background-color: var(--GW-body-background-color);
    --GW-table-caption-border-color: #000;
    --GW-table-row-horizontal-border-color: #000;
    --GW-table-cell-vertical-border-color: var(--GW-body-background-color);
    --GW-table-scrollbar-track-color: var(--GW-body-background-color);
    --GW-table-scrollbar-thumb-color: #aaa;
    --GW-table-scrollbar-thumb-hover-color: #999;
    --GW-table-scrollbar-border-color: #000;
    --GW-table-column-heading-hover-background-color: #e2f0f2;
    --GW-table-sorted-column-heading-background-color: #8bd0ed;
    --GW-table-sorted-column-heading-text-color: #fff;
    --GW-table-sorted-column-heading-text-shadow-color: #000;
    --GW-table-zebra-stripe-alternate-row-background-color: #f6f6f6;
    --GW-table-row-hover-outline-color: #000;

    /*  Code blocks.
        */
    --GW-code-element-border-color: #c8c8c8;
    --GW-code-element-background-color: #fafafa;
    --GW-pre-element-border-color: #c8c8c8;
    --GW-pre-element-background-color: #fafafa;
    --GW-pre-element-scrollbar-track-color: #fafafa;
    --GW-pre-element-scrollbar-thumb-color: #ccc;
    --GW-pre-element-scrollbar-thumb-hover-color: #999;

    /*  Syntax highlight theme.
        */
    --GW-syntax-highlight-color-normal: #1f1c1b;
    --GW-syntax-highlight-color-attribute: #002561;
    --GW-syntax-highlight-color-data-type: inherit;
    --GW-syntax-highlight-color-variable: #666666;
    --GW-syntax-highlight-color-other: inherit;
    --GW-syntax-highlight-color-preprocessor: inherit;
    --GW-syntax-highlight-color-extension: #888888;
    --GW-syntax-highlight-color-comment: #888888;
    --GW-syntax-highlight-color-control-flow: #003900;
    --GW-syntax-highlight-color-keyword: #002561;
    --GW-syntax-highlight-color-operator: #002561;
    --GW-syntax-highlight-color-special-char: #607880;
    --GW-syntax-highlight-color-built-in: #002561;
    --GW-syntax-highlight-color-function: #002561;
    --GW-syntax-highlight-color-constant: inherit;
    --GW-syntax-highlight-color-base-n: inherit;
    --GW-syntax-highlight-color-dec-val: inherit;
    --GW-syntax-highlight-color-float: inherit;
    --GW-syntax-highlight-color-information: inherit;
    --GW-syntax-highlight-color-char: inherit;
    --GW-syntax-highlight-color-string: inherit;
    --GW-syntax-highlight-color-verbatim-string: inherit;
    --GW-syntax-highlight-color-alert: #bf0303;
    --GW-syntax-highlight-color-error: #ff0000;
    --GW-syntax-highlight-color-import: #777777;
    --GW-syntax-highlight-color-special-string: #666666;

    /*  Math.
        */
    --GW-math-block-background-color: #f6f6f6;
    --GW-math-block-scrollbar-border-color: #ccc;
    --GW-math-block-scrollbar-track-color: var(--GW-body-background-color);
    --GW-math-block-scrollbar-thumb-color: #ccc;
    --GW-math-block-scrollbar-thumb-hover-color: #999;

    /*  Drop caps.
        */
    --GW-drop-caps-yinit-color: #0d0d0d;
    --GW-drop-caps-yinit-text-shadow-color: #777;
    --GW-drop-caps-de-zs-color: #1b1b1b;
    --GW-drop-caps-cheshire-color: #191919;
    --GW-drop-caps-kanzlei-color: #191919;

    /*  Admonitions.
        */
    --GW-admonition-default-left-border-color: #909090;
    --GW-admonition-default-background-color: #d8d8d8;
    --GW-admonition-tip-left-border-color: #d8d8d8;
    --GW-admonition-tip-background-color: #f0f0f0;
    --GW-admonition-warning-left-border-color: #5a5a5a;
    --GW-admonition-warning-background-color: #9a9a9a;
    --GW-admonition-warning-text-color: #fff;
    --GW-admonition-warning-title-background-color: #5a5a5a;
    --GW-admonition-warning-icon-color: #fff;
    --GW-admonition-error-left-border-color: #2d2d2d;
    --GW-admonition-error-background-color: #5a5a5a;
    --GW-admonition-error-text-color: #fff;
    --GW-admonition-error-title-background-color: #2d2d2d;
    --GW-admonition-error-icon-color: #fff;
    --GW-admonition-reversed-link-color: #ddd;
    --GW-admonition-reversed-link-color-hover: #ccc;
    --GW-admonition-reversed-link-underline-gradient-line-color: #ccc;
    --GW-admonition-reversed-link-underline-gradient-line-color-hover: #bbb;
    --GW-admonition-reversed-footnote-ref-highlight-background-color: rgba(90,90,90,0.75);

    /*  Sequential nav UI.
        */
    --GW-sequential-nav-link-background-color: var(--GW-body-background-color);

    /*  Links.
        */
    --GW-definition-annotation-text-color: #444;

    /*  Page-specific styles.
        */
    --GW-index-page-bottom-ornament-line-color: #808080;

	/*	Pop-frames (popups or popins).
		*/
    --GW-raw-code-popframe-line-highlight-background-color: #ffd;
    --GW-raw-code-popframe-line-hightlight-border-color: #ddd;
    --GW-raw-code-popframe-line-number-color: #aaa;
    --GW-raw-code-popframe-line-number-divider-color: #ccc;

    /*  Popups.
        */
    --GW-popups-body-background-color: var(--GW-body-background-color);
    --GW-popups-popup-background-color: var(--GW-body-background-color);

    --GW-popups-popup-border-color: #ccc;
    --GW-popups-box-shadow-color: #ccc;
    --GW-popups-popup-border-focused-color: #aaa;
    --GW-popups-box-shadow-focused-color: #aaa;

    --GW-popups-popup-title-bar-background-color: #f6f6f6;
    --GW-popups-popup-title-bar-button-color: #bbb;
    --GW-popups-popup-title-bar-button-color-hover: #000;
	--GW-popups-popup-title-color: #aaa;
    --GW-popups-popup-title-link-hover-color: var(--GW-body-link-hover-color);
    --GW-popups-popup-title-bar-background-focused-color: #e6e6e6;
    --GW-popups-popup-title-bar-button-focused-color: #777;
    --GW-popups-popup-title-bar-button-focused-color-hover: #000;
    --GW-popups-popup-title-bar-submenu-box-shadow-color: #ddd;
	--GW-popups-popup-title-focused-color: #000;
    --GW-popups-popup-title-link-hover-focused-color: var(--GW-body-link-hover-color);

    --GW-popups-popup-scrollbar-track-color: var(--GW-body-background-color);

    --GW-popups-popup-scrollbar-thumb-color: #ddd;
    --GW-popups-popup-scrollbar-thumb-hover-color: #bbb;
    --GW-popups-popup-scrollbar-thumb-focused-color: #ccc;
    --GW-popups-popup-scrollbar-thumb-hover-focused-color: #999;

    --GW-popups-popup-options-dialog-backdrop-background-color: rgba(255,255,255,0.95);
    --GW-popups-popup-options-dialog-background-color: var(--GW-body-background-color);
    --GW-popups-popup-options-dialog-border-color: #aaa;
    --GW-popups-popup-options-dialog-box-shadow-color: #444;
    --GW-popups-popup-options-dialog-horizontal-rule-color: #ccc;
    --GW-popups-popup-options-dialog-button-background-color: var(--GW-body-background-color);
    --GW-popups-popup-options-dialog-button-text-color: #000;
    --GW-popups-popup-options-dialog-button-border-color: #000;
    --GW-popups-popup-options-dialog-button-hover-box-shadow-color: #000;
    --GW-popups-popup-options-dialog-option-button-explanation-text-color: #777;
    --GW-popups-popup-options-dialog-option-button-hover-text-color: #777;
    --GW-popups-popup-options-dialog-radio-button-border-color: #000;
    --GW-popups-show-popup-options-dialog-button-color: #999;

    /*  Popins.
        */
    --GW-popins-body-background-color: var(--GW-body-background-color);
    --GW-popins-popin-background-color: var(--GW-body-background-color);
    --GW-popins-popin-border-color: #aaa;
    --GW-popins-box-shadow-color: #aaa;
    --GW-popins-popin-title-bar-background-color: #fff;
    --GW-popins-popin-title-bar-button-color: #777;
    --GW-popins-popin-scrollbar-thumb-color: #ccc;
    --GW-popins-popin-scrollbar-track-color: var(--GW-body-background-color);
    --GW-popins-popin-scrollbar-thumb-hover-color: #999;

    /*  Image focus.
        */
    --GW-image-focus-image-hover-drop-shadow-color: #777;

    /*  Mode selector (light/dark mode).
        */
    --GW-mode-selector-background-color: var(--GW-body-background-color);
    --GW-mode-selector-border-hover-color: #aaa;
    --GW-mode-selector-button-text-color: #777;
    --GW-mode-selector-button-hover-text-color: #000;

	/*	“Back to top” link.
		*/
	--GW-back-to-top-link-color: #ccc;
	--GW-back-to-top-link-hover-color: #999;
}
</style>
<style id="inlined-styles">
/*********/
/* FONTS */
/*********/
/* fonts are now loaded from /static/css/fonts.css */

/**********/
/* COLORS */
/**********/
/* colors are now in /static/css/colors.css or dark-mode.css */

/*************/
/* VARIABLES */
/*************/

:root {
    --GW-serif-font-stack: "Source Serif Pro",  "Apple Garamond", "Baskerville", "Libre Baskerville", "Times New Roman", "Droid Serif", "Times", serif;
    --GW-sans-serif-font-stack: "Lucida Sans Unicode", "Source Sans Pro", Helvetica, "Trebuchet MS", sans-serif;
    --GW-monospaced-font-stack: "IBM Plex Mono", "Liberation Mono", Consolas, Courier, monospace;

    --GW-body-text-font-size: 1rem;
}

/***********/
/* GENERAL */
/***********/

html {
    padding: 0;
    margin: 0;
    background-color: var(--GW-body-background-color);
    color: var(--GW-body-text-color);
    font-weight: 400;
    font-family: var(--GW-serif-font-stack);
}

body {
    max-width: 1120px;

    font-size: var(--GW-body-text-font-size);
}

@media only screen and (max-width: 649px) {
    html {
        font-size: 18px;
    }
}

@media only screen and (min-width: 650px) {
    body {
        padding: 0 15px 0 5px;
        margin: 0 auto;
    }
    @media only screen and (min-width: 1185px) {
        body {
            padding: 0 60px 0 5px;
        }
    }
    main {
        min-height: 100vh;
        display: flex;
        flex-flow: column;
    }
    /*  Breakpoint for sidenotes.
        */
    @media only screen and (min-width: 1761px) {
        main {
            position: relative;
            right: 40px;
        }
    }
    article {
        flex: 1 1 auto;
    }
    #sidebar {
        position: absolute;
    }
    header,
    article {
        margin-left: 155px;
    }
    @media only screen and (max-width: 1200px) {
        header,
        article {
            margin-left: 145px;
        }
    }
    @media only screen and (max-width: 1120px) {
        header,
        article {
            margin-left: 140px;
        }
    }
    @media only screen and (max-width: 1040px) {
        header,
        article {
            margin-left: 135px;
        }
    }
    @media only screen and (max-width: 960px) {
        header,
        article {
            margin-left: 130px;
        }
    }
}

@media only screen and (max-width: 649px) {
    body {
        margin: 0 20px;
    }
}

svg {
    fill: currentColor;
}

/***********/
/* SIDEBAR */
/***********/

#sidebar a {
    display: block;
}

@media only screen and (min-width: 650px) {
    #sidebar {
        font-variant: small-caps;
        padding: 0 40px 0 10px;
        width: 100px;
    }
    #sidebar a#logo {
        margin: 1em 0 2em 0;
    }
    #sidebar a#logo svg {
        width: 64px;
    }
    #sidebar a.new,
    #sidebar a.patreon {
        border-top: 1px dotted var(--GW-sidebar-horizontal-rule-color);
    }
    #sidebar a.new {
        padding-top: 0.45em;
        margin-top: 0.55em;
    }
    #sidebar a.patreon {
        padding-top: 0.6em;
        margin-top: 0.55em;
        font-size: 0.9em;
        white-space: nowrap;
        line-height: 1.35;
    }
}

@media only screen and (max-width: 649px) {
    #sidebar {
        justify-content: center;
        margin: 1.4em 0 0.5em 0;
    }
    #sidebar a {
        border: 1px dotted var(--GW-sidebar-mobile-link-border-color);
        padding: 4px 10px 3px 10px;
        text-align: center;
        margin: 1px;
    }
    #sidebar a#logo {
        padding: 8px 5px 3px 5px;
    }
    #sidebar,
    #sidebar-links {
        display: flex;
    }
    #sidebar-links {
        flex-flow: row wrap;
        flex: 1 1 100%;
        margin: 0.5em 0 0 0;
    }
    #sidebar a.site,
    #sidebar a.links {
        flex: 1 1 40%;
    }
    #sidebar a.new,
    #sidebar a.mail {
        flex: 1 1 auto;
    }
    #sidebar a.patreon {
        display: none;
    }
    #sidebar #logo {
        margin: calc(0.5em + 1px) 1px 1px 0;
    }
    #sidebar #logo svg {
        width: 2.5rem;
    }
}

/***************/
/* PAGE HEADER */
/***************/

header {
    overflow: auto;
}

header h1 {
    margin: 0.75em 0;
    text-align: center;
    text-transform: none;
    font-variant: small-caps;
    font-size: 2.5em;
    line-height: 1.15;
    font-weight: 600;
    letter-spacing: -1px;
}

@media only screen and (max-width: 649px) {
    header h1 {
        font-size: 2em;
    }
}

/***********************/
/* PAGE METADATA BLOCK */
/***********************/

@media only screen and (max-width: 649px) {
    #page-metadata {
        text-align: center;
    }
}

#page-metadata hr {
    display: none;
}

#page-metadata {
    margin: 0 0 2rem 0;
    overflow: auto;
    font-size: 0.95em;
    line-height: 1.5;
}
#page-metadata p {
    text-align: left;
}

/* Override normal handling of italics: before, we wrapped descriptions in `<em>`; however, they compile to HTML which can contain italics of their own (eg book titles). This causes HTML Tidy to warn about nested italics (which is valid HTML but *usually* means you've made an error - this often triggers when I mess up bolding, for example, like on /GPT-3 ). So we removed the wrapper in favor of the above `font-style: italic`. However, now the titles don't unitalicize automatically like they would for nested italics! So we force italicized titles back to normal Roman font to match the expected typographic convention. */
#page-metadata #page-description {
    display: block;
    margin: 0 auto 0.75em auto;
    font-style: italic;
    text-align: justify;
}
#page-metadata #page-description em {
    font-style: normal;
}

#page-metadata #page-description ~ * {
    font-size: 0.9em;
}

#page-metadata #page-description + br {
    display: none;
}

#page-metadata-block span:nth-of-type(n+3) {
    white-space: nowrap;
}

/* interpunct separators for the metadata fields; looks nicer than semicolons or slashes */
#page-metadata-block > span:not(:last-child)::after {
    content: "\00B7"; /* interpunct MIDDLE DOT (U+00B7) '·' */
    display: inline-block;
    margin: 0 0.35em 0 0.65em;
}

#page-tags,
#page-metadata-block {
    hyphens: none;
}

/*=--------=*/
/*= /index =*/
/*=--------=*/

/* on the index page, we hide article-related formatting, begging, comments, to make it more compact & classy */
body.index #sidebar a.patreon,
body.tags #sidebar a.patreon,
body.index #TOC,
body.index #page-metadata,
body.index #comments,
body.index #disqus_thread,
body.index header {
    display: none;
}

/*  No index self-link. */
body.index #sidebar a#logo {
    pointer-events: none;
}

/*  Make the sections reflow; better than fixed-width tables. */
body.index article section {
    display: inline-block;
    vertical-align: top;
}

/*  Abstract is not a real abstract, just an introduction, no need for the abstract formatting.
    */
body.index .abstract {
    padding: 0;
    border: none;
    margin: 0;
    box-shadow: none;
}

@media only screen and (min-width: 1200px) {
    /*  Leave enough horizontal room to show multiple sections simultaneously, table-like */
    body.index #markdownBody {
        width: 1350px;
        max-width: calc(50vw + 420px);
    }
    body.index p,
    body.index hr {
        max-width: 970px;
    }
}

/*  Add some vertical padding for the introduction.
    */
body.index #markdownBody {
    padding-top: 15px;
}

/*  Headings on the home page.
    */
body.index #markdownBody h1 {
    margin: 1.125em 0 0 0;
}

/*  Lists on the home page.
    */
body.index #markdownBody li,
body.index #markdownBody li p {
    text-align: left;
}
body.index #markdownBody ul,
body.tags #markdownBody ul {
    margin: 1em 3em 0 0;
    padding: 0 0 0 1.375em;
}
body.index #markdownBody li > ul,
body.tags #markdownBody li > ul {
    margin: 0.25em 0 0.25em 0;
}
body.index #markdownBody li,
body.tags #markdownBody li {
    margin-top: 0;
}
body.index #markdownBody ul > li:nth-of-type(n+2) {
    margin: 0;
}


@media only screen and (max-width: 649px) {
    body.index #markdownBody section {
        max-width: 100%;
    }
    body.index #markdownBody ul,
    body.tags #markdownBody ul {
        overflow-wrap: break-word;
        margin-right: 0;
    }
    body.index #markdownBody li,
    body.tags #markdownBody li {
        padding: 1px 0;
        margin: 2px 0 0 0;
    }
    body.index #markdownBody ul > li::before,
    body.tags #markdownBody ul > li::before {
        top: 0.25em;
    }
}

/*********************/
/* TABLE OF CONTENTS */
/*********************/

#TOC {
    border: 1px solid var(--GW-TOC-border-color);
    background-color: var(--GW-TOC-background-color);
    font-family: var(--GW-sans-serif-font-stack);
    margin: 0 2rem 1.5rem 0;
    line-height: 1.25;
    padding: 10px 10px 2px 14px;
    position: relative;
    z-index: 11;
}

#TOC:empty {
    display: none;
}

/*  On mobile, we want the TOC to be inline, but for desktop, we want it side by side with the content.
    */
@media only screen and (max-width: 1280px) {
    #TOC {
        font-size: 0.95rem;
    }
}

@media only screen and (max-width: 1200px) {
    #TOC {
        font-size: 0.90rem;
    }
}

@media only screen and (max-width: 1120px) {
    #TOC {
        font-size: 0.85rem;
        margin: 0 1.5rem 1.25rem 0;
    }
}

@media only screen and (max-width: 1040px) {
    #TOC {
        font-size: 0.80rem;
        margin: 0 1.25rem 1rem 0;
    }
}

@media only screen and (max-width: 960px) {
    #TOC {
        margin: 0 1rem 1rem 0;
    }
}

@media only screen and (min-width: 900px) {
    #TOC {
        float: left;
        max-width: 285px;
    }
}

@media only screen and (max-width: 900px) {
    #TOC {
        float: none;
        margin: 2em auto;
        font-size: 1rem;
    }
    #TOC > ul > li > ul {
        column-count: 2;
    }
}

@media only screen and (max-width: 649px) {
    #TOC a {
        display: inline-block;
    }
    #TOC > ul > li > ul {
        column-count: 1;
    }
    #TOC li li a {
        padding: 0 0 1px 0;
    }
    #TOC li li li a {
        padding: 0 0 2px 0;
    }
    #TOC li li li li a {
        padding: 0 0 3px 0;
    }
    #TOC li li li li a {
        padding: 0 0 4px 0;
    }
}

/*=-----------------=*/
/*= TOC list layout =*/
/*=-----------------=*/

#TOC ul {
    list-style-type: none;
    padding-left: 0em;
    margin-bottom: 0;
    margin-top: 4px;
    padding-left: 1.4em;
    text-indent: 0;
    padding: 0;
    overflow: unset;
}

#TOC ul ul {
    list-style-type: none;
    padding-left: 0.7em;
    margin-top: 2px;
}

#TOC li {
    font-weight: bold;
    margin: 5px 0 10px 0;
    padding-left: 1.125em;
    position: relative;
    overflow-wrap: break-word;
}

#TOC li li {
    margin-bottom: 0;
    font-weight: normal;
    font-size: 0.9em;
}

#TOC p {
    margin-top: 9px;
    margin-bottom: 3px;
}

/*=------------------=*/
/*= TOC link styling =*/
/*=------------------=*/

#TOC a {
    border: 0;
    display: block;
    position: relative;
    padding: 0 0.25rem 0 0;
}

#TOC a:hover {
    background-color: var(--GW-TOC-link-hover-background-color);
    color: var(--GW-TOC-link-hover-color);
}
#TOC a:hover::after {
    content: "";
    display: inline-block;
    position: absolute;
    left: 100%;
    top: 0;
    background-color: var(--GW-TOC-link-hover-indicator-bar-color);
    width: 0.25em;
    height: 100%;
}

/*=--------------------------=*/
/*= Inline code in TOC links =*/
/*=--------------------------=*/

#TOC code {
    font-family: inherit;
    font-size: inherit;
    border: none;
    padding: 0;
    background-color: inherit;
}

/*=-------------------------------=*/
/*= Wikipedia-style TOC numbering =*/
/*=-------------------------------=*/

#TOC > ul {
    counter-reset: htoc_1;
}
#TOC > ul > li::before {
    counter-increment: htoc_1;
    content: counter(htoc_1) "\2006  ";
}
#TOC > ul ul {
    counter-reset: htoc_2;
}
#TOC > ul ul li::before {
    counter-increment: htoc_2;
    content: counter(htoc_1) "." counter(htoc_2) "\2006  ";
}
#TOC > ul ul ul {
    counter-reset: htoc_3;
}
#TOC > ul ul ul li::before {
    counter-increment: htoc_3;
    content: counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "\2006  ";
}
#TOC > ul ul ul ul {
    counter-reset: htoc_4;
}
#TOC > ul ul ul ul li::before {
    counter-increment: htoc_4;
    content: counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "." counter(htoc_4) "\2006  ";
}
#TOC > ul ul ul ul ul {
    counter-reset: htoc_5;
}
#TOC > ul ul ul ul ul li::before {
    counter-increment: htoc_5;
    content: counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "." counter(htoc_4) "." counter(htoc_5) "\2006  ";
}
#TOC > ul ul ul ul ul ul {
    counter-reset: htoc_6;
}
#TOC > ul ul ul ul ul ul li::before {
    counter-increment: htoc_6;
    content: counter(htoc_1) "." counter(htoc_2) "." counter(htoc_3) "." counter(htoc_4) "." counter(htoc_5) "." counter(htoc_6) "\2006  ";
}
#TOC ul li::before {
    position: absolute;
    right: calc(100% - 1em);
    left: unset;
    width: 111px;
    text-align: right;
    font-weight: normal;
    pointer-events: none;
    color: var(--GW-TOC-number-color);
}
#TOC ul li:hover::before {
    color: var(--GW-TOC-number-hover-color);
}

/* The table of contents is a *table*, so align */
#TOC {
    font-variant-numeric: tabular-nums;
}

/****************/
/* MAIN CONTENT */
/****************/

.markdownBody {
    /*  Try to avoid scrollbars on paragraphs: prevents long unbroken
        un-hyphenatable lines from causing H-scrolling
        */
    overflow-wrap: break-word;
}

/*  Breakpoint for sidenotes.
    */
@media only screen and (min-width: 1761px) {
    #markdownBody {
        position: relative;
    }
}

@media only screen and (min-width: 650px) {
    @media only screen and (max-width: 1000px) {
    /*  Replaces A/B-test-set average value; the wider the screen, the more
        line-height is necessary, and no one size suits all, so set 3 brackets
        of increasing height.
        */
        .markdownBody {
            line-height: 1.50;
        }
    }
    @media only screen and (min-width: 1001px) and (max-width: 1200px) {
        .markdownBody {
            line-height: 1.55;
        }
    }
    @media only screen and (min-width: 1201px) {
        .markdownBody {
            line-height: 1.60;
        }
    }
}

/*  Use justified text (words get broken/hyphenated as necessary to avoid a
    'ragged margin'), undoing the default of 'flush left, ragged' right (see
    https://en.wikipedia.org/wiki/Typographic_alignment#Flush_left )

    WARNING: 'auto' hyphenation does not work in Chrome desktop due to long-standing inability/refusal:
    https://bugs.chromium.org/p/chromium/issues/detail?id=47083 https://bugs.chromium.org/p/chromium/issues/detail?id=605840 https://bugs.chromium.org/p/chromium/issues/detail?id=652964
    /images/design/2019-02-19-meme-distractedboyfriend-chromehyphenation.jpg

    To work around Chrome failure, we use Edward Kmett's Haskell-ification of the TeX algorithm & dictionaries ( https://hackage.haskell.org/package/hyphenation ) to run a Hakyll compilation pass to insert soft hyphens. There is one bad side-effect: soft hyphens are often preserved in copy-paste, cluttering text, so a copy-paste JS listener in rewrite.js deletes all fancy hyphens from copy-pastes.
    */
.markdownBody p,
.markdownBody li {
    text-align: justify;
    -webkit-hyphens: auto;
    -ms-hyphens: auto;
    hyphens: auto;
}

/*  Cute 'old-style' numerals, look a little nicer inline in text, and available natively in the SS fonts:
    https://practicaltypography.com/alternate-figures.html#oldstyle-figures
    */
html {
    font-variant-numeric: oldstyle-nums;
}

.abstract > blockquote {
    margin: 0 0 1.5em 0;
    background-color: var(--GW-abstract-background-color);
    border-color: var(--GW-abstract-border-color);
    padding: 0.9rem 1.25rem 0.95rem 1.25rem;
    clear: none;
}

/* auto-smallcaps the first line of the introduction (= the first `<p>` after an 'abstract' div); this avoids the need to manually specify what to smallcaps, and it auto-adjusts to screen/line-width, which is nicer. */
.abstract + p::first-line,
#markdownBody > p:first-child::first-line,
.markdownBody #page-metadata + p::first-line {
    font-feature-settings: 'smcp';
}

h1 {
    margin: 0.6em 0 0.5em -0.75rem;
    font-weight: bold;
    position: relative;
}

@media only screen and (max-width: 649px) {
    h1 {
        margin: 1.25em 0 0.5em 0;
        padding-bottom: 2px;
    }
}

/*=----------------=*/
/*= Heading levels =*/
/*=----------------=*/
h1 {
    font-feature-settings: 'smcp';
    font-size: 1.75em;
    line-height: 1.25;
    letter-spacing: -0.75px;
}

/*************/
/* SIDENOTES */
/*************/

/*  Hide sidenote columns on narrow viewports.
    */
@media only screen and (max-width: 1760px) {
    #sidenote-column-left,
    #sidenote-column-right {
        display: none;
    }
}

/*=--------------=*/
/*= Margin notes =*/
/*=--------------=*/

/*  On wide viewports, hide margin note until it is styled as a sidenote (i.e,
    until it gets the `.sidenote` class applied to it).
    */
@media only screen and (min-width: 1761px) {
    .marginnote {
        display: none;
    }
}

/*********/
/* LINKS */
/*********/

a {
    color: var(--GW-body-link-color); /* off-black */
    text-decoration: none;
}

/*=------------------------=*/
/*= Fancy link underlining =*/
/*=------------------------=*/

/*  Tufte CSS for underlining (https://github.com/edwardtufte/tufte-css)
    The advantage of all this CSS linenoise compared to the previous 'text-decoration: none; border-bottom: 1px Solid grey;' solution from http://devhints.wordpress.com/2006/10/24/css-links-with-different-color-underlines/
    is the 'skip-ink': when dealing with characters with 'descenders', like 'y', 'g', 'p', 'q' etc, with regular underlining the bottom stroke overlaps with the line and it's harder to read;
    by adding this text-shadow stuff and backgrounding, a little break is made in the underline to add contrast and keep the descender fully visible and outlined. (The downside is that the overlapping text-shadows can also slightly damage the rendering of slashes & parentheses, which come too close and get partially erased.)

    Unfortunately, we don't want to add underlines to links in the TOC because it clutters it (all the lines are links and are in small font), so we have to avoid styling the TOC, which is difficult.
    I got around this by adding in the Hakyll template an additional <div> just for the body of the Markdown content, excluding the TOC, and changing the Tufte CSS to target *that* instead.

May be able at some point to simplify this using regular link underlining, since CSS4's `text-decoration-skip-ink` by default avoids overlapping with text descenders (but as of Oct 2019, no Edge/IE or Safari support, and only the latest Firefox 70 supports it; maybe in a few years...): https://developer.mozilla.org/en-US/docs/Web/CSS/text-decoration-skip-ink#Browser_Compatibility https://www.caniuse.com/#feat=mdn-css_properties_text-decoration-skip-ink (Right now, Firefox skip-ink looks quite bad: it doesn't skip enough ink, defeating the point, and also positions the underline badly!)
*/

.markdownBody a:link,
.markdownBody span.defnMetadata,
div#footer a {
    text-decoration: none;
    background-image: linear-gradient(var(--GW-link-underline-gradient-line-color), var(--GW-link-underline-gradient-line-color));
    background-size: 1px 1px;
    background-repeat: repeat-x;
    background-position: 0% calc(100% - 0.1em);

    /*  Disable oldstyle nums in underlined links because the oldstyle nums are almost subscript-like and overlap */
    font-variant-numeric: lining-nums;
}
.markdownBody a:link,
.markdownBody a:link *,
span.defnMetadata,
span.defnMetadata * {
    text-shadow:
         0      0.05em  var(--GW-link-underline-background-color),
         0.05em 0.05em  var(--GW-link-underline-background-color),
        -0.05em 0.05em  var(--GW-link-underline-background-color),
         0.17em 0.05em  var(--GW-link-underline-background-color),
        -0.17em 0.05em  var(--GW-link-underline-background-color),
         0.17em 0       var(--GW-link-underline-background-color),
        -0.17em 0       var(--GW-link-underline-background-color);
}
.markdownBody a:hover {
    background-image: linear-gradient(var(--GW-link-underline-gradient-line-color-hover), var(--GW-link-underline-gradient-line-color-hover));
}

/* eliminate the blurring of headers and links when selecting by overriding the text-shadow: */
::selection {
    text-shadow: none;
    background: var(--GW-text-selection-background-color);
    color: var(--GW-text-selection-color);
}

/*  Prevent code block background color and border from obscuring link
    underlining, for inline <code> elements in links.
    */
a code {
    border-bottom-width: 0;
    word-break: normal;
    background-color: transparent;
}

/**************/
/* PARAGRAPHS */
/**************/

p {
    margin: 0;
}
p + p {
    text-indent: 2.5em;
}
p + figure[class^='float-'] + p {
    text-indent: 2.5em;
}
@media only screen and (max-width: 649px) {
    p + p {
        text-indent: 1em;
    }
    p + figure[class^='float-'] + p {
        text-indent: 1em;
    }
}

/***************/
/* BLOCKQUOTES */
/***************/
/* additional blockquote styling done in default.css */

blockquote {
    --GW-link-underline-background-color: var(--GW-blockquote-background-color);

    margin: 1.625em 0 1.75em 0;
    border: 1px solid var(--GW-blockquote-border-color);
    background-color: var(--GW-blockquote-background-color);
    font-size: 0.95em;
    padding: 1em 1.25em;
}

/*  Three-level of blockquote emphasis (darkening).
    */
blockquote,
blockquote blockquote blockquote blockquote {
    --GW-blockquote-background-color: var(--GW-blockquote-background-color-level-one);
}
blockquote {
    --GW-blockquote-border-color: var(--GW-blockquote-border-color-level-one);
}
blockquote blockquote blockquote blockquote {
    --GW-blockquote-border-color: var(--GW-blockquote-border-color-level-four);
}

blockquote blockquote,
blockquote blockquote blockquote blockquote blockquote {
    --GW-blockquote-border-color: var(--GW-blockquote-border-color-level-two);
    --GW-blockquote-background-color: var(--GW-blockquote-background-color-level-two);
}

blockquote blockquote blockquote,
blockquote blockquote blockquote blockquote blockquote blockquote {
    --GW-blockquote-border-color: var(--GW-blockquote-border-color-level-three);
    --GW-blockquote-background-color: var(--GW-blockquote-background-color-level-three);
}

/*=--------------------=*/
/*= Various edge cases =*/
/*=--------------------=*/

@media only screen and (min-width: 650px) {
    .abstract blockquote {
        overflow: hidden;
    }
}
@media only screen and (max-width: 649px) {
    /* even less horizontal is available on mobile! */
    blockquote {
        margin: 1.25em 0 1.5em 0;
        padding: 0.75em 1em;
    }
}

p + blockquote {
    margin-top: 1em;
}

/***************/
/* CODE BLOCKS */
/***************/

pre {
    overflow: auto;
}

/*  Full-width code blocks.
    */
.full-width-code-block-wrapper {
    max-width: -moz-fit-content;
    max-width: fit-content;
    margin: auto;
    position: relative;
    z-index: 1;
}
.full-width-code-block-wrapper pre {
    padding-right: 1px;
}

/********/
/* MATH */
/********/

.mjpage__block {
    display: block;
    overflow: auto;
}

/**********/
/* TABLES */
/**********/

.table-wrapper {
    margin: 2em 0 2.125em 0;
    overflow-x: auto;
    overflow-y: hidden;
    box-sizing: border-box;
}

/*=-------------------=*/
/*= Full-width tables =*/
/*=-------------------=*/

.table-wrapper.full-width {
    position: relative;
    z-index: 1;
}
.table-wrapper.full-width table {
    width: -moz-fit-content;
    width: fit-content;
    margin: 0 auto;
}

/***********/
/* FIGURES */
/***********/

figure img,
figure video {
    display: block;
    max-height: calc(100vh - 8em);
    max-width: 100%;
    height: auto;
    width: auto;
    margin: 0 auto;
}

/********/
/* MISC */
/********/

/*  enable standard Pandoc attribute-based syntax for small-caps like '[foo]{.smallcaps}'; see https://pandoc.org/MANUAL.html#small-caps
    */
span.smallcaps {
    font-feature-settings: 'smcp';
}

/* Enable SSfP smallcaps for auto-annotated abbreviations/acronyms */
span.smallcaps-auto {
    font-feature-settings: 'smcp', 'c2sc';
    font-variant-numeric: tabular-nums;
}
/* Reduce text-shadow overlap partially erasing letters next to smallcaps: eg in `["microCOVID project"](https://www.microcovid.org/)`, the 'o' in 'micro' will be partially erased and look like a 'c' without some sort of extra spacing */
a > .smallcaps-auto { margin-left: 0.7px; }

/*************/
/* DARK MODE */
/*************/
/* see /static/js/darkmode.js and /static/css/dark-mode.css; this allows the user to toggle at runtime */
</style>
<style id="inlined-dark-mode-styles" media="all and (prefers-color-scheme: dark)">
:root {
    /*  General.
        */
    --GW-body-background-color: #000;
    --GW-body-text-color: #fff;
    --GW-body-link-color: #e5e5e5;
    --GW-body-link-hover-color: #999;

    /*  Selection.
        */
    --GW-text-selection-background-color: #dcdcdc;
    --GW-text-selection-color: #000;

    /*  Link underlining.
        */
    --GW-link-underline-background-color: var(--GW-body-background-color);
    --GW-link-underline-gradient-line-color: #dcdcdc;
    --GW-link-underline-gradient-line-color-hover: #8b8b8b;

    /*  Blockquotes.
        */
    --GW-blockquote-border-color-level-one: #5c5c5c;
    --GW-blockquote-border-color-level-two: #646464;
    --GW-blockquote-border-color-level-three: #747474;
    --GW-blockquote-border-color-level-four: #7f7f7f;
    --GW-blockquote-background-color-level-one: #212121;
    --GW-blockquote-background-color-level-two: #3e3e3e;
    --GW-blockquote-background-color-level-three: #4f4f4f;

    /*  Abstracts.
        */
    --GW-abstract-border-color: #6c6c6c;
    --GW-abstract-background-color: var(--GW-blockquote-background-color-level-one);

    /*  Sidebar.
        */
    --GW-sidebar-horizontal-rule-color: #7c7c7c;
    --GW-sidebar-mobile-link-border-color: #fff;

    /*  Table of contents.
        */
    --GW-TOC-border-color: #5c5c5c;
    --GW-TOC-background-color: #212121;
    --GW-TOC-link-hover-background-color: #363636;
    --GW-TOC-link-hover-color: #fff;
    --GW-TOC-link-hover-indicator-bar-color: #5c5c5c;
    --GW-TOC-number-color: #929292;
    --GW-TOC-number-hover-color: #ddd;

    /*  Collapse blocks.
        */
	--GW-collapse-summary-blockquote-hover-color: #333;
	--GW-collapse-disclosure-button-color: #333;
	--GW-collapse-disclosure-button-hover-color: #494949;
	--GW-collapse-disclosure-button-top-part-text-color: #6c6c6c;
	--GW-collapse-disclosure-button-bottom-part-text-color: #7c7c7c;
	--GW-collapse-disclosure-button-text-hover-color: #000;

    /*  Headings.
        */
    --GW-H1-box-shadow-background-color: var(--GW-body-background-color);
    --GW-H1-box-shadow-line-color: #999;
    --GW-H2-bottom-border-color: #999;

    /*  Comments.
        */
    --GW-comment-section-top-border-color: #8b8b8b;

    /*  Lists.
        */
    --GW-bulleted-list-marker-color: #9f9f9f;

    /*  Figures.
        */
    --GW-figure-outline-color: #999;
    --GW-figure-caption-outline-color: #999;

    /*  Epigraphs.
        */
    --GW-epigraph-quotation-mark-color: #9f9f9f;

    /*  Footnotes.
        */
    --GW-footnote-border-color: #7c7c7c;
    --GW-footnote-highlighted-border-color: #7c7c7c;
    --GW-footnotes-section-top-rule-color: #5c5c5c;
    --GW-footnote-backlink-border-color: #fff;
    --GW-footnote-backlink-border-hover-color: #8b8b8b;
    --GW-footnote-backlink-after-bordered-block-background-color: var(--GW-body-background-color);

    /*  Footnote references.
        */
    --GW-footnote-ref-highlight-box-shadow-color: #7c7c7c;
    --GW-footnote-ref-highlight-background-color: rgba(0, 0, 0, 0.75);

    /*  Sidenotes.
        */
    --GW-sidenote-highlight-background-color: var(--GW-body-background-color);
    --GW-sidenote-highlight-box-shadow-color: #7c7c7c;
    --GW-sidenote-border-color: #7c7c7c;
    --GW-sidenote-scrollbar-track-color: var(--GW-body-background-color);
    --GW-sidenote-scrollbar-thumb-color: #7c7c7c;
    --GW-sidenote-scrollbar-thumb-hover-color: #8b8b8b;
    --GW-sidenote-self-link-border-color: #7c7c7c;

    /*  Tables.
        */
    --GW-table-border-color: #fff;
    --GW-table-full-width-background-color: var(--GW-body-background-color);
    --GW-table-caption-border-color: #fff;
    --GW-table-row-horizontal-border-color: #fff;
    --GW-table-cell-vertical-border-color: var(--GW-body-background-color);
    --GW-table-scrollbar-track-color: var(--GW-body-background-color);
    --GW-table-scrollbar-thumb-color: #7c7c7c;
    --GW-table-scrollbar-thumb-hover-color: #8b8b8b;
    --GW-table-scrollbar-border-color: #fff;
    --GW-table-column-heading-hover-background-color: #2b3637;
    --GW-table-sorted-column-heading-background-color: #166983;
    --GW-table-sorted-column-heading-text-color: #000;
    --GW-table-sorted-column-heading-text-shadow-color: #fff;
    --GW-table-zebra-stripe-alternate-row-background-color: #252525;
    --GW-table-row-hover-outline-color: #fff;

    /*  Code blocks.
        */
    --GW-code-element-border-color: #606060;
    --GW-code-element-background-color: #1d1d1d;
    --GW-pre-element-border-color: #606060;
    --GW-pre-element-background-color: #1d1d1d;
    --GW-pre-element-scrollbar-track-color: #1d1d1d;
    --GW-pre-element-scrollbar-thumb-color: #5c5c5c;
    --GW-pre-element-scrollbar-thumb-hover-color: #8b8b8b;

    /*  Syntax highlight theme.
        */
    --GW-syntax-highlight-color-normal: #f1edec;
    --GW-syntax-highlight-color-attribute: #d8deff;
    --GW-syntax-highlight-color-data-type: inherit;
    --GW-syntax-highlight-color-variable: #b4b4b4;
    --GW-syntax-highlight-color-other: inherit;
    --GW-syntax-highlight-color-preprocessor: inherit;
    --GW-syntax-highlight-color-extension: #999;
    --GW-syntax-highlight-color-comment: #999;
    --GW-syntax-highlight-color-control-flow: #b6edaa;
    --GW-syntax-highlight-color-keyword: #d8deff;
    --GW-syntax-highlight-color-operator: #d8deff;
    --GW-syntax-highlight-color-special-char: #94adb6;
    --GW-syntax-highlight-color-built-in: #d8deff;
    --GW-syntax-highlight-color-function: #d8deff;
    --GW-syntax-highlight-color-constant: inherit;
    --GW-syntax-highlight-color-base-n: inherit;
    --GW-syntax-highlight-color-dec-val: inherit;
    --GW-syntax-highlight-color-float: inherit;
    --GW-syntax-highlight-color-information: inherit;
    --GW-syntax-highlight-color-char: inherit;
    --GW-syntax-highlight-color-string: inherit;
    --GW-syntax-highlight-color-verbatim-string: inherit;
    --GW-syntax-highlight-color-alert: #ff815c;
    --GW-syntax-highlight-color-error: #ff4727;
    --GW-syntax-highlight-color-import: #a6a6a6;
    --GW-syntax-highlight-color-special-string: #b4b4b4;

    /*  Math.
        */
    --GW-math-block-background-color: #252525;
    --GW-math-block-scrollbar-border-color: #5c5c5c;
    --GW-math-block-scrollbar-track-color: var(--GW-body-background-color);
    --GW-math-block-scrollbar-thumb-color: #5c5c5c;
    --GW-math-block-scrollbar-thumb-hover-color: #8b8b8b;

    /*  Drop caps.
        */
    --GW-drop-caps-yinit-color: #f9f9f9;
    --GW-drop-caps-yinit-text-shadow-color: #a6a6a6;
    --GW-drop-caps-de-zs-color: #efefef;
    --GW-drop-caps-cheshire-color: #f1f1f1;
    --GW-drop-caps-kanzlei-color: #f1f1f1;

    /*  Admonitions.
        */
    --GW-admonition-default-left-border-color: #929292;
    --GW-admonition-default-background-color: #4f4f4f;
    --GW-admonition-tip-left-border-color: #4f4f4f;
    --GW-admonition-tip-background-color: #303030;
    --GW-admonition-warning-left-border-color: #bdbdbd;
    --GW-admonition-warning-background-color: #8a8a8a;
    --GW-admonition-warning-text-color: #000;
    --GW-admonition-warning-title-background-color: #bdbdbd;
    --GW-admonition-warning-icon-color: #000;
    --GW-admonition-error-left-border-color: #e1e1e1;
    --GW-admonition-error-background-color: #bdbdbd;
    --GW-admonition-error-text-color: #000;
    --GW-admonition-error-title-background-color: #e1e1e1;
    --GW-admonition-error-icon-color: #000;
    --GW-admonition-reversed-link-color: #494949;
    --GW-admonition-reversed-link-color-hover: #5c5c5c;
    --GW-admonition-reversed-link-underline-gradient-line-color: #5c5c5c;
    --GW-admonition-reversed-link-underline-gradient-line-color-hover: #6c6c6c;
    --GW-admonition-reversed-footnote-ref-highlight-background-color: rgba(189, 189, 189, 0.75);

    /*  Sequential nav UI.
        */
    --GW-sequential-nav-link-background-color: var(--GW-body-background-color);

    /*  Links.
        */
    --GW-definition-annotation-text-color: #cecece;

    /*  Page-specific styles.
        */
    --GW-index-page-bottom-ornament-line-color: #9f9f9f;

	/*	Pop-frames (popups or popins).
		*/
    --GW-raw-code-popframe-line-highlight-background-color: #181700;
    --GW-raw-code-popframe-line-hightlight-border-color: #494949;
    --GW-raw-code-popframe-line-number-color: #7c7c7c;
    --GW-raw-code-popframe-line-number-divider-color: #5c5c5c;

    /*  Popups.
        */
    --GW-popups-body-background-color: var(--GW-body-background-color);
    --GW-popups-popup-background-color: var(--GW-body-background-color);

    --GW-popups-popup-border-color: #5c5c5c;
    --GW-popups-box-shadow-color: #5c5c5c;
    --GW-popups-popup-border-focused-color: #7c7c7c;
    --GW-popups-box-shadow-focused-color: #7c7c7c;

    --GW-popups-popup-title-bar-background-color: #252525;
    --GW-popups-popup-title-bar-button-color: #6c6c6c;
    --GW-popups-popup-title-bar-button-color-hover: #fff;
	--GW-popups-popup-title-color: #7c7c7c;
    --GW-popups-popup-title-link-hover-color: var(--GW-body-link-hover-color);
    --GW-popups-popup-title-bar-background-focused-color: #3e3e3e;
    --GW-popups-popup-title-bar-button-focused-color: #a6a6a6;
    --GW-popups-popup-title-bar-button-focused-color-hover: #fff;
    --GW-popups-popup-title-bar-submenu-box-shadow-color: #494949;
	--GW-popups-popup-title-focused-color: #fff;
    --GW-popups-popup-title-link-hover-focused-color: var(--GW-body-link-hover-color);

    --GW-popups-popup-scrollbar-track-color: var(--GW-body-background-color);

    --GW-popups-popup-scrollbar-thumb-color: #494949;
    --GW-popups-popup-scrollbar-thumb-hover-color: #6c6c6c;
    --GW-popups-popup-scrollbar-thumb-focused-color: #5c5c5c;
    --GW-popups-popup-scrollbar-thumb-hover-focused-color: #8b8b8b;

    --GW-popups-popup-options-dialog-backdrop-background-color: rgba(0, 0, 0, 0.95);
    --GW-popups-popup-options-dialog-background-color: var(--GW-body-background-color);
    --GW-popups-popup-options-dialog-border-color: #7c7c7c;
    --GW-popups-popup-options-dialog-box-shadow-color: #cecece;
    --GW-popups-popup-options-dialog-horizontal-rule-color: #5c5c5c;
    --GW-popups-popup-options-dialog-button-background-color: var(--GW-body-background-color);
    --GW-popups-popup-options-dialog-button-text-color: #fff;
    --GW-popups-popup-options-dialog-button-border-color: #fff;
    --GW-popups-popup-options-dialog-button-hover-box-shadow-color: #fff;
    --GW-popups-popup-options-dialog-option-button-explanation-text-color: #a6a6a6;
    --GW-popups-popup-options-dialog-option-button-hover-text-color: #a6a6a6;
    --GW-popups-popup-options-dialog-radio-button-border-color: #fff;
    --GW-popups-show-popup-options-dialog-button-color: #8b8b8b;

    /*  Popins.
        */
    --GW-popins-body-background-color: var(--GW-body-background-color);
    --GW-popins-popin-background-color: var(--GW-body-background-color);
    --GW-popins-popin-border-color: #7c7c7c;
    --GW-popins-box-shadow-color: #7c7c7c;
    --GW-popins-popin-title-bar-background-color: #000;
    --GW-popins-popin-title-bar-button-color: #a6a6a6;
    --GW-popins-popin-scrollbar-thumb-color: #5c5c5c;
    --GW-popins-popin-scrollbar-track-color: var(--GW-body-background-color);
    --GW-popins-popin-scrollbar-thumb-hover-color: #8b8b8b;

    /*  Image focus.
        */
    --GW-image-focus-image-hover-drop-shadow-color: #a6a6a6;

    /*  Mode selector (light/dark mode).
        */
    --GW-mode-selector-background-color: var(--GW-body-background-color);
    --GW-mode-selector-border-hover-color: #7c7c7c;
    --GW-mode-selector-button-text-color: #a6a6a6;
    --GW-mode-selector-button-hover-text-color: #fff;

	/*	“Back to top” link.
		*/
	--GW-back-to-top-link-color: #5c5c5c;
	--GW-back-to-top-link-hover-color: #8b8b8b;
}
:root {
	/*	Adjust background color to prevent pixels from turning off.
		*/
	--GW-body-background-color: #010101;

	/*	Adjust this one color in the syntax highlighting color scheme to make it
		less saturated. (The automatic inversion algorithm doesn’t quite handle
		it in the way we want.)
		*/
    --GW-syntax-highlight-color-control-flow: #cce1c8;
}

/*	The ‘fill’ attribute of the SVGs is modified in dark mode.
	(We cannot use an invert filter because we don’t want to invert the whole
	 element, only the background-image.)
	*/
.collapse .disclosure-button::before {
    background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M34.52 239.03L228.87 44.69c9.37-9.37 24.57-9.37 33.94 0l22.67 22.67c9.36 9.36 9.37 24.52.04 33.9L131.49 256l154.02 154.75c9.34 9.38 9.32 24.54-.04 33.9l-22.67 22.67c-9.37 9.37-24.57 9.37-33.94 0L34.52 272.97c-9.37-9.37-9.37-24.57 0-33.94z" fill="%235C5C5C"></path></svg>');
}
.collapse .disclosure-button:hover::before {
    background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M34.52 239.03L228.87 44.69c9.37-9.37 24.57-9.37 33.94 0l22.67 22.67c9.36 9.36 9.37 24.52.04 33.9L131.49 256l154.02 154.75c9.34 9.38 9.32 24.54-.04 33.9l-22.67 22.67c-9.37 9.37-24.57 9.37-33.94 0L34.52 272.97c-9.37-9.37-9.37-24.57 0-33.94z" fill="%23000"></path></svg>');
}
.collapse .disclosure-button:checked::before {
    background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z" fill="%235C5C5C"></path></svg>');
}
.collapse .disclosure-button:checked:hover::before {
    background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z" fill="%23000"></path></svg>');
}

/*	The ‘celestial symbol’ horizonal rules are SVGs and must be inverted.
	*/
hr::after,
.horizontalRule-nth-0 hr::after {
    opacity: 1;
    filter: invert(1) drop-shadow(0 0 0 currentColor);
}
.horizontalRule-nth-1 hr::after {
    opacity: 0.75;
    filter: invert(1);
}
.horizontalRule-nth-2 hr::after {
    opacity: 0.85;
    filter: invert(1);
}

/*	Drop cap opacity adjustment.
	*/
.drop-cap-goudy::first-letter,
.drop-cap-goudy > p::first-letter {
    opacity: 0.95;
}

/*	Admonition icons.
	*/
.admonition.tip::before,
.admonition.note::before,
.admonition.warn::before,
.admonition.warning::before,
.admonition.error::before {
    filter: invert(1);
}

div.admonition.warn,
div.admonition.warning,
div.admonition.error {
    --GW-dotted-underline-background-image: var(--GW-dotted-underline-hover-dark-background-image);
}

/*	List bullets are SVGs and have to be inverted.
	*/
ul > li::before {
    filter: invert(1);
}

/*	Exclusion for TOC entries.
	*/
#TOC ul > li::before {
    filter: none;
}

/*	For sortable table column headings, we use dark versions of the up/down/both
	arrow icons.
	*/
table th.tablesorter-header {
    background-image: url('/static/img/tablesorter/tablesorter-bg-dark.gif');
}
table th.tablesorter-headerAsc {
    background-image: url('/static/img/tablesorter/tablesorter-asc-dark.gif');
}
table th.tablesorter-headerDesc {
    background-image: url('/static/img/tablesorter/tablesorter-desc-dark.gif');
}

/*	The pixel-grid-checkerboard pattern of these scroll bars is created by GIF
	background-images; we use alternate versions of the images in dark mode.
	*/
.table-wrapper::-webkit-scrollbar-thumb {
    background-image: url('data:image/gif;base64,R0lGODlhBAAEAPAAMQAAAIiIiCwAAAAABAAEAAACBkwAhqgZBQA7');
}
.table-wrapper::-webkit-scrollbar-thumb:hover {
    background-image: url('data:image/gif;base64,R0lGODlhBAAEAPAAMQAAAL+/vywAAAAABAAEAAACBkwAhqgZBQA7');
}

.sidenote.cut-off .sidenote-outer-wrapper::-webkit-scrollbar-thumb {
    background-image: url('data:image/gif;base64,R0lGODlhBAAEAPAAMQAAAIiIiCwAAAAABAAEAAACBkwAhqgZBQA7');
}
.sidenote.cut-off .sidenote-outer-wrapper::-webkit-scrollbar-thumb:hover {
    background-image: url('data:image/gif;base64,R0lGODlhBAAEAPAAMQAAAL+/vywAAAAABAAEAAACBkwAhqgZBQA7');
}

/*	Images that are marked as invertible by the server are inverted, 
	hue-rotated, and desaturated. Other (non-invertible) images are merely
	desaturated. Hovering over an image restores it to its original state.
	*/
img.invertible,
img.invertible-auto {
    filter: grayscale(50%) invert(100%) brightness(95%) hue-rotate(180deg);
}
img:not(.invertible):not(.invertible-auto) {
    filter: grayscale(50%);
}
img,
img.invertible,
img.invertible-auto {
    transition: filter 0.25s ease;
}
img:hover,
img.invertible:hover,
img.invertible-auto:hover {
    filter: none;
    transition: filter 0s ease 0.25s;
}

/*	The loading spinner for object popups (image, iframe, object) is inverted
	and made more visible in dark mode.
	*/
.popupdiv.loading::before {
	filter: invert(1);
	opacity: 0.4;
}

/*	“Loading failed” messages for object popups.
	*/
.popupdiv.loading-failed::after {
	opacity: 0.4;
}

/*	The mode selector is adjusted to be better visible in dark mode, and its
	button icons inverted.
	*/
div#mode-selector {
    opacity: 0.6;
}
div#mode-selector button::before {
	filter: invert(1);
	opacity: 0.45;
}
div#mode-selector button.select-mode-light::before {
	opacity: 0.55;
}

/*	The dotted underline for Wikipedia and definition links is created by a
	pixel-grid-checkerboard background-image. We use an alternate version of 
	this image in dark mode.
	*/
.markdownBody a[href*='wikipedia.org'].docMetadata:hover,
.markdownBody a[href*='wikimedia.org'].docMetadata:hover,
.markdownBody a[href*='wiktionary.org'].docMetadata:hover,
.markdownBody a[href*='wikimediafoundation.org'].docMetadata:hover,
.markdownBody a[href*='wikisource.org'].docMetadata:hover,
.markdownBody span.defnMetadata:hover {
    background-image: var(--GW-dotted-underline-hover-dark-background-image);
}

/*	All SVG linkicons have to be inverted.
	*/
a.link-self.identifier-link-up::after,
a.link-self.identifier-link-down::after,
a[href$='.pdf' i]::after,
a[href$='/pdf' i]::after,
a[href*='/pdf/' i]::after,
a[href$='type=pdf' i]::after,
a[href*='.pdf#' i]::after,
a[href*='pdfs.semanticscholar.org' i]::after,
a[href$='.epub' i]::after,
a[href*='citeseerx.ist.psu.edu' i]::after,
a[href*='eprint.iacr.org' i]::after,
a[href$='.opml' i]::after,
a[href$='.txt' i]::after,
a[href$='.xml' i]::after,
a[href$='.css' i]::after,
a[href$='.hs' i]::after,
a[href$='.js' i]::after,
a[href$='.json' i]::after,
a[href$='.jsonl' i]::after,
a[href^='/'][href$='.php' i]::after,
a[href$='.conf' i]::after,
a[href$='.sh' i]::after,
a[href$='.R' i]::after,
a[href$='.patch' i]::after,
a[href$='.diff' i]::after,
a[href^="/static/" i][href$=".html" i]::after,
a[href$='.doc' i]::after,
a[href$='.docx' i]::after,
a[href*='docs.google.com' i]::after,
a[href$='.xls' i]::after,
a[href$='.xlsx' i]::after,
a[href$='.ods' i]::after,
a[href$='.csv' i]::after,
a[href$='.gif' i]::after,
a[href$='.bmp' i]::after,
a[href$='.ico' i]::after,
a[href$='.jpg' i]::after,
a[href$='.jpeg' i]::after,
a[href$='.png' i]::after,
a[href$='.svg' i]::after,
a[href$='.xcf' i]::after,
a[href*='imgur.com']::after,
a[href$='.mp3' i]::after,
a[href$='.wav' i]::after,
a[href$='.flac' i]::after,
a[href$='.ogg' i]::after,
a[href$='.rm' i]::after,
a[href$='.swf' i]::after,
a[href$='.avi' i]::after,
a[href$='.mp4' i]::after,
a[href$='.webm' i]::after,
a[href$='.tar' i]::after,
a[href$='.zip' i]::after,
a[href$='.xz'  i]::after,
a[href$='.img' i]::after,
a[href$='.bin' i]::after,
a[href$='.pkl' i]::after,
a[href$='.onnx' i]::after,
a[href$='.pt' i]::after,
a[href$='.ebt' i]::after,
a[href$='.mdb' i]::after,
a[href$='.mht' i]::after,
a[href$='.ttf' i]::after,
a[href*='biorxiv.org']::after,
a[href*='medrxiv.org']::after,
a[href*='substack.com']::after,
a[href*='scholar.google.com']::after,
a[href*='plosone.org']::after,
a[href*='plosmedicine.org']::after,
a[href*='plos.org']::after,
a[href*='www.plos']::after,
a[href*='patreon.com']::after,
a[href*='uptontea.com']::after,
a[href*='wired.com']::after,
a[href*='washingtonpost.com']::after,
a[href*='nytimes.com']::after,
a[href*='nlm.nih.gov']::after,
a[href*='newyorker.com']::after,
a[href*='theguardian.com']::after,
a[href*='www.guardian.co.uk']::after,
a[href*='reddit.com']::after,
a[href*='paulgraham.com']::after,
a[href*='ycombinator.com']::after,
a[href*='intelligence.org']::after,
a[href*='deepmind']::after,
a[href*='docs/.*/*.pdf#deepmind']::after,
a[href*='openai']::after,
a[href$='.pdf#openai']::after,
a[href*='distill.pub']::after,
a[href*='twitter.com']::after,
a[href*='nitter.net']::after,
a[href*='erowid.org']::after,
a[href*='wikipedia.org']::after,
a[href*='wikimedia.org']::after,
a[href*='wiktionary.org']::after,
a[href*='wikimediafoundation.org']::after,
a[href*='wikisource.org']::after,
a[href*='youtube.com']::after,
a[href*='youtu.be']::after,
a[href*='soundcloud.com']::after,
a[href*='bandcamp.com']::after,
a[href*='overflow.net']::after,
a[href*='overflow.com']::after,
a[href*='stackexchange.com']::after,
a[href*='amazon.com']::after,
a[href*='amazon.co.']::after,
a[href*='amzn.com'  ]::after,
a[href*='github.com']::after,
a[href*='dropbox.com']::after,
a[href*='dropboxusercontent.com']::after,
a[href*='mega.nz']::after,
a[href*='webcitation.org'   ]::after,
a[href*='mementoweb.org'    ]::after,
a[href*='archive.org'       ]::after,
a[href*='archive-it.org'    ]::after,
a[href*='archiveteam.org'   ]::after,
a[href*='waybackmachine.org']::after,
a.local-archive-link::after,
a[href*="tumblr.com"]::after,
a[href*="t.umblr.com"]::after,
a[href*="#facebook"]::after,
a[href*="#facebook"][href*="arxiv.org"]::after,
a[href*='docs/.*/*.pdf#.*facebook']::after,
a[href*="facebook.com"]::after,
a[href*="bitcointalk.org"]::after,
a[href*="bitcoin.it"]::after,
a[href*="www.google.com"]::after,
a[href*="#google"]::after,
a[href*="#org=google"]::after,
a[href*="#google"][href*="arxiv.org"]::after,
a[href*="#org=google"][href*="arxiv.org"]::after,
a[href*='docs/.*/*.pdf#.*google']::after,
a[href*='docs/.*/*.pdf#org=google']::after {
    filter: invert(1);
}
</style>
<script type="text/javascript" async="" src="./一个StyleGAN动漫脸编辑详细教程_files/analytics.js.下载"></script><script>
/*  Create global 'GW' object, if need be.
    */
if (typeof window.GW == "undefined")
    window.GW = { };

/*****************/
/* MEDIA QUERIES */
/*****************/

GW.mediaQueries = {
    mobileWidth:           matchMedia("(max-width: 649px)"),
    systemDarkModeActive:  matchMedia("(prefers-color-scheme: dark)"),
    hoverAvailable:        matchMedia("only screen and (hover: hover) and (pointer: fine)"),
    portraitOrientation:   matchMedia("(orientation: portrait)")
};

GW.isMobile = () => {
	/*  We consider a client to be mobile if one of two conditions obtain:
		1. JavaScript detects touch capability, AND viewport is narrow; or,
		2. CSS does NOT detect hover capability.
		*/
	return (   (   ('ontouchstart' in document.documentElement)
				&& GW.mediaQueries.mobileWidth.matches)
			|| !GW.mediaQueries.hoverAvailable.matches);
};

GW.isFirefox = () => {
	return (navigator.userAgent.indexOf("Firefox") > 0);
};

/********************/
/* DEBUGGING OUTPUT */
/********************/

GW.logLevel = localStorage.getItem("gw-log-level") || 0;
GW.logSourcePadLength = 18;
GW.dateTimeFormat = new Intl.DateTimeFormat([], { hour12: false, hour: "numeric", minute: "numeric", second: "numeric" });

function GWLog (string, source = "", level = 1) {
    if (GW.logLevel < level) return;

    let time = Date.now();
    let ms = `${(time % 1000)}`.padStart(3,'0');
    let timestamp = `[${GW.dateTimeFormat.format(time)}.${ms}]  `;
    let sourcestamp = (source > "" ? `[${source}]` : `[ ]`).padEnd(GW.logSourcePadLength, ' ');

    console.log(timestamp + sourcestamp + string);
}
GW.setLogLevel = (level, permanently = false) => {
    if (permanently)
        localStorage.setItem("gw-log-level", level);

    GW.logLevel = level;
};

/***********/
/* HELPERS */
/***********/

/*	Because encodeURIComponent does not conform to RFC 3986; see MDN docs.
	*/
function fixedEncodeURIComponent(str) {
	return encodeURIComponent(str).replace(/[!'()*]/g, function(c) {
		return '%' + c.charCodeAt(0).toString(16);
	});
}

/*	Helper function for AJAX, by kronusaturn
	https://github.com/kronusaturn/lw2-viewer/blob/master/www/script.js
	*/
function urlEncodeQuery(params) {
	return (Object.keys(params)).map(x => (`${x}=${ fixedEncodeURIComponent(params[x]) }`)).join("&");
}

/*	Helper function for AJAX, by kronusaturn
	https://github.com/kronusaturn/lw2-viewer/blob/master/www/script.js
	*/
function doAjax(options) {
	let req = new XMLHttpRequest();
	req.addEventListener("load", (event) => {
		if (event.target.status < 400) {
			if (options["onSuccess"]) options.onSuccess(event);
		} else {
			if (options["onFailure"]) options.onFailure(event);
		}
	});
	let method = (options["method"] || "GET");
	let location = (options.location || document.location) + ((options.params && method == "GET") ? ("?" + urlEncodeQuery(options.params)) : "");
	req.open(method, location);
	if (options["method"] == "POST") {
		req.setRequestHeader("Content-Type", "application/x-www-form-urlencoded");
		req.send(urlEncodeQuery(options.params));
	} else {
		req.send();
	}
}

/*  Adds an event listener to a button (or other clickable element), attaching
    it to both ‘click’ and ‘keyup’ events (for use with keyboard navigation).
    Optionally also attaches the listener to the ‘mousedown’ event, making the
    element activate on mouse down instead of mouse up.
    */
Element.prototype.addActivateEvent = function(fn, includeMouseDown) {
    let ael = this.activateEventListener = (event) => { 
    	if (event.button === 0 || event.key === ' ') 
    		fn(event);
    };
    if (includeMouseDown) this.addEventListener("mousedown", ael);
    this.addEventListener("click", ael);
    this.addEventListener("keyup", ael);
}

/*	Swap classes on the given element.
	First argument is an array with two string elements (the classes).
	Second argument is 0 or 1 (index of class to add; the other is removed).
	*/
Element.prototype.swapClasses = function (classes, whichToAdd) {
	this.classList.add(classes[whichToAdd]);
	this.classList.remove(classes[1 - whichToAdd]);
};

/*	Returns true if the given rects intersect, false otherwise.
	*/
function doRectsIntersect(rectA, rectB) {
    return (rectA.top < rectB.bottom &&
            rectA.bottom > rectB.top &&
            rectA.left < rectB.right &&
            rectA.right > rectB.left);
}

/*  Returns true if the given element intersects the given rect, 
	false otherwise.
    */
function isWithinRect(element, rect) {
	return doRectsIntersect(element.getBoundingClientRect(), rect);
}

/*  Returns true if the given element intersects the viewport, false otherwise.
    */
function isOnScreen (element) {
    return isWithinRect(element, new DOMRect(0, 0, window.innerWidth, window.innerHeight));
}

/*	Returns the string trimmed of opening/closing quotes.
	*/
String.prototype.trimQuotes = function () {
	return this.replace(/^["'“‘]?(.+?)["'”’]?$/, '$1');
};

/*  Returns true if the string begins with any of the given prefixes.
    */
String.prototype.startsWithAnyOf = function (prefixes) {
    for (prefix of prefixes)
        if (this.startsWith(prefix))
            return true;
    return false;
}

/*  Returns true if the string ends with any of the given suffixes.
    */
String.prototype.endsWithAnyOf = function (suffixes) {
    for (suffix of suffixes)
        if (this.endsWith(suffix))
            return true;
    return false;
}

/*  Returns true if the string includes any of the given substrings.
    */
String.prototype.includesAnyOf = function (substrings) {
    for (substring of substrings)
        if (this.includes(substring))
            return true
    return false;
}

/*  Remove given item from array.
	*/
Array.prototype.remove = function (item) {
    let index = this.indexOf(item);
    if (index !== -1)
        this.splice(index, 1);
};

/*  Remove from array the first item that passes the provided test function.
	The test function should take an array item and return true/false.
	*/
Array.prototype.removeIf = function (test) {
    let index = this.findIndex(test);
    if (index !== -1)
        this.splice(index, 1);
};

/*  Run the given function immediately if the page is already loaded, or add
    a listener to run it as soon as the page loads.
    */
function doWhenPageLoaded(f) {
    if (document.readyState == "complete")
        f();
    else
        window.addEventListener("load", () => { f(); });
}

/*  Run the given function immediately if the page content has already loaded
	(DOMContentLoaded event has fired), or add a listener to run it as soon as 
	the event fires.
    */
function doWhenDOMContentLoaded(f) {
    if (GW.DOMContentLoaded == true)
        f();
    else
        window.addEventListener("DOMContentLoaded", () => { f(); });
}

/*  Given an HTML string, creates an element from that HTML, adds it to
    #ui-elements-container (creating the latter if it does not exist), and
    returns the created element.
    */
function addUIElement(element_html) {
    let ui_elements_container = document.querySelector("#ui-elements-container");
    if (!ui_elements_container) {
        ui_elements_container = document.createElement("div");
        ui_elements_container.id = "ui-elements-container";
        document.querySelector("body").appendChild(ui_elements_container);
    }

    ui_elements_container.insertAdjacentHTML("beforeend", element_html);
    return ui_elements_container.lastElementChild;
}

GW.scrollListeners = { };
/*  Adds a scroll event listener to the page.
    */
function addScrollListener(fn, name) {
    let wrapper = (event) => {
        requestAnimationFrame(() => {
            fn(event);
            document.addEventListener("scroll", wrapper, { once: true, passive: true });
        });
    }
    document.addEventListener("scroll", wrapper, { once: true, passive: true });

    // Retain a reference to the scroll listener, if a name is provided.
    if (typeof name != "undefined") {
        GW.scrollListeners[name] = wrapper;
    }
}
/*	Removes a named scroll event listener from the page.
	*/
function removeScrollListener(name) {
	let wrapper = GW.scrollListeners[name];
	if (wrapper) {
		document.removeEventListener("scroll", wrapper);
		GW.scrollListeners[name] = null;
	}
}

/*	Returns val, or def if val == defval. (By default, defval is -1.)
	(In other words, `defval(X,Y,Z)` is “return X if Y is Z [else, just Y]”.)
	*/
function defval(def, val, defval = -1) {
	return (val == defval) ? def : val;
}

/*	Returns val, or min if val < min, or max if val > max.
	(In other words, clamps val to [min,max].)
	*/
function valMinMax(val, min, max) {
	return Math.max(Math.min(val, max), min);
}

/************************/
/* ACTIVE MEDIA QUERIES */
/************************/

/*  This function provides two slightly different versions of its functionality,
    depending on how many arguments it gets.

    If one function is given (in addition to the media query and its name), it
    is called whenever the media query changes (in either direction).

    If two functions are given (in addition to the media query and its name),
    then the first function is called whenever the media query starts matching,
    and the second function is called whenever the media query stops matching.

    If you want to call a function for a change in one direction only, pass an
    empty closure (NOT null!) as one of the function arguments.

    There is also an optional fifth argument. This should be a function to be
    called when the active media query is canceled.
    */
function doWhenMatchMedia(mediaQuery, name, ifMatchesOrAlwaysDo, otherwiseDo = null, whenCanceledDo = null) {
    if (typeof GW.mediaQueryResponders == "undefined")
        GW.mediaQueryResponders = { };

    let mediaQueryResponder = (event, canceling = false) => {
        if (canceling) {
            GWLog(`Canceling media query “${name}”`, "media queries", 1);

            if (whenCanceledDo != null)
                whenCanceledDo(mediaQuery);
        } else {
            let matches = (typeof event == "undefined") ? mediaQuery.matches : event.matches;

            GWLog(`Media query “${name}” triggered (matches: ${matches ? "YES" : "NO"})`, "media queries", 1);

            if (otherwiseDo == null || matches) ifMatchesOrAlwaysDo(mediaQuery);
            else otherwiseDo(mediaQuery);
        }
    };
    mediaQueryResponder();
    mediaQuery.addListener(mediaQueryResponder);

    GW.mediaQueryResponders[name] = mediaQueryResponder;
}

/*  Deactivates and discards an active media query, after calling the function
    that was passed as the whenCanceledDo parameter when the media query was
    added.
    */
function cancelDoWhenMatchMedia(name) {
    GW.mediaQueryResponders[name](null, true);

    for ([ key, mediaQuery ] of Object.entries(GW.mediaQueries))
        mediaQuery.removeListener(GW.mediaQueryResponders[name]);

    GW.mediaQueryResponders[name] = null;
}

/*****************/
/* NOTIFICATIONS */
/*****************/
/*  Options object may have members:

        `once` (boolean; does the handler get removed after being called?)
        `phase` (string; in [e.g. "rewrite"], before [e.g. "<rewrite"], or after
        		 [e.g. ">rewrite"] which phase of that event’s handlers does the
        		 given handler get called?)
    */
GW.notificationCenter = {
	eventHandlers: { },
	handlerPhaseOrders: {
		"GW.contentDidLoad": [ "rewrite", "eventListeners" ]
	},
	addHandlerForEvent: (eventName, f, options = { }) => {
		if (GW.notificationCenter.eventHandlers[eventName] == null)
			GW.notificationCenter.eventHandlers[eventName] = [ ];

		let handlers = GW.notificationCenter.eventHandlers[eventName];
		if (handlers.findIndex(handler => handler.f == f) !== -1)
			return;

		let insertAt = handlers.length;

		let phaseOrder = GW.notificationCenter.handlerPhaseOrders[eventName];
		if (phaseOrder)
			options.phase = (options.phase || "");
		if (options.phase && phaseOrder) {
			let targetPhase = options.phase.match(/^([<>]?)(.+)/)[2];
			let targetPhaseOrder = defval(phaseOrder.length, phaseOrder.indexOf(targetPhase), -1);

			let phaseAt = (index) => {
				if (index >= handlers.length) return null;
				let parts = handlers[index].options.phase.match(/^([<>]?)(.+)/);
				return {
					phase: parts[2], 
					before: (parts[1] == "<"), 
					after: (parts[1] == ">")
				};
			};

			if (options.phase.startsWith("<")) {
				for (var i = 0; i < handlers.length; i++) {
					if (phaseAt(i).phase == targetPhase && !phaseAt(i).before)
						break;
					if (phaseOrder.slice(targetPhaseOrder + 1).includes(phaseAt(i).phase))
						break;
				}
				insertAt = i;
			} else if (options.phase.startsWith(">")) {
				for (var j = handlers.length - 1; j > -1; j--) {
					if (phaseAt(j).phase == targetPhase) {
						j++;
						break;
					}
					if (phaseOrder.slice(0, targetPhaseOrder - 1).includes(phaseAt(j).phase)) {
						j++;
						break;
					}
				}
				insertAt = j;
			} else {
				for (var k = 0; k < handlers.length; k++) {
					if (phaseAt(k).phase == targetPhase && phaseAt(k).after)
						break;
					if (phaseOrder.slice(targetPhaseOrder + 1).includes(phaseAt(k).phase))
						break;
				}
				insertAt = k;
			}
		}

		GW.notificationCenter.eventHandlers[eventName].splice(insertAt, 0, { f: f, options: options });
	},
	removeHandlerForEvent: (eventName, f, options = { }) => {
		if (GW.notificationCenter.eventHandlers[eventName] == null)
			return;

		GW.notificationCenter.eventHandlers[eventName].removeIf(handler => handler.f == f);
	},
	removeAllHandlersForEvent: (eventName) => {
		GW.notificationCenter.eventHandlers[eventName] = null;
	},
	fireEvent: (eventName, eventInfo) => {
		/*  The ‘16’ here is the width of the date field plus spacing.
			The “Source:” text is manually padded to be as wide as “[notification]”.
			*/
		GWLog(`Event “${eventName}” fired.` + `${(
			(eventInfo && eventInfo.source) 
			? ("\n" + "".padStart(16, ' ') + "       Source:".padEnd(GW.logSourcePadLength, ' ') + eventInfo.source) 
			: ""
		)}`, "notification");

		if (GW.notificationCenter.eventHandlers[eventName] == null)
			return;

		for (let i = 0; i < GW.notificationCenter.eventHandlers[eventName].length; i++) {
			let handler = GW.notificationCenter.eventHandlers[eventName][i];
			if (handler.options.condition && !handler.options.condition(eventInfo))
				continue;
			handler.f(eventInfo);
			if (handler.options.once) {
				GW.notificationCenter.eventHandlers[eventName].splice(i, 1);
				i--;
			}
		}
	}
};

/****************/
/* SCROLL STATE */
/****************/

GW.scrollState = {
	lastScrollTop:				window.pageYOffset || document.documentElement.scrollTop,
	unbrokenDownScrollDistance:	0,
	unbrokenUpScrollDistance:	0
};

function updateScrollState(event) {
    GWLog("updateScrollState", "gw.js", 3);

    let newScrollTop = window.pageYOffset || document.documentElement.scrollTop;
    GW.scrollState.unbrokenDownScrollDistance = (newScrollTop > GW.scrollState.lastScrollTop) 
    	? (GW.scrollState.unbrokenDownScrollDistance + newScrollTop - GW.scrollState.lastScrollTop) 
    	: 0;
    GW.scrollState.unbrokenUpScrollDistance = (newScrollTop < GW.scrollState.lastScrollTop) 
    	? (GW.scrollState.unbrokenUpScrollDistance + GW.scrollState.lastScrollTop - newScrollTop) 
    	: 0;
    GW.scrollState.lastScrollTop = newScrollTop;
}
addScrollListener(updateScrollState, "updateScrollStateScrollListener");

/*	Toggles whether the page is scrollable.
	*/
function isPageScrollingEnabled() {
	return !(document.documentElement.classList.contains("no-scroll"));
}
function togglePageScrolling(enable) {
	if (typeof enable == "undefined")
		enable = document.documentElement.classList.contains("no-scroll");

	let preventScroll = (event) => { document.documentElement.scrollTop = GW.scrollState.lastScrollTop; };

	if (enable && !isPageScrollingEnabled()) {
		document.documentElement.classList.toggle("no-scroll", false);
		removeScrollListener("preventScroll");
		addScrollListener(updateScrollState, "updateScrollStateScrollListener");
	} else if (!enable && isPageScrollingEnabled()) {
		document.documentElement.classList.toggle("no-scroll", true);
		addScrollListener(preventScroll, "preventScroll");
		removeScrollListener("updateScrollStateScrollListener");
	}
}

/******************/
/* BROWSER EVENTS */
/******************/

GW.DOMContentLoaded = false;

GWLog("document.readyState." + document.readyState, "browser event");
window.addEventListener("DOMContentLoaded", () => {
    GWLog("window.DOMContentLoaded", "browser event");
    GW.DOMContentLoaded = true;
    GW.notificationCenter.fireEvent("GW.contentDidLoad", {
		source: "DOMContentLoaded",
		document: document.firstElementChild, 
		isMainDocument: true,
		needsRewrite: true, 
		clickable: true, 
		collapseAllowed: true, 
		isCollapseBlock: false,
		isFullPage: true,
		location: new URL(location.href),
		fullWidthPossible: true
    });
});
window.addEventListener("load", () => {
    GWLog("window.load", "browser event");
});
document.addEventListener("readystatechange", () => {
    GWLog("document.readyState." + document.readyState, "browser event");
});
</script>
<script>
/*	This code is part of darkmode.js by Said Achmiz.
	See the file `darkmode.js` for license and more information.
	*/

/*	Dark mode: before anything else loads, check browser localStorage for dark 
	mode preference and immediately toggle sets of CSS color variables/classes 
	to avoid any 'flash of white' or delayed loading. Note: CSS falls back to 
	the media-query browser/OS variable preference, so still works if JS is 
	blocked! (The JS is only necessary for the theme switcher widget allowing 
	'force light'/'force dark' options. If users block JS, set the dark mode 
	preference, and are unhappy when they get dark mode, well, they made their 
	bed and must lie in it.)
	*/

/*  Set specified color mode (auto, light, dark).
    */
function setMode(modeOption) {
    GWLog("setMode", "darkmode.js", 1);

    // Inject the appropriate styles.
    let darkModeStyles = document.querySelector("#inlined-dark-mode-styles, #dark-mode-styles");
    if (darkModeStyles == null) return;
    if (modeOption == 'auto') {
        darkModeStyles.media = "all and (prefers-color-scheme: dark)";
    } else if (modeOption == 'dark') {
        darkModeStyles.media = "all";
    } else {
        darkModeStyles.media = "not all";
    }

    // Update selector state.
    if (window.updateModeSelectorState)
	    updateModeSelectorState();
}

// Get saved mode setting (or default).
let currentMode = localStorage.getItem("selected-mode") || 'auto';

// Activate saved mode.
setMode(currentMode);
</script>


    <!-- fetch the title & body fonts immediately to avoid re-rendering -->
    <link rel="preload" as="style" href="https://www.gwern.net/static/css/default.css">
    <link rel="preload" href="https://www.gwern.net/static/font/ssp/SourceSansPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssp/SourceSansPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssp/SourceSansPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssfp/SourceSerifPro-BASIC-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssfp/SourceSerifPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssfp/SourceSerifPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssfp/SourceSerifPro-BASIC-SemiboldItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssfp/SourceSerifPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ssfp/SourceSerifPro-BASIC-BoldItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
    <link rel="preload" href="https://www.gwern.net/static/font/ibm-plex-mono/IBMPlexMono-Regular.otf" as="font" type="font/otf" crossorigin="anonymous">
    <!-- Full font CSS is loaded in footer. Here, inline only the minimal set of font CSS needed for initial screen, for speed: -->
    <style>
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/ssfp/SourceSerifPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/ssfp/SourceSerifPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap;
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 600;
        font-style: normal;
        src: url('/static/font/ssfp/SourceSerifPro-BASIC-Semibold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/ssfp/SourceSerifPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }

    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/ssp/SourceSansPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/ssp/SourceSansPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/ssp/SourceSansPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    </style>

    
    <!-- JS library for Tufte-style 'sidenotes' (footnotes popped into the left & right margins on sufficiently-wide screens); much more convenient than floating footnotes & endnotes -->
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/sidenotes.js.下载" async=""></script> <!-- since sidenotes visibly render, unlike the other JS libs, we prioritize its loading in the head -->
    
    <!-- Hint at necessary third-party domains -->
    <link rel="preconnect" href="https://www.google-analytics.com/">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com/">

    <meta name="title" content="Making Anime Faces With StyleGAN">
    <meta name="citation_title" content="Making Anime Faces With StyleGAN">
    <meta name="og:title" content="Making Anime Faces With StyleGAN">
    <meta name="twitter:title" content="Making Anime Faces With StyleGAN">
    <meta name="generator" content="hakyll">
    <meta name="creator" content="gwern.net">
    
    <meta name="author" content="Gwern Branwen">
    <meta name="citation_author" content="Gwern Branwen">
    
    <meta name="contact" content="https://www.gwern.net/Links#contact">
    <link rel="index" title="Gwern.net" href="http://www.gwern.net/">
    <meta name="twitter:creator" content="gwern">
    <meta name="twitter:site" content="gwern.net">
    <meta name="og:site" content="gwern.net">
    <meta name="og:type" content="article">
    <meta name="description" content="A tutorial explaining how to train and generate high-quality anime faces with StyleGAN 1/2 neural networks, and tips/scripts for effective StyleGAN use.">
    <meta name="og:description" content="A tutorial explaining how to train and generate high-quality anime faces with StyleGAN 1/2 neural networks, and tips/scripts for effective StyleGAN use.">
    <meta property="og:image" content="https://www.gwern.net/images/gan/stylegan/stylegan-asuka-face-sample.png">
    
    <meta name="keywords" content="anime, NGE, NN, Python, technology, tutorial">
    <meta name="dc.date.issued" content="2019-02-04">
    <meta name="citation_publication_date" content="2019-02-04">
    <meta name="dcterms.modified" content="2021-01-30">
    <link rel="schema.dcterms" href="http://purl.org/dc/terms/">
    <meta name="dcterms.rights" content="CC PD-0">
    <meta name="dc.rights" content="https://creativecommons.org/publicdomain/zero/1.0/">
    <link rel="canonical" href="https://www.gwern.net/Faces">
    <meta name="citation_fulltext_html_url" content="https://www.gwern.net/Faces">
    <meta name="og:url" content="https://www.gwern.net/Faces">
    <meta name="page-source" content="https://www.gwern.net/Faces.page">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="color-scheme" content="light dark">

    <title>Making Anime Faces With StyleGAN · Gwern.net</title>

    <link rel="icon" type="image/png" href="https://www.gwern.net/static/img/logo/logo-favicon-small.png">
    <link rel="apple-touch-icon" type="image/png" href="https://www.gwern.net/static/img/logo/logo-favicon-appletouch.png">
  <style id="full-width-block-layout-styles">:root { 
			--GW-full-width-block-layout-side-margin: 25px;
			--GW-full-width-block-layout-page-width: 1903px;
			--GW-full-width-block-layout-left-adjustment: 20px; 
		}</style><style id="mode-selector-styles">
	#mode-selector {
		position: absolute;
		right: 0;
		display: flex;
		background-color: var(--GW-mode-selector-background-color);
		padding: 0.1em 0.25em 0.3em 0.25em;
		border: 3px solid transparent;
		opacity: 0.3;
		transition:
			opacity 2s ease;
	}
	#mode-selector.hidden {
		opacity: 0;
	}
	#mode-selector:hover {
		transition: none;
		opacity: 1.0;
		border: 3px double var(--GW-mode-selector-border-hover-color);
	}
	#mode-selector button {
		-moz-appearance: none;
		appearance: none;
		border: none;
		background-color: transparent;
		padding: 0.5em;
		margin: 0 0 0 1.375em;
		line-height: 1;
		font-family: var(--GW-sans-serif-font-stack);
		font-size: 0.75rem;
		text-align: center;
		color: var(--GW-mode-selector-button-text-color);
		position: relative;
		display: flex;
	}
	#mode-selector button::before {
		width: 1.2em;
		position: absolute;
		left: -15px;
		opacity: 0.35;
		padding: 1px 0 0 0;
	}
	#mode-selector button.select-mode-auto::before {
		content: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705 0 184 82.311 184 184 0 101.705-82.311 184-184 184z"></path></svg>');
	}
	#mode-selector button.select-mode-light::before {
		content: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 160c-52.9 0-96 43.1-96 96s43.1 96 96 96 96-43.1 96-96-43.1-96-96-96zm246.4 80.5l-94.7-47.3 33.5-100.4c4.5-13.6-8.4-26.5-21.9-21.9l-100.4 33.5-47.4-94.8c-6.4-12.8-24.6-12.8-31 0l-47.3 94.7L92.7 70.8c-13.6-4.5-26.5 8.4-21.9 21.9l33.5 100.4-94.7 47.4c-12.8 6.4-12.8 24.6 0 31l94.7 47.3-33.5 100.5c-4.5 13.6 8.4 26.5 21.9 21.9l100.4-33.5 47.3 94.7c6.4 12.8 24.6 12.8 31 0l47.3-94.7 100.4 33.5c13.6 4.5 26.5-8.4 21.9-21.9l-33.5-100.4 94.7-47.3c13-6.5 13-24.7.2-31.1zm-155.9 106c-49.9 49.9-131.1 49.9-181 0-49.9-49.9-49.9-131.1 0-181 49.9-49.9 131.1-49.9 181 0 49.9 49.9 49.9 131.1 0 181z"></path></svg>');
		opacity: 0.45;
		width: 1.3em;
		padding: 0;
		left: -15px;
	}
	#mode-selector button.select-mode-dark {
		margin-left: 1.125em;
	}
	#mode-selector button.select-mode-dark::before {
		content: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M283.211 512c78.962 0 151.079-35.925 198.857-94.792 7.068-8.708-.639-21.43-11.562-19.35-124.203 23.654-238.262-71.576-238.262-196.954 0-72.222 38.662-138.635 101.498-174.394 9.686-5.512 7.25-20.197-3.756-22.23A258.156 258.156 0 0 0 283.211 0c-141.309 0-256 114.511-256 256 0 141.309 114.511 256 256 256z"></path></svg>');
		width: 1.15em;
		left: -12px;
	}
	#mode-selector button:not(.selected):hover::before {
		opacity: 1.0;
	}
	#mode-selector button:hover,
	#mode-selector button.selected {
		box-shadow:
			0 2px 0 6px var(--GW-mode-selector-background-color) inset,
			0 1px 0 6px currentColor inset;
	}
	#mode-selector button:not(:disabled):hover {
		color: var(--GW-mode-selector-button-hover-text-color);
		cursor: pointer;
	}
	#mode-selector button:not(:disabled):active {
		transform: translateY(2px);
		box-shadow:
			0 0px 0 6px var(--GW-mode-selector-background-color) inset,
			0 -1px 0 6px currentColor inset;
	}
	#mode-selector button.active:not(:hover)::after {
		content: "";
		position: absolute;
		bottom: 0.25em;
		left: 0;
		right: 0;
		border-bottom: 1px dotted currentColor;
		width: calc(100% - 12px);
		margin: auto;
	}
	@media only screen and (max-width: 1535px) {
		#mode-selector {
			flex-flow: column;
			padding: 0.1em 0.1em 0.2em 0.15em;
			align-items: flex-start;
		}
		#mode-selector button + button {
			margin-top: 0.25em;
		}
		#mode-selector button.select-mode-light::before {
			left: -16px;
		}
		#mode-selector button.select-mode-dark {
			margin-left: 1.375em;
		}
		#mode-selector button.select-mode-dark::before {
			left: -15px;
		}
	}
	@media only screen and (max-width: 1279px) {
		#mode-selector {
			padding: 0.1em 0.1em 0.25em 0.15em;
		}
		#mode-selector button[class^='select-mode-'] {
			color: transparent;
			margin: 0;
			padding: 0;
			width: 2em;
			height: 2em;
			align-items: center;
			box-shadow: none;
			opacity: 0.55;
		}
		#mode-selector  button[class^='select-mode-'] + button {
			margin-top: 0.5em;
		}
		#mode-selector button[class^='select-mode-']::before {
			left: 12.5%;
			width: 75%;
			height: 75%;
		}
		#mode-selector button:not(:disabled):hover {
			color: transparent;
		}
		#mode-selector button:hover,
		#mode-selector button.selected {
			opacity: 1.0;
		}
	}
    </style><link rel="prefetch" href="https://artbreeder.com/"><style type="text/css">[contenteditable=true]:active,[contenteditable=true]:focus{outline:thin solid #00b977;background-color:rgba(0,185,119,.05)}.cyxy-target-popup{padding:1.3rem 12px;position:absolute;display:inline-flex;flex-direction:row;overflow:scroll;vertical-align:middle;z-index:199099;top:1px;left:1px;background:#fff;opacity:.98;height:auto;width:auto;border:1px solid #e6e6e6;box-shadow:0 0 8px 0 rgba(0,0,0,.13);border-radius:5px}@media (max-width:468px){.cyxy-target-popup{left:10%}}#cyxy-popup-left-slide{margin-right:14px}#cyxy-popup-left-slide,#cyxy-popup-right-slide{height:22px;display:inline;vertical-align:middle;cursor:pointer}#cyxy-popup-right-slide{margin-left:0}#cyxy-popup-userinfo{display:inline}.cyxy-target-count{display:inline;vertical-align:middle;font-size:10px}#cyxy-popup-avatar{display:inline;height:32px;vertical-align:middle;border-radius:16px}#cyxy-popup-name-time{display:inline-flex;flex-direction:column;vertical-align:middle;text-align:left;margin-left:6px}#cyxy-popup-name{font-size:14px;color:#333;height:18px;overflow:hidden;max-width:84px}#cyxy-popup-time{font-size:12px;margin-top:4px;color:#999}.cyxy-footer{display:none;position:fixed;bottom:0;padding:0;left:0;right:0;margin:auto;opacity:.9;border:1px solid #e6e6e6;box-shadow:0 0 8px 0 rgba(0,0,0,.13);border-radius:2px;z-index:201712;text-align:center}.cyxy-footer-p{padding:12px 0;margin:0;font-size:12px;color:#333;background:#fff;text-align:center;line-height:1.6;font-weight:200}#cyxy-popup-favour{text-align:center;display:inline;margin-right:20px;margin-left:46px;cursor:pointer}#cyxy-popup-oppose{text-align:center;display:inline;cursor:pointer}#cyxy-popup-favour-img{display:inline;height:20px;vertical-align:middle}#cyxy-popup-oppose-img{display:inline;height:18px;vertical-align:middle}#cyxy-popup-favour-num,#cyxy-popup-oppose-num{font-size:14px;margin-left:4px;color:#999}@media (max-width:320px){#cyxy-popup-favour{margin-right:.8rem;margin-left:1.5rem}#cyxy-popup-left-slide{margin-right:.8rem}#cyxy-popup-right-slide{margin-left:1rem}}.layui-m-layer{position:relative;z-index:19891014}.layui-m-layer *{box-sizing:content-box}.layui-m-layermain,.layui-m-layershade{position:fixed;left:0;top:0;width:100%;height:100%}.layui-m-layershade{background-color:rgba(0,0,0,.7);pointer-events:auto}.layui-m-layermain{display:table;font-family:Helvetica,arial,sans-serif;pointer-events:none}.layui-m-layermain .layui-m-layersection{display:table-cell;vertical-align:middle;text-align:center}.layui-m-layerchild{position:relative;display:inline-block;text-align:left;background-color:#fff;font-size:14px;border-radius:5px;box-shadow:0 0 8px rgba(0,0,0,.1);pointer-events:auto;-webkit-overflow-scrolling:touch;animation-fill-mode:both;animation-duration:.2s}@keyframes layui-m-anim-scale{0%{opacity:0;transform:scale(.5)}to{opacity:1;transform:scale(1)}}.layui-m-anim-scale{animation-name:layui-m-anim-scale;-webkit-animation-name:layui-m-anim-scale}@keyframes layui-m-anim-up{0%{opacity:0;transform:translateY(800px)}to{opacity:1;transform:translateY(0)}}.layui-m-anim-up{animation-name:layui-m-anim-up}.layui-m-layer0 .layui-m-layerchild{width:90%;max-width:640px}.layui-m-layer1 .layui-m-layerchild{border:none;border-radius:0}.layui-m-layer2 .layui-m-layerchild{width:auto;max-width:260px;min-width:40px;border:none;background:0 0;box-shadow:none;color:#fff}.layui-m-layerchild h3{padding:0 10px;height:60px;line-height:60px;font-size:16px;font-weight:400;border-radius:5px 5px 0 0;text-align:center}.layui-m-layerbtn span,.layui-m-layerchild h3{text-overflow:ellipsis;overflow:hidden;white-space:nowrap}.layui-m-layercont{padding:50px 30px;line-height:22px;text-align:center}.layui-m-layer1 .layui-m-layercont{padding:0;text-align:left}.layui-m-layer2 .layui-m-layercont{text-align:center;padding:0;line-height:0}.layui-m-layer2 .layui-m-layercont i{width:25px;height:25px;margin-left:8px;display:inline-block;background-color:#fff;border-radius:100%;animation:layui-m-anim-loading 1.4s infinite ease-in-out;animation-fill-mode:both}.layui-m-layerbtn,.layui-m-layerbtn span{position:relative;text-align:center;border-radius:0 0 5px 5px}.layui-m-layer2 .layui-m-layercont p{margin-top:20px}@keyframes layui-m-anim-loading{0%,80%,to{transform:scale(0);-webkit-transform:scale(0)}40%{transform:scale(1);-webkit-transform:scale(1)}}.layui-m-layer2 .layui-m-layercont i:first-child{margin-left:0;animation-delay:-.32s}.layui-m-layer2 .layui-m-layercont i.layui-m-layerload{animation-delay:-.16s}.layui-m-layer2 .layui-m-layercont>div{line-height:22px;padding-top:7px;margin-bottom:20px;font-size:14px}.layui-m-layerbtn{display:box;display:-moz-box;display:-webkit-box;width:100%;height:50px;line-height:50px;font-size:0;border-top:1px solid #d0d0d0;background-color:#f2f2f2}.layui-m-layerbtn span{display:block;box-flex:1;-webkit-box-flex:1;font-size:14px;cursor:pointer}.layui-m-layerbtn span[yes]{color:#40affe}.layui-m-layerbtn span[no]{border-right:1px solid #d0d0d0;border-radius:0 0 0 5px}.layui-m-layerbtn span:active{background-color:#f6f6f6}.layui-m-layerend{position:absolute;right:7px;top:10px;width:30px;height:30px;border:0;font-weight:400;background:0 0;cursor:pointer;-webkit-appearance:none;font-size:30px}.layui-m-layerend:after,.layui-m-layerend:before{position:absolute;left:5px;top:15px;content:"";width:18px;height:1px;background-color:#999;transform:rotate(45deg);-webkit-transform:rotate(45deg);border-radius:3px}.layui-m-layerend:after{transform:rotate(-45deg);-webkit-transform:rotate(-45deg)}body .layui-m-layer .layui-m-layer-footer{position:fixed;width:95%;max-width:100%;margin:0 auto;left:0;right:0;bottom:10px;background:0 0}.layui-m-layer-footer .layui-m-layercont{padding:20px;border-radius:5px 5px 0 0;background-color:hsla(0,0%,100%,.8)}.layui-m-layer-footer .layui-m-layerbtn{display:block;height:auto;background:0 0;border-top:none}.layui-m-layer-footer .layui-m-layerbtn span{background-color:hsla(0,0%,100%,.8)}.layui-m-layer-footer .layui-m-layerbtn span[no]{color:#fd482c;border-top:1px solid #c2c2c2;border-radius:0 0 5px 5px}.layui-m-layer-footer .layui-m-layerbtn span[yes]{margin-top:10px;border-radius:5px}body .layui-m-layer .layui-m-layer-msg{width:auto;max-width:90%;margin:0 auto;bottom:-150px;background-color:rgba(0,0,0,.7);color:#fff}.layui-m-layer-msg .layui-m-layercont{padding:10px 20px}.cyxy-function{bottom:140px}.cyxy-function,.cyxy-personal{position:fixed;right:20px;z-index:109999;cursor:pointer}.cyxy-personal{bottom:190px}.cyxy-personal .cyxy-favorite-btn{border:2px solid #5ebb8d;box-sizing:border-box}.cyxy-switch{position:relative;display:inline-block;width:54px;height:28px}.cyxy-switch input{display:none}.slider{cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#ccc}.slider,.slider:before{position:absolute;transition:.4s}.slider:before{content:"";height:20px;width:20px;left:4px;bottom:4px;background-color:#fff}input:checked+.slider{background-color:#00b976}input:focus+.slider{box-shadow:0 0 1px #00b976}input:checked+.slider:before{transform:translateX(26px)}.cyxy-favorite{position:fixed;bottom:90px;right:20px;z-index:109999;cursor:pointer}.cyxy-favorite-btn{height:36px;width:36px;border-radius:50%;overflow:hidden}.slider.round{border-radius:34px}.slider.round:before{border-radius:50%}.collection-success,.collection-success:hover{color:#fff}.layui-m-layercont .cyxy-trs-target{display:none}.collection-icon{width:12px;height:13px;background:url("//staging.caiyunapp.com/imgs/layar-target.png") no-repeat;display:inline-block;background-size:cover;background-position:50%}.collection-success>a{margin-left:12px;vertical-align:middle}.cy_free_box{position:relative}.cy_free_box img{width:100%;cursor:pointer;-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.cy_free_button img{width:40%;margin:0 10px}.cy_free_del{position:absolute;width:6%;height:6%;right:0;top:0;cursor:pointer;z-index:10}.layui-m-layer-cy_free_content{background:inherit!important}</style><style>.cyxy-video-trans {
        position: fixed;
        bottom: 90px;
        right: 20px;
        z-index: 109999;
        cursor: pointer;
        filter: grayscale(100%);
    }
    .cyxy-video-trans-btn {
        height: 36px !important;
        width: 36px !important;
        border-radius: 50%;
        overflow: hidden;
    }</style><style>
    .lang_radio {
        display: none;
    }
    .lang_radio_img {
        vertical-align: unset !important;
        width: 13px !important;
        height: 13px !important;
        margin-top: 0px !important;
        cursor: pointer;
        margin-right: 3px !important;
        margin-top: 4px !important;
        display: inline;
        min-height: unset !important;
        min-width: unset !important;
    }
</style><link rel="prefetch" href="https://github.com/NVlabs/stylegan"></head>
  <body class="faces drop-caps-kanzlei">

    <main>
      <div id="sidebar">
        <a id="logo" rel="home me contents" href="https://www.gwern.net/index" class="has-content spawns-popup" data-attribute-title="Homepage: categorized list of articles index">
          <svg viewBox="0 0 64 75" alt="Gwern.net Site Logo (a Gothic/Fraktur blackletter capital G)"><use href="/static/img/logo/logo-smooth.svg#logo"></use></svg>
        </a>
        <div id="sidebar-links">
        <a class="site has-content spawns-popup cyxy-trs-source cyxy-trs-source-ted" href="https://www.gwern.net/About" rel="author" data-attribute-title="Site ideals, source, content, traffic, examples, license">Site<font class="cyxy-trs-target"> 网站</font></a>
        <a class="links has-content spawns-popup cyxy-trs-source cyxy-trs-source-ted" href="https://www.gwern.net/Links" rel="author" data-attribute-title="Who am I online, what have I done, what am I like? Contact information; sites I use; things I&#39;ve worked on">Me<font class="cyxy-trs-target"> 我</font></a>
        <a class="new has-content spawns-popup cyxy-trs-source cyxy-trs-source-ted" href="https://www.gwern.net/Changelog" data-attribute-title="Changelog of what&#39;s new or updated">Changes<font class="cyxy-trs-target"> 改变</font></a>
        <a class="mail no-popup cyxy-trs-source cyxy-trs-source-ted" href="https://gwern.substack.com/" title="Monthly mailing list: newsletter signup form" rel="me">News<font class="cyxy-trs-target"> 新闻</font></a>
        <a class="patreon no-popup cyxy-trs-source" href="https://www.patreon.com/gwern" title="Link to Patreon donation profile to support my writing" rel="me">support on<font class="cyxy-trs-target"> 支持</font><br><span class="cyxy-trs-source">PATREON</span></a>
        </div>
      </div>

      <header>
        <h1 class="cyxy-trs-source cyxy-trs-source-ted">Making Anime Faces With StyleGAN</h1><h1 class="cyxy-trs-source cyxy-trs-target" contenteditable="true">用时尚造型做动漫脸</h1>
      </header>

      <article>

        
        <div id="page-metadata" class="markdownBody">
          <p class="cyxy-trs-source cyxy-trs-source-ted">
            <span id="page-description" title="Short description of page.">A tutorial explaining how to train and generate high-quality anime faces with StyleGAN 1/2 neural networks, and tips/scripts for effective StyleGAN use.</span>
            <br>
            <span id="page-tags" title="List of tags for this page."><em><a href="https://www.gwern.net/tags/anime" class="has-content spawns-popup" data-attribute-title="All pages tagged &#39;anime&#39;.">anime</a>⁠, <a href="https://www.gwern.net/tags/NGE" class="has-content spawns-popup" data-attribute-title="All pages tagged &#39;NGE&#39;.">NGE</a>⁠, <a href="https://www.gwern.net/tags/NN" class="has-content spawns-popup" data-attribute-title="All pages tagged &#39;NN&#39;.">NN</a>⁠, <a href="https://www.gwern.net/tags/Python" class="has-content spawns-popup" data-attribute-title="All pages tagged &#39;Python&#39;.">Python</a>⁠, <a href="https://www.gwern.net/tags/technology" class="has-content spawns-popup" data-attribute-title="All pages tagged &#39;technology&#39;.">technology</a>⁠, <a href="https://www.gwern.net/tags/tutorial" class="has-content spawns-popup" data-attribute-title="All pages tagged &#39;tutorial&#39;.">tutorial</a></em></span>
            <br>
            <span id="page-metadata-block">
              
              <span id="page-date-range"><span id="page-creation" title="Date page contents were begun."><em>2019-02-04</em></span>–<span id="page-source"><a title="Link to latest revision of the raw Markdown text source for this page, &#39;Making Anime Faces With StyleGAN&#39;: /Faces.page" href="https://www.gwern.net/Faces.page" class="no-popup"><span id="page-modified" title="Date of last major modification to this page."><em>2021-01-30</em></span></a></span></span>
              <span id="page-status" title="Writing status of current page: ranges &#39;abandoned&#39;/&#39;notes&#39;/&#39;draft&#39;/&#39;in progress&#39;/&#39;finished&#39;"><em>finished</em></span>
              <span id="page-confidence"><a href="https://www.gwern.net/About#confidence-tags" class="has-content spawns-popup" data-attribute-title="Explanation of &#39;confidence&#39; metadata: probability of overall being meaningfully correct, expressed as Kesselman Estimative Words (ranging 0–100%: &#39;certain&#39;/&#39;highly likely&#39;/&#39;likely&#39;/&#39;possible&#39;/&#39;unlikely&#39;/&#39;highly unlikely&#39;/&#39;remote&#39;/&#39;impossible&#39;)">certainty</a>: <em>highly likely</em></span>
              <span id="page-importance"><a href="https://www.gwern.net/About#importance-tags" class="has-content spawns-popup" data-attribute-title="Explanation of &#39;importance&#39; metadata: rating 1–10 about how much a topic matters to the world.">importance</a>: <em>5</em></span>
            </span>
          </p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">一个教程解释如何训练和生成高质量的动漫脸与 StyleGAN 1/2神经网络，提示/脚本有效的 StyleGAN 使用。动漫，NGE，NN，Python，技术，教程2019-02-04-2021-01-30完成确定性: 非常可能的重要性: 5</p>
          <hr>
        </div>

        <noscript><div class="admonition warning"><div class="admonition-title">JS Disabled</div> For support of <a href="/About#design" title="About: Gwern.net Design: principles, features, links, tricks">website features</a> (table-sorting/collapsible-sections/image-zooms/link-annotations/floating-footnotes/Disqus-comments), please enable JavaScript.</div></noscript>
        

        <div id="TOC"><ul>
<li><a href="https://www.gwern.net/Faces#examples" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Examples<font class="cyxy-trs-target"> 例子</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#background" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Background<font class="cyxy-trs-target"> 背景</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#applications" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Applications<font class="cyxy-trs-target"> 申请</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#why-dont-gans-work" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Why Don’t GANs Work?<font class="cyxy-trs-target" contenteditable="true"> 为什么甘斯不工作？</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#faq" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">FAQ<font class="cyxy-trs-target"> 常见问题</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#copyright" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Copyright<font class="cyxy-trs-target"> 版权所有</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#training-requirements" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Training requirements<font class="cyxy-trs-target"> 培训要求</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#data" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Data<font class="cyxy-trs-target" contenteditable="true"> 境监察及审核资料</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#compute" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Compute<font class="cyxy-trs-target"> 计算</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#data-preparation" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Data Preparation<font class="cyxy-trs-target"> 数据准备</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#faces-preparation" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Faces preparation<font class="cyxy-trs-target"> 面部准备</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#cropping" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Cropping<font class="cyxy-trs-target"> 种植</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#cleaning-upscaling" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Cleaning &amp; Upscaling<font class="cyxy-trs-target" contenteditable="true"> 清洁和升级</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#discriminator-ranking" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Discriminator ranking<font class="cyxy-trs-target" contenteditable="true"> 识别器排名</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#upscaling" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Upscaling<font class="cyxy-trs-target"> 扩大规模</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#quality-checks-data-augmentation" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Quality Checks &amp; Data Augmentation<font class="cyxy-trs-target" contenteditable="true"> 质量检查和数据增强</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#upscaling-conversion" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Upscaling &amp; Conversion<font class="cyxy-trs-target" contenteditable="true"> 升级和转换</font></span></a></li>
</ul></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#training" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Training<font class="cyxy-trs-target"> 培训</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#installation" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Installation<font class="cyxy-trs-target"> 安装</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#configuration" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Configuration<font class="cyxy-trs-target"> 配置</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#running" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Running<font class="cyxy-trs-target"> 跑步</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#sampling" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Sampling<font class="cyxy-trs-target"> 抽取样本</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#psitruncation-trick" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Psi/“truncation trick”<font class="cyxy-trs-target" contenteditable="true"> 超自然力/”截断技巧”</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#random-samples" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Random Samples<font class="cyxy-trs-target"> 随机样本</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#karras-et-al-2018-figures" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Karras&nbsp;et&nbsp;al&nbsp;2018 Figures<font class="cyxy-trs-target" contenteditable="true"> 卡拉斯等人2018年数据</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#videos" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Videos<font class="cyxy-trs-target"> 视频</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#training-montage" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Training Montage<font class="cyxy-trs-target" contenteditable="true"> 训练蒙太奇</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#interpolations" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Interpolations<font class="cyxy-trs-target"> 插入</font></span></a></li>
</ul></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#models" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Models<font class="cyxy-trs-target"> 模特</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#anime-faces" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces<font class="cyxy-trs-target"> 动画人物</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#twdne" class="has-content spawns-popup"><span class="cyxy-trs-source">TWDNE</span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#anime-bodies" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Bodies<font class="cyxy-trs-target"> 动漫人体</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#conditional-anime-faces-arfafax" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Conditional Anime Faces, Arfafax<font class="cyxy-trs-target" contenteditable="true"> 有条件的动画面孔</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#conditional-gan-problems" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Conditional GAN Problems<font class="cyxy-trs-target" contenteditable="true"> 有条件的 GAN 问题</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#tag-face-usage" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Tag → Face Usage<font class="cyxy-trs-target" contenteditable="true"> 标签→面部使用</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#extended-stylegan2-danbooru2019-aydao" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Extended StyleGAN2 Danbooru2019, Aydao<font class="cyxy-trs-target" contenteditable="true"> 延伸 StyleGAN2 Danbooru2019，Aydao</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#stylegan2-ext-modifications" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">StyleGAN2-ext Modifications<font class="cyxy-trs-target" contenteditable="true"> StyleGAN2-ext 修改程序</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#tadne-training" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">TADNE Training<font class="cyxy-trs-target"> 训练</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#tadne-download" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">TADNE Download<font class="cyxy-trs-target"> 下载</font></span></a></li>
</ul></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#transfer-learning" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Transfer Learning<font class="cyxy-trs-target"> 迁移学习</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#anime-faces-character-faces" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → Character Faces<font class="cyxy-trs-target" contenteditable="true"> 动漫人脸→人物人脸</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#holo" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Holo<font class="cyxy-trs-target"> 全息图</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#asuka" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Asuka<font class="cyxy-trs-target" contenteditable="true"> 女名女子名</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#zuihou" class="has-content spawns-popup"><span class="cyxy-trs-source">Zuihou</span></a></li>
<li><a href="https://www.gwern.net/Faces#ganso" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Ganso<font class="cyxy-trs-target" contenteditable="true"> 女名女子名</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#akizuki" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Akizuki<font class="cyxy-trs-target"> 秋月</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#ptilopsis" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Ptilopsis<font class="cyxy-trs-target" contenteditable="true"> 女名女子名</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#fate" class="has-content spawns-popup"><span><em class="cyxy-trs-source">Fate<font class="cyxy-trs-target"> 命运</font></em></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#saber" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Saber<font class="cyxy-trs-target" contenteditable="true"> 女名女子名</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#fategrand-order" class="has-content spawns-popup"><span><em class="cyxy-trs-source">Fate/Grand Order<font class="cyxy-trs-target" contenteditable="true"> 命运/大秩序</font></em></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#louise" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Louise<font class="cyxy-trs-target"> 露易丝</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#lelouch" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Lelouch<font class="cyxy-trs-target" contenteditable="true"> 男名男子名</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#asashio" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Asashio<font class="cyxy-trs-target"> 足尾</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#marisa-kirisame-the-komeijis" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Marisa Kirisame &amp; the Komeijis<font class="cyxy-trs-target" contenteditable="true"> 玛丽莎 · 基里萨姆和公明党</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#lexington" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Lexington<font class="cyxy-trs-target"> 列克星敦</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#hayasaka-ai" class="has-content spawns-popup"><span class="cyxy-trs-source">Hayasaka Ai</span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#ahegao" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Ahegao<font class="cyxy-trs-target" contenteditable="true"> 女名女子名</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#emilia-rezero" class="has-content spawns-popup"><span class="cyxy-trs-source">Emilia (<font class="cyxy-trs-target" contenteditable="true"> 艾米莉亚(</font><em class="cyxy-trs-source">Re:Zero<font class="cyxy-trs-target" contenteditable="true"> 回应: 零</font></em>)</span></a></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-anime-headshots" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → Anime Headshots<font class="cyxy-trs-target" contenteditable="true"> 动漫人脸→动漫大头照</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-portrait" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → Portrait<font class="cyxy-trs-target" contenteditable="true"> 动画人脸→肖像</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#portrait-improvements" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Portrait Improvements<font class="cyxy-trs-target"> 人像改进</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#portrait-results" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Portrait Results<font class="cyxy-trs-target"> 人像结果</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-male-faces" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → Male Faces<font class="cyxy-trs-target" contenteditable="true"> 动漫面孔→男性面孔</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-ukiyo-e-faces" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source">Anime Faces → <font class="cyxy-trs-target" contenteditable="true"> 动漫面孔→</font><em class="cyxy-trs-source">Ukiyo-e<font class="cyxy-trs-target"> 浮世绘</font></em> Faces<font class="cyxy-trs-target"> 面孔</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-western-portrait-faces" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → Western Portrait Faces<font class="cyxy-trs-target" contenteditable="true"> 动漫人脸→西方人物肖像</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-danbooru2018" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → Danbooru2018<font class="cyxy-trs-target" contenteditable="true"> 动漫面孔→丹伯鲁2018</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#ffhq-variations" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">FFHQ Variations<font class="cyxy-trs-target"> 变化</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#anime-faces-ffhq-faces" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → FFHQ Faces<font class="cyxy-trs-target" contenteditable="true"> 动漫面孔→ FFHQ 面孔</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-anime-faces-ffhq-faces" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces → Anime Faces + FFHQ Faces<font class="cyxy-trs-target" contenteditable="true"> 动漫人脸→动漫人脸 + FFHQ 人脸</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#anime-faces-ffhq-danbooru2018" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Anime Faces + FFHQ → Danbooru2018<font class="cyxy-trs-target" contenteditable="true"> 动漫面孔 + FFHQ → Danbooru2018</font></span></a></li>
</ul></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Reversing StyleGAN To Control &amp; Modify Images<font class="cyxy-trs-target" contenteditable="true"> 扭转 StyleGAN 控制和修改图像</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#editing-rare-attributes" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Editing Rare Attributes<font class="cyxy-trs-target" contenteditable="true"> 编辑稀有属性</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#stylegan-2" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">StyleGAN 2<font class="cyxy-trs-target"> 2</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#running-s2" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Running S2<font class="cyxy-trs-target"> 跑步中二</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#future-work" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Future Work<font class="cyxy-trs-target"> 未来工作</font></span></a>
<ul>
<li><a href="https://www.gwern.net/Faces#imagenet-stylegan" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">ImageNet StyleGAN<font class="cyxy-trs-target" contenteditable="true"> 图片来源: imageet StyleGAN</font></span></a></li>
</ul></li>
<li><a href="https://www.gwern.net/Faces#biggan" class="has-content spawns-popup"><span class="cyxy-trs-source">BigGAN</span></a></li>
<li><a href="https://www.gwern.net/Faces#see-also" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">See Also<font class="cyxy-trs-target"> 参见</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#external-links" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">External Links<font class="cyxy-trs-target"> 外部链接</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#appendix" class="has-content spawns-popup"><span class="cyxy-trs-source cyxy-trs-source-ted">Appendix<font class="cyxy-trs-target"> 附录</font></span></a></li>
<li><a href="https://www.gwern.net/Faces#footnotes" class="has-content spawns-popup"><span class="cyxy-trs-source">Footnotes</span></a></li>
</ul></div>
<div id="markdownBody" class="markdownBody"><div class="abstract">
<blockquote>
<p class="cyxy-trs-source cyxy-trs-source-ted">Gen­er­a­tive neural net­works, such as <span class="smallcaps-auto">GAN</span>s, have <a href="https://www.gwern.net/Faces#why-dont-gans-work" class="link-self identifier-link-down has-content spawns-popup">strug­gled for years</a> to gen­er­ate de­cen­t-qual­ity anime faces, de­spite their great suc­cess with pho­to­graphic im­agery such as real hu­man faces. The task has now been effec­tively solved, for anime faces as well as many other do­mains, by the de­vel­op­ment of a new gen­er­a­tive ad­ver­sar­ial net­work, <a href="https://arxiv.org/abs/1812.04948" id="karras-et-al-2018-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;A Style-Based Generator Architecture for Generative Adversarial Networks&#39;, Karras et al 2018"><em>Style<span class="smallcaps-auto">GAN</span></em></a>⁠, whose <a href="https://github.com/NVlabs/stylegan" class="no-popup">source code</a> was re­leased in Feb­ru­ary 2019.</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">多年来，生成神经网络(比如 GANs)一直在努力生成高质量的动画脸，尽管它们在生成真人脸等摄影图像方面取得了巨大成功。这项任务现在已经有效地解决了，动漫的面孔以及许多其他领域，通过开发一个新的生成对抗性网络，StyleGAN，其源代码发布于2019年2月。</p>
<p class="cyxy-trs-source cyxy-trs-source-ted">I <a href="https://www.gwern.net/Faces#examples" class="link-self identifier-link-down has-content spawns-popup">show off</a> my Style<span class="smallcaps-auto">GAN</span> 1/<wbr>2 CC-0-li­censed anime faces &amp; videos, pro­vide down­loads for the fi­nal mod­els &amp; <a href="https://www.gwern.net/Crops#danbooru2019-portraits" id="gwern-crops-danbooru2019-portraits" class="link-local docMetadata has-annotation spawns-popup">anime por­trait face dataset</a>⁠, pro­vide the ‘miss­ing man­ual’ &amp; ex­plain how I trained them based on <a href="https://www.gwern.net/Danbooru2020#danbooru2018" class="link-local has-content spawns-popup">Dan­booru2017/<wbr>2018</a> with source code for the <a href="https://www.gwern.net/Faces#data-preparation" class="link-self identifier-link-down has-content spawns-popup">data pre­pro­cess­ing</a>⁠, doc­u­ment <a href="https://www.gwern.net/Faces#installation" class="link-self identifier-link-down has-content spawns-popup">in­stal­la­tion</a> &amp; <a href="https://www.gwern.net/Faces#configuration" class="link-self identifier-link-down has-content spawns-popup">con­fig­u­ra­tion</a> &amp; <a href="https://www.gwern.net/Faces#running" class="link-self identifier-link-down has-content spawns-popup">train­ing tricks</a>⁠.</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">我展示了我的 StyleGAN 1/2 cc-0授权的动漫面孔和视频，提供了最终模型和动漫人像面孔数据集的下载，提供了“缺失手册”，并解释了我如何根据 Danbooru2017/2018训练他们与数据预处理，文件安装和配置及培训技巧的源代码。</p>
<p class="cyxy-trs-source cyxy-trs-source-ted">For ap­pli­ca­tion, I doc­u­ment var­i­ous scripts for gen­er­at­ing <a href="https://www.gwern.net/Faces#sampling" class="link-self identifier-link-down has-content spawns-popup">im­ages &amp; videos</a>⁠, briefly <a href="https://www.gwern.net/Faces#twdne" class="link-self identifier-link-down has-content spawns-popup">de­scribe the web­site</a> <a href="https://www.thiswaifudoesnotexist.net/" id="gwern-www-thiswaifudoesnotexist-net" class="docMetadata has-annotation spawns-popup">“This Waifu Does Not Ex­ist”</a> <a href="https://www.gwern.net/TWDNE" id="gwern-twdne" class="link-local docMetadata has-annotation spawns-popup">I set up</a> as a pub­lic demo &amp; <a href="https://www.gwern.net/Faces#extended-stylegan2-danbooru2019-aydao" class="link-self identifier-link-down has-content spawns-popup">its fol­lowup</a> <a href="https://thisanimedoesnotexist.ai/" id="nearcyan-et-al-2021" class="docMetadata has-annotation spawns-popup">This Anime Does Not Ex­ist.ai (<span class="smallcaps-auto">TADNE</span>)</a> (see also <a href="https://artbreeder.com/" id="simon-2019" class="docMetadata has-annotation spawns-popup">Art­breeder</a>), dis­cuss how the trained mod­els can be <a href="https://www.gwern.net/Faces#transfer-learning" class="link-self identifier-link-down has-content spawns-popup">used for trans­fer learn­ing</a> such as gen­er­at­ing high­-qual­ity faces of anime char­ac­ters with small datasets (eg <a href="https://www.gwern.net/Faces#holo" class="link-self identifier-link-down has-content spawns-popup">Holo</a> or <a href="https://www.gwern.net/Faces#asuka" class="link-self identifier-link-down has-content spawns-popup">Asuka Souryuu Lan­g­ley</a>), and touch on <a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images" class="link-self identifier-link-down has-content spawns-popup">more ad­vanced Style<span class="smallcaps-auto">GAN</span> ap­pli­ca­tions</a> like en­coders &amp; con­trol­lable gen­er­a­tion.</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">为了应用，我记录了各种生成图像和视频的脚本，简要描述了我作为一个公开演示及其后续这部动画不存在的网站“ This Waifu Does Not Exist”,讨论训练好的模型如何用于转移学习，例如用小数据集生成高质量的动画人物面孔(例如 Holo 或者 Asuka souyuu Langley) ，以及触及更先进的程序设计应用，例如编码器和可控生成。</p>
<p class="cyxy-trs-source cyxy-trs-source-ted">The <a href="https://www.gwern.net/Faces-graveyard" id="gwern-faces-graveyard" class="link-local docMetadata has-annotation spawns-popup">anime face grave­yard</a> gives sam­ples of my fail­ures with ear­lier <span class="smallcaps-auto">GAN</span>s for anime face gen­er­a­tion, and I pro­vide sam­ples &amp; model from a rel­a­tively large-s­cale <a href="https://www.gwern.net/Faces#biggan" class="link-self identifier-link-down has-content spawns-popup">Big<span class="smallcaps-auto">GAN</span></a> train­ing run sug­gest­ing that Big<span class="smallcaps-auto">GAN</span> may be the next step for­ward to gen­er­at­ing ful­l-s­cale anime im­ages.</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">动画脸墓地提供了我之前在动画脸部生成方面失败的样本，我提供了一个相对大规模的 BigGAN 培训运行的样本和模型，表明 BigGAN 可能是下一步制作全尺寸动画图像的方法。</p>
<p class="cyxy-trs-source cyxy-trs-source-ted">A minute of read­ing could save an hour of de­bug­ging!</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">一分钟的阅读可以节省一个小时的调试时间！</p>
</blockquote>
</div>
<p class="drop-cap-kanzlei cyxy-trs-source cyxy-trs-source-ted">When Ian Good­fel­low’s first <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" class="docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">GAN</span></a> pa­per <a href="https://arxiv.org/abs/1406.2661" id="goodfellow-et-al-2014-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Generative Adversarial Networks&#39;, Goodfellow et al 2014">came out in 2014</a>⁠, with its blurry 64px grayscale faces, I said to my­self, “given the rate at which <span class="smallcaps-auto">GPU</span>s &amp; NN ar­chi­tec­tures im­prove, in a few years, we’ll prob­a­bly be able to throw a few <span class="smallcaps-auto">GPU</span>s at some anime col­lec­tion like Dan­booru and the re­sults will be <em>hi­lar­i­ous</em>.” There is some­thing in­trin­si­cally amus­ing about try­ing to make com­put­ers draw ani­me, and it would be much more fun than work­ing with yet more celebrity head­shots or Im­a­geNet sam­ples; fur­ther, ani­me/<wbr>il­lus­tra­tions/<wbr>­draw­ings are so differ­ent from the ex­clu­sive­ly-pho­to­graphic datasets al­ways (over)used in con­tem­po­rary ML re­search that I was cu­ri­ous how it would work on ani­me—­bet­ter, worse, faster, or differ­ent fail­ure mod­es? Even more amus­ing—if ran­dom im­ages be­come doable, then text → im­ages would not be far be­hind.</p><p class="drop-cap-kanzlei cyxy-trs-source cyxy-trs-target" contenteditable="true">2014年，当伊恩 · 古德菲勒(Ian Goodfellow)的第一篇 GAN 论文发表时，我对自己说，“鉴于 gpu 和 NN 架构的改进速度，再过几年，我们可能会在丹博鲁(Danbooru)这样的动画系列中加入几个 gpu，结果会非常搞笑。”试图让计算机绘制动画本身就有一些有趣的东西，而且这比使用更多的名人头像或 ImageNet 样本要有趣得多;此外，动画/插图/绘图与当代机器学习研究中一直(过度)使用的专有摄影数据集是如此不同，以至于我很好奇它在动画上会如何工作ーー更好、更差、更快还是不同的故障模式？更有趣的是，如果随机图像变得可行，那么文本→图像也不会落后太多。</p>
<p class="cyxy-trs-source cyxy-trs-source-ted">So when <span class="smallcaps-auto">GAN</span>s hit <a href="https://arxiv.org/abs/1606.03498#openai" id="salimans-et-al-2016-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Improved Techniques for Training GANs&#39;, Salimans et al 2016">128px color im­ages on Im­a­geNet</a>⁠, and could do some­what pass­able <a href="https://www.gwern.net/docs/www/mmlab.ie.cuhk.edu.hk/b0d75986063b235a3bc539689b337a389de2eca8.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" data-attribute-title="&#39;Large-scale CelebFaces Attributes (CelebA) Dataset&#39;, Liu et al 2015 (Original URL: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html )">CelebA face sam­ples</a> around 2015, along with my <a href="https://www.gwern.net/RNN-metadata" id="gwern-rnn-metadata" class="link-local docMetadata has-annotation spawns-popup" data-attribute-title="Teaching a text-generating char-RNN to automatically imitate many different authors by labeling the input text by author; additional experiments include imitating Geocities and retraining GPT-2 on a large Project Gutenberg poetry corpus.">char-<span class="smallcaps-auto">RNN</span> ex­per­i­ments</a>⁠, I be­gan ex­per­i­ment­ing with <a href="https://github.com/soumith/dcgan.torch" class="no-popup">Soumith Chin­ta­la’s im­ple­men­ta­tion</a> of <a href="https://arxiv.org/abs/1511.06434" id="radford-et-al-2015" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&#39;, Radford et al 2015"><span class="smallcaps-auto">DCGAN</span></a>⁠, re­strict­ing my­self to faces of sin­gle anime char­ac­ters where I could eas­ily scrape up ~5–10k faces. (I did a lot of <a href="https://en.wikipedia.org/wiki/Asuka_Souryuu_Langley" class="docMetadata has-annotation spawns-popup">Asuka Souryuu Lan­g­ley</a> from <em><a href="https://en.wikipedia.org/wiki/Neon_Genesis_Evangelion" class="docMetadata has-annotation spawns-popup">Neon Gen­e­sis Evan­ge­lion</a></em> be­cause she has a col­or-cen­tric de­sign which made it easy to tell if a <span class="smallcaps-auto">GAN</span> run was mak­ing any pro­gress: blonde-red hair, blue eyes, and red hair or­na­ments.)</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">因此，2015年左右，当 GANs 在 ImageNet 上的彩色图像达到128px，并且可以做一些还过得去的 CelebA 脸部样本，以及我的 char-RNN 实验时，我开始尝试 Soumith Chintala 对 DCGAN 的实现，将自己限制在单个动画角色的脸部上，在那里我可以很容易地搜集到大约5-10k 的脸部。(我在新世纪福音战士做了很多明日香 souyuu Langley 的设计，因为她有一个以颜色为中心的设计，很容易看出 GAN 运行是否有任何进展: 金红色的头发，蓝色的眼睛，红色的发饰。)</p>
<p class="cyxy-trs-source cyxy-trs-source-ted">It did not work. De­spite many runs on my lap­top &amp; a bor­rowed desk­top, <span class="smallcaps-auto">DCGAN</span> never got re­motely near to the level of the CelebA face sam­ples, typ­i­cally top­ping out at red­dish blobs be­fore di­verg­ing or out­right crash­ing.<a href="https://www.gwern.net/Faces#sn1" class="footnote-ref spawns-popup" id="fnref1" role="doc-noteref"><sup>1</sup></a> Think­ing per­haps the prob­lem was too-s­mall datasets &amp; I needed to train on <em>all</em> the faces, I be­gan cre­at­ing the Dan­booru2017 ver­sion of <a href="https://www.gwern.net/Danbooru2020" id="gwern-danbooru2020" class="link-local docMetadata has-annotation spawns-popup" data-attribute-title="Danbooru2018 is a large-scale anime image database with 3.33m+ images annotated with 99.7m+ tags; it can be useful for machine learning purposes such as image recognition and generation">“Dan­booru2018: A Large-S­cale Crowd­sourced and Tagged Anime Il­lus­tra­tion Dataset”</a>⁠. Armed with a large dataset, I sub­se­quently be­gan work­ing through par­tic­u­larly promis­ing mem­bers of the <a href="https://github.com/hindupuravinash/the-gan-zoo" class="no-popup"><span class="smallcaps-auto">GAN</span> zoo</a>⁠, em­pha­siz­ing <span class="smallcaps-auto">SOTA</span> &amp; open im­ple­men­ta­tions.</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">但是没有成功。尽管我的笔记本电脑和借来的台式机上运行了很多次，但是 DCGAN 从来没有达到 CelebA 脸部样品的水平，通常在发散或彻底崩溃之前在红色斑点处达到顶峰。1. 考虑到问题可能在于数据集太小 &amp; 我需要对所有的脸进行训练，我开始创建2017年版的“ Danbooru2018: 大规模众包和标记动画插图数据集”。有了大量的数据集，我随后开始通过 GAN 动物园中特别有前途的成员进行工作，重点是 SOTA 和开放实现。</p>
<p class="cyxy-trs-source cyxy-trs-source-ted">Among oth­ers, <a href="https://nitter.cc/gwern/status/828311639472611328" class="no-popup">I have</a> <a href="https://nitter.cc/gwern/status/828718629181075466" class="no-popup">tried</a> <a href="https://arxiv.org/abs/1612.03242" id="zhang-et-al-2016-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks&#39;, Zhang et al 2016">Stack<span class="smallcaps-auto">GAN</span></a>⁠/ <a href="https://arxiv.org/abs/1710.10916" id="zhang-et-al-2017-0" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks&#39;, Zhang et al 2017">Stack<span class="smallcaps-auto">GAN</span>++</a> &amp; <a href="https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173" title="Auto-Regressive Generative Models (PixelRNN, PixelCNN++)" class="no-popup">Pix­el*NN*</a> (failed to get run­ning)<a href="https://www.gwern.net/Faces#sn2" class="footnote-ref spawns-popup" id="fnref2" role="doc-noteref"><sup>2</sup></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#wgan" class="link-local has-content spawns-popup"><span class="smallcaps-auto">WGAN-GP</span></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#glow" class="link-local has-content spawns-popup">Glow</a>⁠, <a href="https://www.gwern.net/Faces-graveyard#gan-qp" class="link-local has-content spawns-popup"><span class="smallcaps-auto">GAN-QP</span></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#msg-gan" class="link-local has-content spawns-popup"><span class="smallcaps-auto">MSG-GAN</span></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#self-attention-gan-tensorflow" class="link-local has-content spawns-popup"><span class="smallcaps-auto">SAGAN</span></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#vgan" class="link-local has-content spawns-popup"><span class="smallcaps-auto">VGAN</span></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#pokegan" class="link-local has-content spawns-popup">Poke<span class="smallcaps-auto">GAN</span></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#biggan-unofficial" class="link-local has-content spawns-popup">Big<span class="smallcaps-auto">GAN</span></a><a href="https://www.gwern.net/Faces#sn3" class="footnote-ref spawns-popup" id="fnref3" role="doc-noteref"><sup>3</sup></a>⁠, <a href="https://www.gwern.net/Faces-graveyard#progan" class="link-local has-content spawns-popup">Pro<span class="smallcaps-auto">GAN</span></a>⁠, &amp; Style<span class="smallcaps-auto">GAN</span>. These ar­chi­tec­tures vary widely in their de­sign &amp; core al­go­rithms and which of the many sta­bi­liza­tion tricks (<a href="https://arxiv.org/abs/1910.00927" id="wiatrak-et-al-2019-0" class="docMetadata has-annotation spawns-popup" data-attribute-title="Stabilizing Generative Adversarial Network Training: A Survey">Wia­trak &amp; Al­brecht 2019</a>) they use, but they were more sim­i­lar in their re­sults: dis­mal.</p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">其中，我试过 StackGAN/StackGAN + + &amp; Pixel * NN * (未能启动)2，WGAN-GP，Glow，GAN-QP，MSG-GAN，SAGAN，VGAN，PokeGAN，BigGAN3，ProGAN，&amp; StyleGAN。这些架构在其设计和核心算法以及使用的许多稳定技巧(Wiatrak &amp; Albrecht 2019)方面差异很大，但它们的结果更为相似: 令人沮丧。</p>
<p class="cyxy-trs-source">Glow &amp; Big<span class="smallcaps-auto">GAN</span> had promis­ing re­sults re­ported on CelebA &amp; Im­a­geNet re­spec­tive­ly, but un­for­tu­nately their train­ing re­quire­ments were out of the ques­tion.<a href="https://www.gwern.net/Faces#sn4" class="footnote-ref spawns-popup" id="fnref4" role="doc-noteref"><sup>4</sup></a> (As in­ter­est­ing as <a href="https://www.gwern.net/docs/www/deepmind.com/447b506f21f54f1a28dd820b9d028171c05f067a.pdf" id="ganin-et-al-2018" class="docMetadata localArchive has-annotation spawns-popup" rel="archived alternate nofollow" data-url-original="https://deepmind.com/documents/183/SPIRAL.pdf" data-attribute-title="&#39;SPIRAL: Synthesizing Programs for Images using Reinforced Adversarial Learning&#39;, Ganin et al 2018 (Original URL: https://deepmind.com/documents/183/SPIRAL.pdf )"><span class="smallcaps-auto">SPIRAL</span></a> and <a href="https://arxiv.org/abs/1706.07068" id="elgammal-et-al-2017-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;CAN: Creative Adversarial Networks, Generating &#39;Art&#39; by Learning About Styles and Deviating from Style Norms&#39;, Elgammal et al 2017"><span class="smallcaps-auto">CAN</span></a> are, no source was re­leased and I could­n’t even at­tempt them.)</p>
<p class="cyxy-trs-source">While some re­mark­able tools like <a href="https://github.com/lllyasviel/style2paints" id="zhang-et-al-2018" class="docMetadata has-annotation spawns-popup">PaintsTrans­fer­/<wbr>style2­paints</a> were cre­at­ed, and there were the oc­ca­sional semi­-suc­cess­ful anime face <span class="smallcaps-auto">GAN</span>s like <a href="https://github.com/tdrussell/IllustrationGAN" class="no-popup">Illustration<span class="smallcaps-auto">GAN</span></a>⁠, the most no­table at­tempt at anime face gen­er­a­tion was <a href="http://make.girls.moe/#/" class="no-popup">Make Girl­s.­moe</a> (<a href="https://arxiv.org/abs/1708.05509" id="jin-et-al-2017-8" class="docMetadata has-annotation spawns-popup" data-attribute-title="Towards the Automatic Anime Characters Creation with Generative Adversarial Networks">Jin&nbsp;et&nbsp;al&nbsp;2017</a>). <span class="smallcaps-auto">MGM</span> could, in­ter­est­ing­ly, do <em>in­-browser</em> 256px anime face gen­er­a­tion us­ing tiny <span class="smallcaps-auto">GAN</span>s, but that is a dead end. <span class="smallcaps-auto">MGM</span> ac­com­plished that much by mak­ing the prob­lem eas­ier: they added some light su­per­vi­sion in the form of a crude tag em­bed­ding<a href="https://www.gwern.net/Faces#sn5" class="footnote-ref spawns-popup" id="fnref5" role="doc-noteref"><sup>5</sup></a>⁠, and then sim­pli­fy­ing the prob­lem dras­ti­cally to <em>n</em> = 42k faces cropped from pro­fes­sional video game char­ac­ter art­work, which I re­garded as not an ac­cept­able so­lu­tion—the faces were small &amp; bor­ing, and it was un­clear if this data-clean­ing ap­proach could scale to anime faces in gen­er­al, much less anime im­ages in gen­er­al. They are rec­og­niz­ably anime faces but the res­o­lu­tion is low and the qual­ity is not great:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="2017  16 ran­dom Make Girl­s.­Moe face sam­ples (4×4 grid)" height="1040" loading="lazy" sizes="(max-width: 768px) 100vw, 1040px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-18-makegirlsmoe-faces-16randomsamples.jpg" srcset="/images/gan/2019-03-18-makegirlsmoe-faces-16randomsamples.jpg-768px.jpg 768w, /images/gan/2019-03-18-makegirlsmoe-faces-16randomsamples.jpg 1040w" width="1040" class="focusable" accesskey="l"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">2017 <span class="smallcaps-auto cyxy-trs-source">SOTA</span>: 16 ran­dom Make Girl­s.­Moe face sam­ples (4×4 grid)</figcaption></span></span></figure>
<p class="cyxy-trs-source">Typ­i­cal­ly, a <span class="smallcaps-auto">GAN</span> would di­verge after a day or two of train­ing, or it would col­lapse to pro­duc­ing a lim­ited range of faces (or a sin­gle face), or if it was sta­ble, sim­ply con­verge to a low level of qual­ity with a lot of fuzzi­ness; per­haps the most typ­i­cal fail­ure mode was het­e­rochro­mia (which is <a href="https://safebooru.org/index.php?page=post&amp;s=list&amp;tags=heterochromia" class="no-popup">com­mon in anime</a> but not <em>that</em> com­mon)—mis­matched eye col­ors (each color in­di­vid­u­ally plau­si­ble), from the Gen­er­a­tor ap­par­ently be­ing un­able to co­or­di­nate with it­self to pick con­sis­tent­ly. With more re­cent ar­chi­tec­tures like <span class="smallcaps-auto">VGAN</span> or <span class="smallcaps-auto">SAGAN</span>, which care­fully weaken the Dis­crim­i­na­tor or which add ex­treme­ly-pow­er­ful com­po­nents like self­-at­ten­tion lay­ers, I could reach fuzzy 128px faces.</p>
<p class="cyxy-trs-source">Given the mis­er­able fail­ure of all the prior NNs I had tried, I had be­gun to se­ri­ously won­der if there was some­thing about non-pho­tographs which made them in­trin­si­cally un­able to be eas­ily mod­eled by con­vo­lu­tional neural net­works (the com­mon in­gre­di­ent to them al­l). Did con­vo­lu­tions ren­der it un­able to gen­er­ate sharp lines or flat re­gions of col­or? Did <a href="https://www.gwern.net/Faces#why-dont-gans-work" class="link-self identifier-link-down has-content spawns-popup">reg­u­lar <span class="smallcaps-auto">GAN</span>s work only</a> be­cause pho­tographs were made al­most en­tirely of blurry tex­tures?</p>
<p class="cyxy-trs-source">But Big<span class="smallcaps-auto">GAN</span> demon­strated that a large cut­ting-edge <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture could scale, given enough train­ing, to <em>all of Im­a­geNet</em> at even 512px. And <a href="https://www.gwern.net/Faces-graveyard#progan" class="link-local has-content spawns-popup">Pro<span class="smallcaps-auto">GAN</span></a> demon­strated that reg­u­lar <span class="smallcaps-auto">CNN</span>s <em>could</em> learn to gen­er­ate sharp clear anime im­ages with only <em>some­what</em> in­fea­si­ble amounts of train­ing. <a href="https://arxiv.org/abs/1710.10196" id="karras-et-al-2017-0" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;ProGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation&#39;, Karras et al 2017">Pro<span class="smallcaps-auto">GAN</span></a> (<a href="https://github.com/tkarras/progressive_growing_of_gans" class="no-popup">source</a>⁠; <a href="https://youtu.be/XOxxPcy5Gr4" class="has-content spawns-popup">video</a>), while ex­pen­sive and re­quir­ing &gt;6 <span class="smallcaps-auto">GPU</span>-weeks<a href="https://www.gwern.net/Faces#sn6" class="footnote-ref spawns-popup" id="fnref6" role="doc-noteref"><sup>6</sup></a>⁠, did <em>work</em> and was even pow­er­ful enough to over­fit sin­gle-char­ac­ter face datasets; I did­n’t have enough <span class="smallcaps-auto">GPU</span> time to train on un­re­stricted face datasets, much less anime im­ages in gen­er­al, but merely get­ting this far was ex­cit­ing. Be­cause, a com­mon se­quence in DL/<span class="smallcaps-auto">DRL</span> (un­like many ar­eas of AI) is that a prob­lem seems in­tractable for long pe­ri­ods, un­til some­one mod­i­fies a scal­able ar­chi­tec­ture slight­ly, pro­duces some­what-cred­i­ble (not nec­es­sar­ily hu­man or even near-hu­man) re­sults, and then throws a ton of com­pute/<wbr>­data at it and, since the ar­chi­tec­ture scales, it rapidly ex­ceeds <span class="smallcaps-auto">SOTA</span> and ap­proaches hu­man lev­els (and po­ten­tially ex­ceeds hu­man-level). Now I just needed a faster <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture which I could train a much big­ger model with on a much big­ger dataset.</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="A his­tory of  gen­er­a­tion of anime faces: ‘do want’ to ‘oh no’ to ‘awe­some’" height="232" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-07-stylegan-animefaces-interpolationreactionface.jpg" srcset="/images/gan/stylegan/2019-02-07-stylegan-animefaces-interpolationreactionface.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-07-stylegan-animefaces-interpolationreactionface.jpg 1400w" title="Style transfer demo: transferring multiple facial expressions onto a single seed face." width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">A his­tory of <span class="smallcaps-auto cyxy-trs-source">GAN</span> gen­er­a­tion of anime faces: ‘do want’ to ‘oh no’ to ‘awe­some’</figcaption></span></span></figure>
<p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> was the fi­nal break­through in pro­vid­ing Pro<span class="smallcaps-auto">GAN</span>-level ca­pa­bil­i­ties but fast: by switch­ing to a rad­i­cally differ­ent ar­chi­tec­ture, it min­i­mized the need for the slow pro­gres­sive grow­ing (per­haps elim­i­nat­ing it en­tirely<a href="https://www.gwern.net/Faces#sn7" class="footnote-ref spawns-popup" id="fnref7" role="doc-noteref"><sup>7</sup></a>), and learned effi­ciently at mul­ti­ple lev­els of res­o­lu­tion, with bonuses in pro­vid­ing much more con­trol of the gen­er­ated im­ages with its “style trans­fer” metaphor.</p>
<section id="examples" class="level1">
<h1><a href="https://www.gwern.net/Faces#examples" title="Link to section: § &#39;Examples&#39;" class="no-popup cyxy-trs-source">Examples</a></h1>
<p class="cyxy-trs-source">First, some demon­stra­tions of what is pos­si­ble with Style<span class="smallcaps-auto">GAN</span> on anime faces:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="When it works: a hand-s­e­lected  sam­ple from my Asuka Souryuu Lan­g­ley-fine­tuned " height="486" loading="lazy" src="./一个StyleGAN动漫脸编辑详细教程_files/stylegan-asuka-face-sample.webp" title="A blond-haired blue-eyed anime face looking at the viewer based on the _Neon Genesis Evangelion_ character, Asuka Souryuu Langley." width="486" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">When it works: a hand-s­e­lected Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> sam­ple from my <a href="https://www.gwern.net/Faces#asuka" class="link-self identifier-link-down has-content spawns-popup cyxy-trs-source">Asuka Souryuu Lan­g­ley-fine­tuned</a> Style<span class="smallcaps-auto cyxy-trs-source">GAN</span></figcaption></span></span></figure>
<figure class="full-width" style="margin-left: calc( (-1 * (var(--GW-full-width-block-layout-left-adjustment) / 2)) + (var(--GW-full-width-block-layout-side-margin)) - ((var(--GW-full-width-block-layout-page-width) - 100%) / 2) ); margin-right: calc( (var(--GW-full-width-block-layout-left-adjustment) / 2) + (var(--GW-full-width-block-layout-side-margin)) - ((var(--GW-full-width-block-layout-page-width) - 100%) / 2) );">

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="64 of the best  anime face sam­ples se­lected from so­cial me­dia (click to zoom)." class="full-width focusable" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-01-stylegan-twdne-64bestsamples.jpg" srcset="/images/gan/stylegan/2019-03-01-stylegan-twdne-64bestsamples.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-01-stylegan-twdne-64bestsamples.jpg 1400w" width="1400"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">64 of the best <span class="smallcaps-auto cyxy-trs-source">TWDNE</span> anime face sam­ples se­lected from so­cial me­dia (click to zoom).</figcaption></span></span></figure>
<figure class="full-width" style="margin-left: calc( (-1 * (var(--GW-full-width-block-layout-left-adjustment) / 2)) + (var(--GW-full-width-block-layout-side-margin)) - ((var(--GW-full-width-block-layout-page-width) - 100%) / 2) ); margin-right: calc( (var(--GW-full-width-block-layout-left-adjustment) / 2) + (var(--GW-full-width-block-layout-side-margin)) - ((var(--GW-full-width-block-layout-page-width) - 100%) / 2) );">

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="100 ran­dom sam­ple im­ages from the  anime faces on " class="full-width focusable" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/thiswaifudoesnotexist-100samples.jpg" srcset="/images/gan/stylegan/thiswaifudoesnotexist-100samples.jpg-768px.jpg 768w, /images/gan/stylegan/thiswaifudoesnotexist-100samples.jpg 1400w" width="1400"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">100 ran­dom sam­ple im­ages from the Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> anime faces on <span class="smallcaps-auto cyxy-trs-source">TWDNE</span></figcaption></span></span></figure>
<p class="cyxy-trs-source">Even a quick look at the <span class="smallcaps-auto">MGM</span> &amp; Style<span class="smallcaps-auto">GAN</span> sam­ples demon­strates the lat­ter to be su­pe­rior in res­o­lu­tion, fine de­tails, and over­all ap­pear­ance (although the <span class="smallcaps-auto">MGM</span> faces ad­mit­tedly have fewer global mis­takes). It is also su­pe­rior to my 2018 Pro<span class="smallcaps-auto">GAN</span> faces. Per­haps the most strik­ing fact about these faces, which should be em­pha­sized for those for­tu­nate enough not to have spent as much time look­ing at aw­ful <span class="smallcaps-auto">GAN</span> sam­ples as I have, is not that the in­di­vid­ual faces are good, but rather that the faces are so <em>di­verse</em>, par­tic­u­larly when I look through face sam­ples with <a href="https://www.gwern.net/Faces#psitruncation-trick" class="link-self identifier-link-down has-content spawns-popup">Ψ ≥ 1</a>—it is not just the hair/<wbr>­eye color or head ori­en­ta­tion or fine de­tails that differ, but the over­all style ranges from CG to car­toon sketch, and even the ‘me­dia’ differ, I could swear many of these are try­ing to im­i­tate wa­ter­col­ors, char­coal sketch­ing, or oil paint­ing rather than dig­i­tal draw­ings, and some come off as rec­og­niz­ably ’90s-anime-style vs ’00s-anime-style. (I could look through sam­ples all day de­spite the global er­rors be­cause so many are in­ter­est­ing, which is not some­thing I could say of the <span class="smallcaps-auto">MGM</span> model whose nov­elty is quickly ex­haust­ed, and it ap­pears that users of my <span class="smallcaps-auto">TWDNE</span> web­site feel sim­i­larly as the av­er­age length of each visit is 1m:55s.)</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-11-stylegan-danbooru2017faces-interpolation.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-11-stylegan-danbooru2017faces-interpolation.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> of the <a href="https://nitter.cc/gwern/status/1095131651246575616" class="no-popup cyxy-trs-source">2019-02-11 face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span></a> demon­strat­ing gen­er­al­iza­tion.
</figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt=" anime face in­ter­po­la­tion videos are Elon Musk™-ap­proved!" height="798" loading="lazy" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-13-twitter-_ryobot-styleganelonmusklike.jpg" width="440" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> anime face in­ter­po­la­tion videos are <a href="https://nitter.cc/_Ryobot/status/1095619589495353346" class="no-popup cyxy-trs-source">Elon Musk™-ap­proved</a><a href="https://www.gwern.net/Faces#sn8" class="footnote-ref spawns-popup" id="fnref8" role="doc-noteref"><sup>8</sup></a>!</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-15-stylegan-faces-interpolation.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
Later <a href="https://www.gwern.net/images/gan/stylegan/2019-03-15-stylegan-faces-interpolation.mp4" class="has-content spawns-popup cyxy-trs-source">in­ter­po­la­tion video</a> (2019-03-08 face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>)
</figcaption></span></span></figure>
</section>
<section id="background" class="level1">
<h1><a href="https://www.gwern.net/Faces#background" title="Link to section: § &#39;Background&#39;" class="no-popup cyxy-trs-source">Background</a></h1>
<figure class="float-left">

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Ex­am­ple of the  up­scal­ing im­age pyra­mid ar­chi­tec­ture: small → large (vi­su­al­iza­tion by Shawn Presser)" class="float-left focusable" height="1164" loading="lazy" src="./一个StyleGAN动漫脸编辑详细教程_files/2020-06-30-shawnpresser-stylegan-ffhq-imagepyramid.webp" width="532"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Ex­am­ple of the Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> up­scal­ing im­age pyra­mid ar­chi­tec­ture: small → large (vi­su­al­iza­tion by <a href="https://nitter.cc/theshawwn/status/1277873213343592448" title="One limitation of StyleGAN is that it generates a &#39;pyramid&#39; of images. The first layer makes a 4×4 image, which is upscaled and passed through the next layer (8×8), and so on, until out pops the final 1024×1024. by the time you reach 32×32, the overall structure of the object is established (is this a face? is it a dog?) yet only the first 4 layers of the model were allowed to contribute to that decision! For a 1024×1024 model, that means 6 out of 10 layers of weights are irrelevant." class="no-popup cyxy-trs-source">Shawn Presser</a>)</figcaption></span></span></figure>
<p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> was pub­lished in 2018 as <a href="https://arxiv.org/abs/1812.04948" id="karras-et-al-2018-2" class="docMetadata has-annotation spawns-popup">“A Style-Based Gen­er­a­tor Ar­chi­tec­ture for Gen­er­a­tive Ad­ver­sar­ial Net­works”, Kar­ras&nbsp;et&nbsp;al&nbsp;2018</a> (<a href="https://github.com/NVlabs/stylegan" class="no-popup">source code</a>⁠; <a href="https://www.youtube.com/watch?v=kSLJriaOumA" class="has-content spawns-popup">demo video</a>⁠/ <a href="https://www.youtube.com/watch?v=SPI5uGCnxlc" class="has-content spawns-popup" data-attribute-title="[StyleGAN] A Style-Based Generator Architecture for GANs, part 1 (algorithm review) | TDLS [2019-01-24">al­go­rith­mic re­view video</a>⁠/ <a href="https://www.youtube.com/watch?v=_bh3U9HB-kg" class="has-content spawns-popup" data-attribute-title="[StyleGAN] A Style-Based Generator Architecture for GANs, part 2 (results and discussion) | TDLS [2019-01-24]">re­sults &amp; dis­cus­sions video</a>⁠; <a href="https://github.com/ak9250/stylegan-art/blob/master/styleganportraits.ipynb" class="no-popup">Co­lab note­book</a><a href="https://www.gwern.net/Faces#sn9" class="footnote-ref spawns-popup" id="fnref9" role="doc-noteref"><sup>9</sup></a>⁠; <a href="https://github.com/genforce/genforce" title="GenForce: an efficient PyTorch library for deep generative modeling (StyleGANv1v2, PGGAN, etc)" class="no-popup">Gen­Force Py­Torch reim­ple­men­ta­tion</a> with model zoo/ <a href="https://github.com/manicman1999/StyleGAN-Keras" class="no-popup">Keras</a>⁠; ex­plain­ers: <a href="https://www.gwern.net/docs/www/skymind.ai/03fa0a4700605e06a45de6b4097264aa6453af00.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://skymind.ai/wiki/generative-adversarial-network-gan" data-attribute-title="(Original URL: https://skymind.ai/wiki/generative-adversarial-network-gan )">Sky­mind.ai</a>⁠/ <a href="https://www.gwern.net/docs/www/www.lyrn.ai/612a2e29ed1860d12bdb2bb84f82e83471e6b605.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/" data-attribute-title="(Original URL: https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/ )">Lyrn.ai</a>⁠/ <a href="https://www.youtube.com/watch?v=1ct_P3IZow0" class="has-content spawns-popup">Two Minute Pa­pers video</a>). Style<span class="smallcaps-auto">GAN</span> takes the stan­dard <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture em­bod­ied by Pro<span class="smallcaps-auto">GAN</span> (whose source code it reuses) and, like the sim­i­lar <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture <a href="https://arxiv.org/abs/1810.01365" id="chen-et-al-2018-0" class="docMetadata has-annotation spawns-popup" data-attribute-title="On Self Modulation for Generative Adversarial Networks">Chen&nbsp;et&nbsp;al&nbsp;2018</a>⁠, draws in­spi­ra­tion from the field of “style trans­fer” (essen­tially in­vented by <a href="https://arxiv.org/abs/1508.06576" id="gatys-et-al-2015-8" class="docMetadata has-annotation spawns-popup">Gatys&nbsp;et&nbsp;al&nbsp;2014</a>), by chang­ing the Gen­er­a­tor (G) which cre­ates the im­age by re­peat­edly up­scal­ing its res­o­lu­tion to take, at each level of res­o­lu­tion from 8px→16px→32px→64px→128px etc a ran­dom in­put or “style noise”, which is com­bined with <a href="https://arxiv.org/abs/1703.06868" id="huang-belongie-2017-3" class="docMetadata has-annotation spawns-popup">AdaIN</a> and is used to tell the Gen­er­a­tor how to ‘style’ the im­age at that res­o­lu­tion by chang­ing the hair or chang­ing the skin tex­ture and so on. ‘Style noise’ at a low res­o­lu­tion like 32px affects the im­age rel­a­tively glob­al­ly, per­haps de­ter­min­ing the hair length or col­or, while style noise at a higher level like 256px might affect how frizzy in­di­vid­ual strands of hair are. In con­trast, Pro<span class="smallcaps-auto">GAN</span> and al­most all other <span class="smallcaps-auto">GAN</span>s in­ject noise into the G as well, but only at the be­gin­ning, which ap­pears to work not nearly as well (per­haps be­cause it is diffi­cult to prop­a­gate that ran­dom­ness ‘up­wards’ along with the up­scaled im­age it­self to the later lay­ers to en­able them to make con­sis­tent choic­es?). To put it sim­ply, by sys­tem­at­i­cally pro­vid­ing a bit of ran­dom­ness at each step in the process of gen­er­at­ing the im­age, Style<span class="smallcaps-auto">GAN</span> can ‘choose’ vari­a­tions effec­tive­ly.</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Kar­ras et al 2018,  vs  ar­chi­tec­ture: “Fig­ure 1. While a tra­di­tional gen­er­a­tor [29] feeds the la­tent code [z] though the in­put layer on­ly, we first map the in­put to an in­ter­me­di­ate la­tent space W, which then con­trols the gen­er­a­tor through adap­tive in­stance nor­mal­iza­tion (AdaIN) at each con­vo­lu­tion lay­er. Gauss­ian noise is added after each con­vo­lu­tion, be­fore eval­u­at­ing the non­lin­ear­i­ty. Here”A&quot; stands for a learned affine trans­form, and “B” ap­plies learned per-chan­nel scal­ing fac­tors to the noise in­put. The map­ping net­work f con­sists of 8 lay­ers and the syn­the­sis net­work g con­sists of 18 lay­er­s—two for each res­o­lu­tion (42-−10242). The out­put of the last layer is con­verted to  us­ing a sep­a­rate 1×1 con­vo­lu­tion, sim­i­lar to Kar­ras et al. [29]. Our gen­er­a­tor has a to­tal of 26.2M train­able pa­ra­me­ters, com­pared to 23.1M in the tra­di­tional gen­er­a­tor.&quot;" class="invertible focusable" height="805" loading="lazy" sizes="(max-width: 768px) 100vw, 920px" src="./一个StyleGAN动漫脸编辑详细教程_files/2018-karras-stylegan-figure1-styleganarchitecture.webp" srcset="/images/gan/stylegan/2018-karras-stylegan-figure1-styleganarchitecture.png-768px.png 768w, /images/gan/stylegan/2018-karras-stylegan-figure1-styleganarchitecture.png 920w" width="920"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">Kar­ras&nbsp;et&nbsp;al&nbsp;2018, Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> vs Pro<span class="smallcaps-auto cyxy-trs-source">GAN</span> ar­chi­tec­ture: “Fig­ure 1. While a tra­di­tional gen­er­a­tor <a href="https://arxiv.org/abs/1710.10196" id="karras-et-al-2017-0" class="docMetadata has-annotation spawns-popup cyxy-trs-source" data-attribute-title="&#39;ProGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation&#39;, Karras et al 2017">[29]</a> feeds the la­tent code [<em>z</em>] though the in­put layer on­ly, we first map the in­put to an in­ter­me­di­ate la­tent space <em>W</em>, which then con­trols the gen­er­a­tor through adap­tive in­stance nor­mal­iza­tion (AdaIN) at each con­vo­lu­tion lay­er. Gauss­ian noise is added after each con­vo­lu­tion, be­fore eval­u­at­ing the non­lin­ear­i­ty. Here”A" stands for a learned affine trans­form, and “B” ap­plies learned per-chan­nel scal­ing fac­tors to the noise in­put. The map­ping net­work <em>f</em> con­sists of 8 lay­ers and the syn­the­sis net­work <em>g</em> con­sists of 18 lay­er­s—two for each res­o­lu­tion (4<sup>2</sup>-−1024<sup>2</sup>). The out­put of the last layer is con­verted to <span class="smallcaps-auto cyxy-trs-source">RGB</span> us­ing a sep­a­rate 1×1 con­vo­lu­tion, sim­i­lar to Kar­ras et al. [29]. Our gen­er­a­tor has a to­tal of 26.2M train­able pa­ra­me­ters, com­pared to 23.1M in the tra­di­tional gen­er­a­tor."</figcaption></span></span></figure>
<p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> makes a num­ber of ad­di­tional im­prove­ments, but they ap­pear to be less im­por­tant: for ex­am­ple, it in­tro­duces a new <a href="https://github.com/NVlabs/ffhq-dataset" title="Flickr-Faces-HQ Dataset (FFHQ): a high-quality image dataset of human faces, originally created as a benchmark for generative adversarial networks (GAN)" class="no-popup">“<span class="smallcaps-auto">FFHQ</span>”</a> face/<wbr>­por­trait dataset with 1024px im­ages in or­der to show that Style<span class="smallcaps-auto">GAN</span> con­vinc­ingly im­proves on Pro<span class="smallcaps-auto">GAN</span> in fi­nal im­age qual­i­ty; switches to a loss which is more well-be­haved than the usual lo­gis­tic-style loss­es; and ar­chi­tec­ture-wise, it makes un­usu­ally heavy use of ful­ly-con­nected (FC) lay­ers to process an ini­tial ran­dom in­put, no less than 8 lay­ers of 512 neu­rons, where most <span class="smallcaps-auto">GAN</span>s use 1 or 2 FC lay­ers.<a href="https://www.gwern.net/Faces#sn10" class="footnote-ref spawns-popup" id="fnref10" role="doc-noteref"><sup>10</sup></a> More strik­ing is that it omits tech­niques that other <span class="smallcaps-auto">GAN</span>s have found crit­i­cal for be­ing able to train at 512px–1024px scale: it does not use newer losses like the <a href="https://arxiv.org/abs/1807.00734" id="jolicoeurmartineau-2018-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;The relativistic discriminator: a key element missing from standard GAN&#39;, Jolicoeur-Martineau 2018">rel­a­tivis­tic loss</a>⁠, <span class="smallcaps-auto">SAGAN</span>-style self­-at­ten­tion lay­ers in ei­ther G/<wbr>D, <a href="https://arxiv.org/abs/1810.00821" id="peng-et-al-2018-0" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow&#39;, Peng et al 2018"><span class="smallcaps-auto">VGAN</span>-style</a> vari­a­tional Dis­crim­i­na­tor bot­tle­necks, con­di­tion­ing on a tag or cat­e­gory em­bed­ding<a href="https://www.gwern.net/Faces#sn11" class="footnote-ref spawns-popup" id="fnref11" role="doc-noteref"><sup>11</sup></a>⁠, Big<span class="smallcaps-auto">GAN</span>-style large mini­batch­es, differ­ent noise dis­tri­b­u­tions<a href="https://www.gwern.net/Faces#sn12" class="footnote-ref spawns-popup" id="fnref12" role="doc-noteref"><sup>12</sup></a>⁠, ad­vanced reg­u­lar­iza­tion like <a href="https://arxiv.org/abs/1705.10941" id="yoshida-miyato-2017-5" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Spectral norm regularization for improving the generalizability of deep learning&#39;, Yoshida &amp; Miyato 2017">spec­tral</a> <a href="https://arxiv.org/abs/1802.05957" id="miyato-et-al-2018-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Spectral Normalization for Generative Adversarial Networks&#39;, Miyato et al 2018">nor­mal­iza­tion</a>⁠, etc.<a href="https://www.gwern.net/Faces#sn13" class="footnote-ref spawns-popup" id="fnref13" role="doc-noteref"><sup>13</sup></a> One pos­si­ble rea­son for Style<span class="smallcaps-auto">GAN</span>’s suc­cess is the way it com­bines out­puts from the mul­ti­ple lay­ers into a sin­gle fi­nal im­age rather than re­peat­edly up­scal­ing; when we vi­su­al­ize the out­put of each layer as an <span class="smallcaps-auto">RGB</span> im­age in anime Style<span class="smallcaps-auto">GAN</span>s, there is a strik­ing di­vi­sion of la­bor be­tween lay­er­s—­some lay­ers fo­cus on mono­chrome out­li­nes, while oth­ers fill in tex­tured re­gions of col­or, and they sum up into an im­age with sharp lines and good color gra­di­ents while main­tain­ing de­tails like eyes.</p>
<p class="cyxy-trs-source">Aside from the FCs and style noise &amp; nor­mal­iza­tion, it is a vanilla ar­chi­tec­ture. (One odd­ity is the use of only 3×3 con­vo­lu­tions &amp; so few lay­ers in each up­scal­ing block; a more con­ven­tional up­scal­ing block than Style<span class="smallcaps-auto">GAN</span>’s 3×3→3×3 would be <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=18" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=18" data-attribute-title="Figure 16: (a) A typical architectural layout for BigGAN-deep&#39;s _G_ (Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=18 )">some­thing like Big<span class="smallcaps-auto">GAN</span></a> which does 1×1 → 3×3 → 3×3 → 1×1. It’s not clear if this is a good idea as it lim­its the spa­tial in­flu­ence of each pixel by pro­vid­ing lim­ited re­cep­tive fields<a href="https://www.gwern.net/Faces#sn14" class="footnote-ref spawns-popup" id="fnref14" role="doc-noteref"><sup>14</sup></a>⁠.) Thus, if one has some fa­mil­iar­ity with train­ing a Pro<span class="smallcaps-auto">GAN</span> or an­other <span class="smallcaps-auto">GAN</span>, one can im­me­di­ately work with Style<span class="smallcaps-auto">GAN</span> with no trou­ble: the train­ing dy­nam­ics are sim­i­lar and the hy­per­pa­ra­me­ters have their usual mean­ing, and the code­base is much the same as the orig­i­nal Pro<span class="smallcaps-auto">GAN</span> (with the main ex­cep­tion be­ing that <code>config.py</code> has been re­named <code>train.py</code> (or <code>run_training.py</code> in S2) and the orig­i­nal <code>train.py</code>, which stores the crit­i­cal con­fig­u­ra­tion pa­ra­me­ters, has been moved to <code>training/training_loop.py</code>; there is still no sup­port for com­mand-line op­tions and Style<span class="smallcaps-auto">GAN</span> must be con­trolled by edit­ing <code>train.py</code>/<wbr><code>training_loop.py</code> by hand).</p>
<section id="applications" class="level2">
<h2><a href="https://www.gwern.net/Faces#applications" title="Link to section: § &#39;Applications&#39;" class="no-popup cyxy-trs-source">Applications</a></h2>
<p class="cyxy-trs-source">Be­cause of its speed and sta­bil­i­ty, when the source code was re­leased on 2019-02-04 (a date that will long be noted in the <span class="smallcaps-auto">ANN</span>als of <span class="smallcaps-auto">GAN</span>ime), the Nvidia mod­els &amp; sam­ple dumps were quickly pe­rused &amp; new Style<span class="smallcaps-auto">GAN</span>s trained on a wide va­ri­ety of im­age types, yield­ing, in ad­di­tion to the orig­i­nal faces/<wbr>­cart­s/<wbr>­cats of Kar­ras&nbsp;et&nbsp;al&nbsp;2018:</p>
<div class="columns">
<ul>
<li><p class="cyxy-trs-source"><a href="https://thispersondoesnotexist.com/" id="wang-2019" class="docMetadata has-annotation spawns-popup">“This Per­son Does Not Ex­ist”</a> (ran­dom sam­ples from the 1024px <span class="smallcaps-auto">FFHQ</span> face Style<span class="smallcaps-auto">GAN</span>)</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source">quizzes: <a href="http://www.whichfaceisreal.com/" class="no-popup cyxy-trs-source">“Which Face is Real”</a>⁠/ <a href="https://www.gwern.net/docs/www/blurrd.ai/9ea516dda02990f569e1b9d212b0633c3c21a359.html" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://blurrd.ai/realorfake/" data-attribute-title="(Original URL: https://blurrd.ai/realorfake/ )">“Real or Fake?”</a></li>
<li class="cyxy-trs-source">vot­ing: <a href="https://thecleverest.com/judgefakepeople/main.php?sort=highest" class="no-popup cyxy-trs-source">“Judge Fake Peo­ple”</a></li>
<li><a href="https://www.youtube.com/watch?v=VZT29WnYib4" class="has-content spawns-popup cyxy-trs-source" data-attribute-title="StyleGAN Generates Instagram Portraits, Tafseer Ahmed (ai.imagine) [2019-08-27]">In­sta­gram por­traits</a></li>
</ul></li>
<li><p class="cyxy-trs-source"><strong>cats</strong>: <a href="http://thesecatsdonotexist.com/" class="no-popup">“These Cats Do Not Ex­ist”</a>⁠, <a href="https://thiscatdoesnotexist.com/" class="no-popup">“This Cat Does Not Ex­ist”</a> (<a href="https://www.gwern.net/docs/www/aiweirdness.com/9c3a8b1db4f7b95affdbb9122d0e62d96d50f17a.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://aiweirdness.com/post/182633984547/gancats" data-attribute-title="GANcats (Original URL: http://aiweirdness.com/post/182633984547/gancats )">cat fail­ure modes</a>⁠; <a href="https://nitter.cc/genekogan/status/1093180351437029376" class="no-popup">in­ter­po­la­tion/<wbr>style-trans­fer</a>)/ <a href="https://nitter.cc/MichaelFriese10/status/1151236302559305728" class="no-popup">cor­gies</a></p></li>
<li><p class="cyxy-trs-source">ho­tel <strong>rooms</strong> (with char-<span class="smallcaps-auto">RNN</span>-generated text de­scrip­tion­s): <a href="https://thisrentaldoesnotexist.com/" class="no-popup">“This Rental Does Not Ex­ist”</a></p>
<ul>
<li class="cyxy-trs-source"><a href="https://nitter.cc/crschmidt/status/1099562911960350720" class="no-popup cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">kitchen/<wbr>­din­ing room/<wbr>liv­ing room/<wbr>bed­room</a> (us­ing trans­fer learn­ing)</li>
</ul></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/xsteenbrugge/status/1096820308164661248" class="no-popup">satel­lite im­agery</a>⁠; <a href="https://nitter.cc/crschmidt/status/1097200249779769344" class="no-popup">Gothic cathe­drals</a>⁠; <a href="https://nitter.cc/refikanadol/status/1106798493299949568" class="no-popup">Frank Gehry build­ings</a>⁠; <a href="https://nitter.cc/roadrunning01/status/1109488507591028740" class="no-popup">cityscapes</a>⁠; <a href="https://nitter.cc/erikswahn/status/1123951017148788738" class="no-popup">floor plans</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://www.thiswaifudoesnotexist.net/" id="gwern-www-thiswaifudoesnotexist-net" class="docMetadata has-annotation spawns-popup">“This Waifu Does Not Ex­ist”</a> (<a href="https://www.gwern.net/TWDNE" id="gwern-twdne" class="link-local docMetadata has-annotation spawns-popup">back­ground/<wbr>im­ple­men­ta­tion</a>)</p>
<ul>
<li><p class="cyxy-trs-source"><a href="https://artbreeder.com/" id="simon-2019" class="docMetadata has-annotation spawns-popup">Art­breeder</a></p>
<p class="cyxy-trs-source">large up­grade over <span class="smallcaps-auto">TWDNE</span>: ran­dom gen­er­a­tion, ex­plo­ration, im­age at­tribute edit­ing, sav­ing to a gallery, and cross­breed­ing por­traits</p></li>
<li><p class="cyxy-trs-source"><a href="https://waifulabs.com/" id="studios-2019" class="docMetadata has-annotation spawns-popup">Waifu Labs</a></p>
<p class="cyxy-trs-source">in­ter­ac­tive waifu gen­er­a­tor (im­proved re­sults, in­spired by <span class="smallcaps-auto">TWDNE</span> &amp; us­ing Dan­booru2018 as a dataset)</p></li>
<li><p class="cyxy-trs-source"><a href="https://github.com/a312863063/seeprettyface-ganerator-dongman" class="no-popup"><code>seeprettyface-ganerator-dongman</code></a></p></li>
</ul></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/highqualitysh1t/status/1095699293011435520" class="no-popup">pho­to-booth selfies</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/knjcode/status/1102771002222637056" class="no-popup">ra­men</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/kikko_fr/status/1094685986691399681" class="no-popup">West­ern por­traits</a> (<a href="https://imgur.com/a/8nkMmeB" class="no-popup">2</a>)</p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/roadrunning01/status/1111686125431783424" class="no-popup">Poke­mon (suc­cess­ful)</a> (<a href="https://nitter.cc/MichaelFriese10/status/1127614400750346240" class="no-popup">2</a>⁠, <a href="https://github.com/t04glovern/stylegan-pokemon" class="no-popup">3</a>⁠; <a href="https://www.gwern.net/docs/www/iguanamouth.tumblr.com/684fe209b483666b075e17101ed6f1fa559b3aa5.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://iguanamouth.tumblr.com/post/158982472537/pokemon-generated-by-neural-network" data-attribute-title="(Original URL: https://iguanamouth.tumblr.com/post/158982472537/pokemon-generated-by-neural-network )">hu­man il­lus­tra­tion</a>)</p></li>
<li><p class="cyxy-trs-source"><strong>fonts</strong>: <a href="https://nitter.cc/kikko_fr/status/1095603397179396098" class="no-popup">1</a>⁠/ <a href="https://www.gwern.net/docs/www/www.machinelearningfont.com/fe110cb3d8e248d55cb3b0b8d665ad2bc12882bb.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://www.machinelearningfont.com/" data-attribute-title="A Machine Learning Font (Original URL: http://www.machinelearningfont.com/ )">2</a>⁠; <a href="https://medium.com/@robert.munro/creating-new-scripts-with-stylegan-c16473a50fd0" class="no-popup">Uni­code char­ac­ters</a>⁠; <a href="https://nitter.cc/kintopp/status/1218795800400101376" class="no-popup">kanji</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/zaidalyafeai/status/1346841324461416458" class="no-popup">Ara­bic cal­lig­ra­phy</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/PINguAR/status/1097130957163937792" class="no-popup">graffiti sam­ples</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/drose101/status/1108104217577832449" class="no-popup">wine la­bels</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/mattjarviswall/status/1110548997729452035" class="no-popup">Large Logo Dataset</a>⁠; <a href="https://github.com/cedricoeldorf/ConditionalStyleGAN" class="no-popup">Cedric Oel­dor­f’s Con­di­tional Style<span class="smallcaps-auto">GAN</span></a></p></li>
<li><p class="cyxy-trs-source"><a href="https://www.youtube.com/watch?v=jjmu5_ewCIE" class="has-content spawns-popup"><em>Doom</em> tex­tures</a>⁠; <a href="https://www.gwern.net/docs/www/futurism.com/bbc0ecee0c78799d6ed0c4c829be21639d859ef2.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://futurism.com/neural-network-draw-doom-guy-high-res" data-attribute-title="Someone Used a Neural Network to Draw Doom Guy in High-Res: A series of algorithms turned the famous pixelated face into an HD portrait (Original URL: https://futurism.com/neural-network-draw-doom-guy-high-res )"><em>Doom</em> char­ac­ters</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://old.reddit.com/r/computervision/comments/bfcnbj/p_stylegan_on_oxford_visual_geometry_group/" class="no-popup">flow­ers</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://medium.com/gradientcrescent/this-president-does-not-exist-generating-artistic-portraits-of-donald-trump-using-stylegan-a97a17902dd4" title="This President Does Not Exist: Generating Artistic Portraits of Donald Trump using StyleGAN Transfer Learning: Theory and Implementation in Tensorflow" class="no-popup">Don­ald Trump</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/iforcedabot.com/a30a9bb8807603c3cf5c9c5ceae58b72d713e980.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://iforcedabot.com/i-have-no-mana-and-i-must-tap/" data-attribute-title="I Have No Mana And I Must Tap (Original URL: https://iforcedabot.com/i-have-no-mana-and-i-must-tap/ )"><em>Mag­ic: The Gath­er­ing</em> cards</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/ionicdevil/status/1122756808991330304" class="no-popup">Tom Cruise faces</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://github.com/colinrsmall/ehm_faces" title="Eastside Hockey Manager faces, Colin R. Small" class="no-popup">Hockey player faces</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/old.reddit.com/dfb59128bcec3a1c64cc432d40e5b7a8fdf8672a.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/bkrn3i/p_stylegan_trained_on_album_covers/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/bkrn3i/p_stylegan_trained_on_album_covers/ )">mu­sic al­bum cov­ers</a>⁠; <a href="https://www.gwern.net/docs/www/booksby.ai/68b19f355772f1ea531093c6eaabe5f8db2cc0ff.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://booksby.ai/" data-attribute-title="(Original URL: https://booksby.ai/ )">book cov­ers</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://thiseyedoesnotexist.com/story/" class="no-popup">eyes</a> (Pro<span class="smallcaps-auto">GAN</span>)</p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/realmeatyhuman/status/1233084317032681483" title="Curated output from a StyleGAN 2 model trained on images that trigger pareidolia in the viewer - scraped from the `#iseefaces` and `#pareidolia` hashtags on Instagram." class="no-popup">Face parei­do­lia</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/MichaelFriese10/status/1130604229372997632" class="no-popup">emo­jis</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/MichaelFriese10/status/1132777932802236417" class="no-popup">Wik­i­how il­lus­tra­tions</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://thisvesseldoesnotexist.com/" class="no-popup">vases</a></p></li>
<li><p class="cyxy-trs-source">watch­es: <a href="https://www.gwern.net/docs/www/digital-thinking.de/7bd165cc46b1bd8f5f6daea25734705e22a437da.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://digital-thinking.de/watchgan-advancing-generated-watch-images-with-stylegans/" data-attribute-title="(Original URL: http://digital-thinking.de/watchgan-advancing-generated-watch-images-with-stylegans/ )">1</a>⁠/ <a href="https://www.gwern.net/docs/www/evigio.com/0f836f4dfc7bbbeee3bc1b053519762dce72b77b.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://evigio.com/post/generating-new-watch-designs-with-stylegan" data-attribute-title="Generating New Watch Designs With StyleGAN (Original URL: https://evigio.com/post/generating-new-watch-designs-with-stylegan )">2</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://github.com/EvgenyKashin/ganarts" title="This T-shirt does not exist" class="no-popup">T-shirts</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/old.reddit.com/950c4e33225555335b10c22861432e09b2d99482.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MediaSynthesis/comments/ea5qoy/butterflies_generated_with_stylegan/" data-attribute-title="I trained a StyleGAN on images of butterflies from the Natural History Museum in London. (Original URL: https://old.reddit.com/r/MediaSynthesis/comments/ea5qoy/butterflies_generated_with_stylegan/ )">but­ter­flies</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/svilentodorov.xyz/69c5a3b645df542da7fe471c487ee18931444860.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://svilentodorov.xyz/blog/stylegan-for-evil/" data-attribute-title="StyleGAN for Evil: Trypophobia and Clockwork Oranging (Original URL: https://svilentodorov.xyz/blog/stylegan-for-evil/ )">Gen­er­at­ing creepy im­ages</a> (<a href="https://en.wikipedia.org/wiki/Trypophobia" class="docMetadata has-annotation spawns-popup">try­popho­bia</a>)</p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/ai/2020-05-05-tjukanov-mapdreameraicartography.html" class="has-content spawns-popup" data-attribute-title="Mapdreamer - AI cartography">maps</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://arxiv.org/abs/2011.05552" id="xue-2020" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;End-to-End Chinese Landscape Painting Creation Using Generative Adversarial Networks&#39;, Xue 2020">Chi­nese land­scape art</a></p></li>
</ul>
</div>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Im­age­quilt vi­su­al­iza­tion of the wide range of vi­sual sub­jects  has been ap­plied to" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1201px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-19-stylegan-imagequilt-allstyleganexamples.jpg" srcset="/images/gan/stylegan/2019-03-19-stylegan-imagequilt-allstyleganexamples.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-19-stylegan-imagequilt-allstyleganexamples.jpg 1201w" width="1201" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source"><a href="https://www.gwern.net/docs/www/www.edwardtufte.com/12ebe387f4f198ca94cd0a6adb15e7a62bdb56e2.html#bboard_content" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0003wk#bboard_content" data-attribute-title="(Original URL: https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0003wk#bboard_content )">Im­age­quilt</a> vi­su­al­iza­tion of the wide range of vi­sual sub­jects Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> has been ap­plied to</figcaption></span></span></figure>
</section>
<section id="why-dont-gans-work" class="level2">
<h2><a href="https://www.gwern.net/Faces#why-dont-gans-work" title="Link to section: § &#39;Why Don’t GANs Work?&#39;" class="no-popup cyxy-trs-source">Why Don’t GANs Work?</a></h2>
<p class="cyxy-trs-source">Why does Style<span class="smallcaps-auto">GAN</span> work so well on anime im­ages while other <span class="smallcaps-auto">GAN</span>s worked not at all or slowly at best?</p>
<p class="cyxy-trs-source">The les­son I took from <a href="https://arxiv.org/abs/1711.10337" id="lucic-et-al-2017" class="docMetadata has-annotation spawns-popup">“Are <span class="smallcaps-auto">GAN</span>s Cre­ated Equal? A Large-S­cale Study”</a>⁠, Lu­cic&nbsp;et&nbsp;al&nbsp;2017, is that CelebA/<span class="smallcaps-auto">CIFAR</span>10 are too easy, as al­most all eval­u­ated <span class="smallcaps-auto">GAN</span> ar­chi­tec­tures were ca­pa­ble of oc­ca­sion­ally achiev­ing good <span class="smallcaps-auto">FID</span> if one sim­ply did enough it­er­a­tions &amp; hy­per­pa­ra­me­ter tun­ing.</p>
<p class="cyxy-trs-source">In­ter­est­ing­ly, I con­sis­tently ob­serve in train­ing <em>all</em> <span class="smallcaps-auto">GAN</span>s on anime that clear lines &amp; sharp­ness &amp; cel-like smooth gra­di­ents ap­pear only to­ward the end of train­ing, after typ­i­cally ini­tially blurry tex­tures have co­a­lesced. This sug­gests an in­her­ent bias of <span class="smallcaps-auto">CNN</span>s: color im­ages work be­cause they pro­vide some de­gree of tex­tures to start with, but lin­eart/<wbr>­mono­chrome stuff fails be­cause the <span class="smallcaps-auto">GAN</span> op­ti­miza­tion dy­nam­ics flail around. This is con­sis­tent with <a href="https://arxiv.org/abs/1811.12231" id="geirhos-et-al-2018" class="docMetadata has-annotation spawns-popup" data-attribute-title="ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness">Geirhos&nbsp;et&nbsp;al&nbsp;2018’s find­ings</a>—which uses style trans­fer to con­struct a data-aug­ment­ed/<wbr>­trans­formed “Styl­ized-Im­a­geNet”—show­ing that Im­a­geNet <span class="smallcaps-auto">CNN</span>s are lazy and, be­cause the tasks can be achieved to some de­gree with tex­ture-only clas­si­fi­ca­tion (as demon­strated by sev­eral of Geirhos&nbsp;et&nbsp;al&nbsp;2018’s au­thors via <a href="https://openreview.net/forum?id=SkfMWhAqYQ" title="&#39;Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet&#39;, Brendel &amp; Bethge 2019" class="no-popup">“Bag­Nets”</a>), fo­cus on tex­tures un­less oth­er­wise forced; and by <a href="https://arxiv.org/abs/1907.07640" id="orhan-2019-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Robustness properties of Facebook&#39;s ResNeXt WSL models&#39;, Orhan 2019">ResNeXt</a> &amp; <a href="https://arxiv.org/abs/1911.09071" id="hermann-et-al-2019" class="docMetadata has-annotation spawns-popup" data-attribute-title="Exploring the Origins and Prevalence of Texture Bias in Convolutional Neural Networks">Her­mann &amp; Ko­rn­blith 2019</a>⁠, who find that al­though <span class="smallcaps-auto">CNN</span>s are per­fectly ca­pa­ble of em­pha­siz­ing shape over tex­ture, low­er-per­form­ing mod­els tend to rely more heav­ily on tex­ture and that many kinds of train­ing (in­clud­ing <a href="https://arxiv.org/abs/1907.02544" id="donahue-simonyan-2019-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Large Scale Adversarial Representation Learning&#39;, Donahue &amp; Simonyan 2019">BigBi<span class="smallcaps-auto">GAN</span></a>) will in­duce a tex­ture fo­cus, sug­gest­ing tex­ture tends to be low­er-hang­ing fruit. So while <span class="smallcaps-auto">CNN</span>s <em>can</em> learn sharp lines &amp; shapes rather than tex­tures, the typ­i­cal <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture &amp; train­ing al­go­rithm do not make it easy. Since <span class="smallcaps-auto">CIFAR</span>10/CelebA can be fairly de­scribed as be­ing just as heavy on tex­tures as Im­a­geNet (which is not true of anime im­ages), it is not sur­pris­ing that <span class="smallcaps-auto">GAN</span>s train eas­ily on them start­ing with tex­tures and grad­u­ally re­fin­ing into good sam­ples but then strug­gle on ani­me.</p>
<p class="cyxy-trs-source">This raises a ques­tion of whether the Style<span class="smallcaps-auto">GAN</span> ar­chi­tec­ture is nec­es­sary and whether many <span class="smallcaps-auto">GAN</span>s might work, if only one had good style trans­fer for anime im­ages and could, to de­feat the tex­ture bi­as, gen­er­ate many ver­sions of each anime im­age which kept the shape while chang­ing the color palet­te? (Cur­rent style trans­fer meth­ods like the <a href="https://github.com/naoto0804/pytorch-AdaIN" title="naoto0804/pytorch-AdaIN: Unofficial pytorch implementation of &#39;Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization&#39; [Huang+, ICCV2017]" class="no-popup">AdaIN Py­Torch im­ple­men­ta­tion</a> used by Geirhos&nbsp;et&nbsp;al&nbsp;2018, do not work well on anime im­ages, iron­i­cally enough, be­cause they are trained on pho­to­graphic im­ages, typ­i­cally us­ing the old <span class="smallcaps-auto">VGG</span> mod­el.)</p>
</section>
</section>
<section id="faq" class="level1">
<h1><a href="https://www.gwern.net/Faces#faq" title="Link to section: § &#39;FAQ&#39;" class="no-popup cyxy-trs-source">FAQ</a></h1>
<div class="epigraph">
<blockquote>
<p class="cyxy-trs-source">“…Its so­cial ac­count­abil­ity seems sort of like that of de­sign­ers of mil­i­tary weapons: un­cul­pa­ble right up un­til they get a lit­tle too good at their job.”</p>
<p class="cyxy-trs-source"><a href="https://en.wikipedia.org/wiki/David_Foster_Wallace" class="docMetadata has-annotation spawns-popup">David Fos­ter Wal­lace</a>⁠, <a href="https://www.thefreelibrary.com/E+unibus+pluram%3A+television+and+U.S.+fiction.-a013952319" class="no-popup">“<em>E unibus plu­ram</em>: Tele­vi­sion and U.S. Fic­tion”</a></p>
</blockquote>
</div>
<p class="cyxy-trs-source">To ad­dress some com­mon ques­tions peo­ple have after see­ing gen­er­ated sam­ples:</p>
<ul>
<li><p class="cyxy-trs-source"><strong>Over­fit­ting</strong>: “Aren’t Style<span class="smallcaps-auto">GAN</span> (or Big<span class="smallcaps-auto">GAN</span>) just over­fit­ting &amp; mem­o­riz­ing data?”</p>
<p class="cyxy-trs-source">Amus­ing­ly, this is not a ques­tion any­one re­ally both­ered to ask of ear­lier <span class="smallcaps-auto">GAN</span> ar­chi­tec­tures, which is a sign of progress. Over­fit­ting is a bet­ter prob­lem to have than un­der­fit­ting, be­cause over­fit­ting means you can use a smaller model or more data or more ag­gres­sive reg­u­lar­iza­tion tech­niques, while un­der­fit­ting means your ap­proach just is­n’t work­ing.</p>
<p class="cyxy-trs-source">In any case, while there is cur­rently no way to con­clu­sively prove that cut­ting-edge <span class="smallcaps-auto">GAN</span>s are not 100% mem­o­riz­ing (be­cause they should be mem­o­riz­ing to a con­sid­er­able ex­tent in or­der to learn im­age gen­er­a­tion, and eval­u­at­ing gen­er­a­tive mod­els is hard in gen­er­al, and for <span class="smallcaps-auto">GAN</span>s in par­tic­u­lar, be­cause they don’t pro­vide stan­dard met­rics like like­li­hoods which could be used on held-out sam­ples), there are sev­eral rea­sons to think that they are not <em>just</em> mem­o­riz­ing:<a href="https://www.gwern.net/Faces#sn15" class="footnote-ref spawns-popup" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<ol type="1">
<li><p class="cyxy-trs-source"><span class="smallcaps">Sam­ple/<wbr>­Dataset Over­lap</span>: a stan­dard check for over­fit­ting is to com­pare gen­er­ated im­ages to their clos­est matches us­ing <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search" class="docMetadata has-annotation spawns-popup">near­est-neigh­bors</a> (where dis­tance is de­fined by fea­tures like a <span class="smallcaps-auto">CNN</span> em­bed­ding) lookup; an ex­am­ple of this are <a href="https://www.gwern.net/docs/www/openaccess.thecvf.com/fd901982b300a9c92523141dfd49cbc0a95eca4a.pdf#page=7" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_StackGAN_Text_to_ICCV_2017_paper.pdf#page=7" data-attribute-title="(Original URL: http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_StackGAN_Text_to_ICCV_2017_paper.pdf#page=7 )">Stack<span class="smallcaps-auto">GAN</span>’s Fig­ure 6</a> &amp; Big<span class="smallcaps-auto">GAN</span>’s <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=14" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=14" data-attribute-title="(Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=14 )">Fig­ures 10–14</a>⁠, where the pho­to­re­al­is­tic sam­ples are nev­er­the­less com­pletely differ­ent from the most sim­i­lar Im­a­geNet dat­a­points. This has not been done for Style<span class="smallcaps-auto">GAN</span> yet but I would­n’t ex­pect differ­ent re­sults as <span class="smallcaps-auto">GAN</span>s typ­i­cally pass this check. (It’s worth not­ing that <a href="https://en.wikipedia.org/wiki/Clearview_AI" class="docMetadata has-annotation spawns-popup">Clearview AI’s</a> fa­cial recog­ni­tion <a href="https://nitter.cc/kashhill/status/1218542846694871040" class="no-popup">re­port­edly does not re­turn Flickr matches</a> for ran­dom <span class="smallcaps-auto">FFHQ</span> Style<span class="smallcaps-auto">GAN</span> faces, sug­gest­ing the gen­er­ated faces gen­uinely look like new faces rather than any of the orig­i­nal Flickr faces.)</p>
<p class="cyxy-trs-source">One in­trigu­ing <a href="https://www.gwern.net/Faces#discriminator-ranking" class="link-self identifier-link-down has-content spawns-popup">ob­ser­va­tion about <span class="smallcaps-auto">GAN</span>s made by the Big<span class="smallcaps-auto">GAN</span> pa­per</a> is that the crit­i­cisms of Gen­er­a­tors mem­o­riz­ing dat­a­points may be pre­cisely the op­po­site of re­al­i­ty: <span class="smallcaps-auto">GAN</span>s may work pri­mar­ily by the <em>Dis­crim­i­na­tor</em> (adap­tive­ly) over­fit­ting to dat­a­points, thereby re­pelling the Gen­er­a­tor away from real dat­a­points and forc­ing it to learn nearby pos­si­ble im­ages which col­lec­tively span the im­age dis­tri­b­u­tion. (With enough data, this cre­ates gen­er­al­iza­tion be­cause “neural nets are lazy” and only learn to gen­er­al­ize when eas­ier strate­gies fail.)</p></li>
<li><p class="cyxy-trs-source"><span class="smallcaps">Se­man­tic Un­der­stand­ing</span>: <span class="smallcaps-auto">GAN</span>s ap­pear to learn mean­ing­ful con­cepts like in­di­vid­ual ob­jects, as demon­strated by “la­tent space ad­di­tion” or re­search tools like <a href="https://www.gwern.net/docs/www/blog.acolyer.org/e254b57c006a9b2b19e9460d0eafa1860a1c505b.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://blog.acolyer.org/2019/02/27/gan-dissection-visualizing-and-understanding-generative-adversarial-networks/" data-attribute-title="(Original URL: https://blog.acolyer.org/2019/02/27/gan-dissection-visualizing-and-understanding-generative-adversarial-networks/ )"><span class="smallcaps-auto">GAN</span>dissection</a>⁠/ <a href="https://arxiv.org/abs/1811.10153" id="suzuki-et-al-2018" class="docMetadata has-annotation spawns-popup" data-attribute-title="Spatially Controllable Image Synthesis with Internal Representation Collaging">Suzuki&nbsp;et&nbsp;al&nbsp;2018</a>⁠; im­age ed­its like ob­ject dele­tion­s/<wbr>ad­di­tions (<a href="https://arxiv.org/abs/2007.15646" id="bau-et-al-2020-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="Rewriting a Deep Generative Model">Bau&nbsp;et&nbsp;al&nbsp;2020</a>) or seg­ment­ing ob­jects like dogs from their back­grounds (<a href="https://arxiv.org/abs/2002.03754" id="voynov-babenko-2020-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="Unsupervised Discovery of Interpretable Directions in the GAN Latent Space">Voynov &amp; Babenko 2020</a>⁠/ <a href="https://arxiv.org/abs/2006.04988" id="voynov-et-al-2020-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Big GANs Are Watching You: Towards Unsupervised Object Segmentation with Off-the-Shelf Generative Models">Voynov&nbsp;et&nbsp;al&nbsp;2020</a>) are diffi­cult to ex­plain with­out some gen­uine un­der­stand­ing of im­ages.</p></li>
</ol>
<p class="cyxy-trs-source">In the case of Style<span class="smallcaps-auto">GAN</span> anime faces, there are <a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images" class="link-self identifier-link-down has-content spawns-popup">en­coders and con­trol­lable face gen­er­a­tion</a> now which demon­strate that the la­tent vari­ables do map onto mean­ing­ful fac­tors of vari­a­tion &amp; the model must have gen­uinely learned about cre­at­ing im­ages rather than merely mem­o­riz­ing real im­ages or im­age patch­es. Sim­i­lar­ly, when we use the “trun­ca­tion trick”/<wbr>ψ to sam­ple from rel­a­tively ex­treme un­likely im­ages and we look at the dis­tor­tions, they show how gen­er­ated im­ages break down in se­man­ti­cal­ly-rel­e­vant ways, which would not be the case if it was just pla­gia­rism. (A par­tic­u­larly ex­treme ex­am­ple of the power of the learned Style<span class="smallcaps-auto">GAN</span> prim­i­tives is <a href="https://www.gwern.net/docs/ai/2019-abdal.pdf" id="abdal-et-al-2019" class="docMetadata has-annotation spawns-popup" data-attribute-title="Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?">Ab­dal&nbsp;et&nbsp;al&nbsp;2019</a>’s demon­stra­tion that Kar­ras et al’s <span class="smallcaps-auto">FFHQ</span> faces Style<span class="smallcaps-auto">GAN</span> can be used to gen­er­ate fairly re­al­is­tic im­ages of cat­s/<wbr>­dogs/<wbr>­cars.)</p>
<ol start="3" type="1">
<li><p class="cyxy-trs-source"><span class="smallcaps">La­tent Space Smooth­ness</span>: in gen­er­al, in­ter­po­la­tion in the la­tent space (<em>z</em>) shows smooth changes of im­ages and log­i­cal trans­for­ma­tions or vari­a­tions of face fea­tures; if Style<span class="smallcaps-auto">GAN</span> were merely mem­o­riz­ing in­di­vid­ual dat­a­points, the in­ter­po­la­tion would be ex­pected to be low qual­i­ty, yield many ter­ri­ble faces, and ex­hibit ‘jumps’ in be­tween points cor­re­spond­ing to re­al, mem­o­rized, dat­a­points. The Style<span class="smallcaps-auto">GAN</span> anime face mod­els do not ex­hibit this. (In con­trast, the Holo Pro<span class="smallcaps-auto">GAN</span>, which over­fit bad­ly, does show se­vere prob­lems in its la­tent space in­ter­po­la­tion videos.)</p>
<p class="cyxy-trs-source">Which is not to say that <span class="smallcaps-auto">GAN</span>s do not have is­sues: “mode drop­ping” seems to still be an is­sue for Big<span class="smallcaps-auto">GAN</span> de­spite the ex­pen­sive large-mini­batch train­ing, which is over­fit­ting to some de­gree, and Style<span class="smallcaps-auto">GAN</span> pre­sum­ably suffers from it too.</p></li>
<li><p class="cyxy-trs-source"><span class="smallcaps"><a href="https://www.gwern.net/Faces#transfer-learning" class="link-self identifier-link-down has-content spawns-popup">Trans­fer Learn­ing</a></span>: <span class="smallcaps-auto">GAN</span>s have been used for semi­-su­per­vised learn­ing (eg gen­er­at­ing plau­si­ble ‘la­beled’ sam­ples to train a clas­si­fier on), im­i­ta­tion learn­ing like <a href="https://arxiv.org/abs/1606.03476" id="ho-ermon-2016-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Generative Adversarial Imitation Learning&#39;, Ho &amp; Ermon 2016"><span class="smallcaps-auto">GAIL</span></a>⁠, and re­train­ing on fur­ther datasets; if the G is merely mem­o­riz­ing, it is diffi­cult to ex­plain how any of this would work.</p></li>
</ol></li>
<li><p class="cyxy-trs-source"><strong>Com­pute Re­quire­ments</strong>: “Does­n’t Style<span class="smallcaps-auto">GAN</span> take too long to train?”</p>
<p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> is re­mark­ably fast-train­ing for a <span class="smallcaps-auto">GAN</span>. With the anime faces, I got bet­ter re­sults after 1–3 days of Style<span class="smallcaps-auto">GAN</span> train­ing than I’d got­ten with &gt;3 weeks of Pro<span class="smallcaps-auto">GAN</span> train­ing. The train­ing times quoted by the Style<span class="smallcaps-auto">GAN</span> repo may sound scary, but they are, in prac­tice, a steep over­es­ti­mate of what you ac­tu­ally need, for sev­eral rea­sons:</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source"><span class="smallcaps cyxy-trs-source">Lower Res­o­lu­tion</span>: the largest fig­ures are for 1024px im­ages but you may not need them to be that large or even <em class="cyxy-trs-source">have</em> a big dataset of 1024px im­ages. For anime faces, 1024px-sized faces are rel­a­tively rare, and train­ing at 512px &amp; up­scal­ing 2× to 1024 with <code>waifu2x</code><a href="https://www.gwern.net/Faces#sn16" class="footnote-ref spawns-popup" id="fnref16" role="doc-noteref"><sup>16</sup></a> works fine &amp; is much faster. Since up­scal­ing is rel­a­tively sim­ple &amp; easy, an­other strat­egy is to change the pro­gres­sive-grow­ing sched­ule: in­stead of pro­ceed­ing to the fi­nal res­o­lu­tion as fast as pos­si­ble, in­stead ad­just the sched­ule to stop at a more fea­si­ble res­o­lu­tion &amp; spend the bulk of train­ing time <em class="cyxy-trs-source">there</em> in­stead and then do just enough train­ing at the fi­nal res­o­lu­tion to learn to up­scale (eg spend 10% of train­ing grow­ing to 512px, then 80% of train­ing time at 512px, then 10% at 1024px).</li>
<li class="cyxy-trs-source"><span class="smallcaps cyxy-trs-source">Di­min­ish­ing Re­turns</span>: the largest gains in im­age qual­ity are seen in the first few days or weeks of train­ing with the re­main­ing train­ing be­ing not that use­ful as they fo­cus on im­prov­ing small de­tails (so just a few days may be more than ad­e­quate for your pur­pos­es, es­pe­cially if you’re will­ing to se­lect a lit­tle more ag­gres­sively from sam­ples)</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source"><span class="smallcaps cyxy-trs-source">Trans­fer Learn­ing</span> from a re­lated model can save days or weeks of train­ing, as there is no need to train from scratch; with the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, one can train a char­ac­ter-spe­cific Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> with a few hours or days at most, and cer­tainly do not need to spend mul­ti­ple weeks train­ing from scratch! (as­sum­ing that would­n’t just cause over­fit­ting) Sim­i­lar­ly, if one wants to train on some 1024px face dataset, why start from scratch, tak­ing ~1000 <span class="smallcaps-auto cyxy-trs-source">GPU</span>-hours, when you can start from Nvidi­a’s <span class="smallcaps-auto cyxy-trs-source">FFHQ</span> face model which is al­ready fully trained, and can con­verge in a frac­tion of the from-scratch time? For 1024px, you could use a su­per-res­o­lu­tion <span class="smallcaps-auto cyxy-trs-source">GAN</span> like to up­scale? Al­ter­nate­ly, you could change the im­age pro­gres­sion bud­get to spend most of your time at 512px and then at the tail end try 1024px.</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source"><span class="smallcaps cyxy-trs-source">One-Time Costs</span>: the up­front cost of a few hun­dred dol­lars of <span class="smallcaps-auto cyxy-trs-source">GPU</span>-time (at in­flated <span class="smallcaps-auto cyxy-trs-source">AWS</span> prices) may seem steep, but should be kept in per­spec­tive. As with al­most all NNs, train­ing 1 Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> model can be lit­er­ally tens of mil­lions of times more ex­pen­sive than sim­ply run­ning the Gen­er­a­tor to pro­duce 1 im­age; but it also need be paid only once by only one per­son, and the to­tal price need not even be paid by the same per­son, given trans­fer learn­ing, but can be amor­tized across var­i­ous datasets. In­deed, given how fast run­ning the Gen­er­a­tor is, the trained model does­n’t even need to be run on a <span class="smallcaps-auto cyxy-trs-source">GPU</span>. (The rule of thumb is that a <span class="smallcaps-auto cyxy-trs-source">GPU</span> is 20–30× faster than the same thing on <span class="smallcaps-auto cyxy-trs-source">CPU</span>, with rare in­stances when over­head dom­i­nates of the <span class="smallcaps-auto cyxy-trs-source">CPU</span> be­ing as fast or faster, so since gen­er­at­ing 1 im­age takes on the or­der of ~0.1s on <span class="smallcaps-auto cyxy-trs-source">GPU</span>, a <span class="smallcaps-auto cyxy-trs-source">CPU</span> can do it in ~3s, which is ad­e­quate for many pur­pos­es.)</li>
</ul></li>
<li><p class="cyxy-trs-source"><strong>Copy­right In­fringe­ment</strong>: “Who owns Style<span class="smallcaps-auto">GAN</span> im­ages?”</p>
<ol type="1">
<li><p class="cyxy-trs-source">The Nvidia <span class="smallcaps">Source Code &amp; Re­leased Mod­els</span> for Style<span class="smallcaps-auto">GAN</span> 1 are un­der a <a href="https://en.wikipedia.org/wiki/Creative_Commons_license" class="docMetadata has-annotation spawns-popup">CC</a>-<a href="https://creativecommons.org/licenses/by-nc/4.0/" class="no-popup">BY-NC</a> li­cense, and you can­not edit them or pro­duce “de­riv­a­tive works” such as re­train­ing their <span class="smallcaps-auto">FFHQ</span>, cat, or cat Style<span class="smallcaps-auto">GAN</span> mod­els. (Style<span class="smallcaps-auto">GAN</span> 2 is un­der a new <a href="https://nvlabs.github.io/stylegan2/license.html" class="no-popup">“Nvidia Source Code Li­cense-NC”</a>⁠, which ap­pears to be effec­tively the same as the CC-BY-NC with the ad­di­tion of a <a href="https://en.wikipedia.org/wiki/Software_patents_and_free_software#Patent_retaliation" class="docMetadata has-annotation spawns-popup">patent re­tal­i­a­tion clause</a>⁠.)</p>
<p class="cyxy-trs-source">If a model is trained from scratch, then that does not ap­ply as the source code is sim­ply an­other tool used to cre­ate the model and noth­ing about the CC-BY-NC li­cense forces you to do­nate the copy­right to Nvidia. (It would be odd if such a thing did hap­pen—if your word proces­sor claimed to trans­fer the copy­rights of every­thing writ­ten in it to Mi­crosoft!)</p>
<p class="cyxy-trs-source">For those con­cerned by the CC-BY-NC li­cense, a 512px <span class="smallcaps-auto">FFHQ</span> con­fig-f Style<span class="smallcaps-auto">GAN</span> 2 has been trained &amp; <a href="https://nitter.cc/AydaoGMan/status/1269690778324013061" class="no-popup">re­leased into the pub­lic do­main</a> by Ay­dao, and is avail­able for down­load from <a href="https://mega.nz/file/eQdHkShY#8wyNKs343L7YUjwXlEg3cWjqK2g2EAIdYz5xbkPy3ng" title="ffhq-512-avg-tpurun1.pkl (348MB)" class="no-popup">Mega</a> and my rsync mir­ror:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">rsync</span> <span class="at">--verbose</span> rsync://78.46.86.149:873/biggan/2020-06-07-aydao-stylegan2-configf-ffhq-512-avg-tpurun1.pkl.xz ./</code></pre></div></li>
<li><p class="cyxy-trs-source"><span class="smallcaps">Mod­els</span> in gen­eral are gen­er­ally con­sid­ered <a href="https://en.wikipedia.org/wiki/Transformativeness" class="docMetadata has-annotation spawns-popup">“trans­for­ma­tive works”</a> and the copy­right own­ers of what­ever data the model was trained on have no copy­right on the mod­el. (The fact that the datasets or in­puts are copy­righted is ir­rel­e­vant, as train­ing on them is uni­ver­sally con­sid­ered <a href="https://www.gwern.net/docs/www/cdn.openai.com/cd988b9e4e4a3752b48cc6576a19479eb066e27d.pdf" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://cdn.openai.com/policy-submissions/OpenAI+Comments+on+Intellectual+Property+Protection+for+Artificial+Intelligence+Innovation.pdf" data-attribute-title="Comment Regarding Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation (Original URL: https://cdn.openai.com/policy-submissions/OpenAI+Comments+on+Intellectual+Property+Protection+for+Artificial+Intelligence+Innovation.pdf )">fair use</a> and trans­for­ma­tive, sim­i­lar to artists or search en­gi­nes; see the fur­ther read­ing.) The model is copy­righted to whomever cre­ated it. Hence, Nvidia has copy­right on the mod­els it cre­ated but I have copy­right un­der the mod­els I trained (which I re­lease un­der CC-0).</p></li>
<li><p class="cyxy-trs-source"><span class="smallcaps">Sam­ples</span> are trick­i­er. The usual wide­ly-s­tated le­gal in­ter­pre­ta­tion is that the stan­dard copy­right law po­si­tion is that only <a href="https://www.gwern.net/docs/www/www.copyright.gov/a3b588fae3ff15956a602010794dc5264f50a731.pdf#Compendium%20300.indd%3A.122046%3A96431" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.copyright.gov/comp3/chap300/ch300-copyrightable-authorship.pdf#Compendium%20300.indd%3A.122046%3A96431" data-attribute-title="(Original URL: https://www.copyright.gov/comp3/chap300/ch300-copyrightable-authorship.pdf#Compendium%20300.indd%3A.122046%3A96431 )">hu­man au­thors can earn a copy­right</a> and that ma­chi­nes, an­i­mals, inan­i­mate ob­jects or most fa­mous­ly, <a href="https://en.wikipedia.org/wiki/Monkey_selfie_copyright_dispute" class="docMetadata has-annotation spawns-popup">mon­keys</a>⁠, can­not. The US Copy­right Office states clearly that re­gard­less of whether we re­gard a <span class="smallcaps-auto">GAN</span> as a ma­chine or a some­thing more in­tel­li­gent like an an­i­mal, ei­ther way, it does­n’t count:</p>
<blockquote>
<p class="cyxy-trs-source">A work of au­thor­ship must pos­sess “some min­i­mal de­gree of cre­ativ­ity” to sus­tain a copy­right claim. <em>Feist</em>, 499 U.S. at 358, 362 (c­i­ta­tion omit­ted). “[T]he req­ui­site level of cre­ativ­ity is ex­tremely low.” Even a “slight amount” of cre­ative ex­pres­sion will suffice. “The vast ma­jor­ity of works make the grade quite eas­i­ly, as they pos­sess some cre­ative spark, ‘no mat­ter how crude, hum­ble or ob­vi­ous it might be.’” <em>Id</em>. at 346 (c­i­ta­tion omit­ted).</p>
<p class="cyxy-trs-source">… To qual­ify as a work of “au­thor­ship” a work must be cre­ated by a hu­man be­ing. See <em>Bur­row-Giles Lith­o­graphic Co.</em>, 111 U.S. at 58. Works that do not sat­isfy this re­quire­ment are not copy­rightable. The Office will not reg­is­ter works pro­duced by na­ture, an­i­mals, or plants.</p>
<p class="cyxy-trs-source"><em>Ex­am­ples</em>:</p>
<ul>
<li class="cyxy-trs-source">A pho­to­graph taken by a mon­key.</li>
<li class="cyxy-trs-source">A mural painted by an ele­phant.</li>
</ul>
<p class="cyxy-trs-source">…the Office will not reg­is­ter works pro­duced by a ma­chine or mere me­chan­i­cal process that op­er­ates ran­domly or au­to­mat­i­cally with­out any cre­ative in­put or in­ter­ven­tion from a hu­man au­thor.</p>
</blockquote>
<p class="cyxy-trs-source">A dump of ran­dom sam­ples such as the Nvidia sam­ples or <span class="smallcaps-auto">TWDNE</span> there­fore has no copy­right &amp; by de­fi­n­i­tion is in the pub­lic do­main.</p>
<p class="cyxy-trs-source">A new copy­right can be cre­at­ed, how­ev­er, if a hu­man au­thor is suffi­ciently ‘in the loop’, so to speak, as to ex­ert a <em>de min­imis</em> amount of cre­ative effort, even if that ‘cre­ative effort’ is sim­ply se­lect­ing a sin­gle im­age out of a dump of thou­sands of them or twid­dling knobs (eg on Make Girl­s.­Moe). <a href="https://www.gwern.net/docs/www/crypko.ai/115e46a8f2b9e7b2571c5b7e25776afcace22c53.html#/faqs" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://crypko.ai/#/faqs" data-attribute-title="(Original URL: https://crypko.ai/#/faqs )">Crypko</a>⁠, for ex­am­ple, <a href="https://www.gwern.net/docs/www/www.itmedia.co.jp/b3d6b75b333f330aa2744f3712b1045d31b14cd1.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.itmedia.co.jp/news/articles/1711/28/news020.html" data-attribute-title="萌えキャラ生成AI、学習データを&#39;ネットの海&#39;からゲッチュするのはアリか？ (1/5) (Original URL: https://www.itmedia.co.jp/news/articles/1711/28/news020.html )">take this po­si­tion.</a></p></li>
</ol>
<p class="cyxy-trs-source">Fur­ther read­ing on com­put­er-gen­er­ated art copy­rights:</p>
<ul>
<li class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/scholarship.law.duke.edu/a1e3497c25f17ccc4a747c6ae83eb694aba25b70.pdf#.pdf" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=1023&amp;context=dltr#.pdf" data-attribute-title="(Original URL: https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=1023&amp;context=dltr#.pdf )">“Copy­rights In Com­put­er-Gen­er­ated Works: Whom, If Any­one, Do We Re­ward?”</a>⁠, Glasser 2001</li>
<li class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/www.rutgerslawreview.com/17928b67e95f0fea8cf2f6e99a304da202e8fc08.pdf" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="http://www.rutgerslawreview.com/wp-content/uploads/2017/07/Robert-Denicola-Ex-Machina-69-Rutgers-UL-Rev-251-2016.pdf" data-attribute-title="(Original URL: http://www.rutgerslawreview.com/wp-content/uploads/2017/07/Robert-Denicola-Ex-Machina-69-Rutgers-UL-Rev-251-2016.pdf )">“Ex Machi­na: Copy­right Pro­tec­tion For Com­put­er-Gen­er­ated Works”</a>⁠, Deni­cola 2016</li>
<li class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/files.osf.io/ec8146bbff8fb02b928571d483bc2874b494ec9a.pdf#.pdf" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://files.osf.io/v1/resources/np2jd/providers/osfstorage/59614dec594d9002288271b6?action=download&amp;version=1&amp;direct#.pdf" data-attribute-title="(Original URL: https://files.osf.io/v1/resources/np2jd/providers/osfstorage/59614dec594d9002288271b6?action=download&amp;version=1&amp;direct#.pdf )">“Com­puter Gen­er­ated Works and Copy­right: Selfies, Traps, Ro­bots, AI and Ma­chine Learn­ing”</a>⁠, Lam­bert 2017</li>
<li class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/alj.artrepreneur.com/f835f8518c1fd7de85799be53b58063914806a38.html" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://alj.artrepreneur.com/the-next-rembrandt-who-holds-the-copyright-in-computer-generated-art/" data-attribute-title="(Original URL: https://alj.artrepreneur.com/the-next-rembrandt-who-holds-the-copyright-in-computer-generated-art/ )">“Who holds the Copy­right in AI Cre­ated Art?”</a>⁠, Steve Schlack­man (2018)</li>
<li class="cyxy-trs-source"><a href="https://www.gwern.net/docs/ai/2019-gervais.pdf" id="gervais-2019" class="docMetadata has-annotation spawns-popup cyxy-trs-source">“The Ma­chine as Au­thor”</a>⁠, Ger­vais 2019</li>
<li class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/www.artnome.com/e1343df6df3d5f9a437eec5801e18acc8b3d326d.html" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://www.artnome.com/news/2019/3/27/why-is-ai-art-copyright-so-complicated" data-attribute-title="(Original URL: https://www.artnome.com/news/2019/3/27/why-is-ai-art-copyright-so-complicated )">“Why Is AI Art Copy­right So Com­pli­cat­ed?”</a>⁠, Ja­son Bai­ley (2019)</li>
<li class="cyxy-trs-source"><a href="https://www.theverge.com/2019/4/17/18299563/ai-algorithm-music-law-copyright-human" class="no-popup cyxy-trs-source">“We’ve been warned about AI and mu­sic for over 50 years, but no one’s pre­pared: ‘This road is lit­er­ally be­ing paved as we’re walk­ing on it’”</a> (<em class="cyxy-trs-source">The Verge</em> 2019)</li>
</ul></li>
</ul>
<section id="copyright" class="level2">
<h2><a href="https://www.gwern.net/Faces#copyright" title="Link to section: § &#39;Copyright&#39;" class="no-popup cyxy-trs-source">Copyright</a></h2>
<p class="cyxy-trs-source">Per the copy­right point above, all my gen­er­ated videos and sam­ples and mod­els are re­leased un­der the <a href="https://creativecommons.org/share-your-work/public-domain/cc0/" class="no-popup">CC-0</a> (pub­lic do­main equiv­a­lent) li­cense. Source code listed may be de­riv­a­tive works of Nvidi­a’s CC-BY-NC-li­censed Style<span class="smallcaps-auto">GAN</span> code, and may be CC-BY-NC.</p>
</section>
</section>
<section id="training-requirements" class="level1">
<h1><a href="https://www.gwern.net/Faces#training-requirements" title="Link to section: § &#39;Training requirements&#39;" class="no-popup cyxy-trs-source">Training requirements</a></h1>
<section id="data" class="level2">
<h2><a href="https://www.gwern.net/Faces#data" title="Link to section: § &#39;Data&#39;" class="no-popup cyxy-trs-source">Data</a></h2>
<div class="epigraph">
<blockquote>
<p class="cyxy-trs-source">“The road of ex­cess leads to the palace of wis­dom<br>
…If the fool would per­sist in his folly he would be­come wise<br>
…You never know what is enough un­less you know what is more than enough. …If oth­ers had not been fool­ish, we should be so.”</p>
<p class="cyxy-trs-source">William Blake, “Proverbs of Hell”, <a href="https://en.wikipedia.org/wiki/The_Marriage_of_Heaven_and_Hell" class="docMetadata has-annotation spawns-popup"><em>The Mar­riage of Heaven and Hell</em></a></p>
</blockquote>
</div>
<p class="cyxy-trs-source">The nec­es­sary size for a dataset de­pends on the com­plex­ity of the do­main and whether trans­fer learn­ing is be­ing used. Style<span class="smallcaps-auto">GAN</span>’s de­fault set­tings yield a 1024px Gen­er­a­tor with 26.2M pa­ra­me­ters, which is a large model and can soak up po­ten­tially mil­lions of im­ages, so there is no such thing as too much.</p>
<p class="cyxy-trs-source">For learn­ing de­cen­t-qual­ity anime faces from scratch, a min­i­mum of 5000 ap­pears to be nec­es­sary in prac­tice; for learn­ing a spe­cific char­ac­ter when us­ing the anime face Style<span class="smallcaps-auto">GAN</span>, po­ten­tially as lit­tle as ~500 (e­spe­cially with data aug­men­ta­tion) can give good re­sults. For do­mains as com­pli­cated as “any cat photo” like Kar­ras&nbsp;et&nbsp;al&nbsp;2018’s cat Style<span class="smallcaps-auto">GAN</span> which is trained on the <a href="https://www.gwern.net/docs/www/www.yf.io/581816a0c032371cb5939dbb4af1a04d956e82b6.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.yf.io/p/lsun" data-attribute-title="(Original URL: https://www.yf.io/p/lsun )"><span class="smallcaps-auto">LSUN</span></a> <span class="smallcaps-auto">CATS</span> cat­e­gory of ~1.8M<a href="https://www.gwern.net/Faces#sn17" class="footnote-ref spawns-popup" id="fnref17" role="doc-noteref"><sup>17</sup></a> cat pho­tos, that ap­pears to ei­ther not be enough or Style<span class="smallcaps-auto">GAN</span> was not trained to con­ver­gence; Kar­ras&nbsp;et&nbsp;al&nbsp;2018 note that “<span class="smallcaps-auto">CATS</span> con­tin­ues to be a diffi­cult dataset due to the high in­trin­sic vari­a­tion in pos­es, zoom lev­els, and back­grounds.”<a href="https://www.gwern.net/Faces#sn18" class="footnote-ref spawns-popup" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
</section>
<section id="compute" class="level2">
<h2><a href="https://www.gwern.net/Faces#compute" title="Link to section: § &#39;Compute&#39;" class="no-popup cyxy-trs-source">Compute</a></h2>
<p class="cyxy-trs-source">To fit rea­son­able mini­batch sizes, one will want <span class="smallcaps-auto">GPU</span>s with &gt;11GB <span class="smallcaps-auto">VRAM</span>. At 512px, that will only train <em>n</em> = 4, and go­ing be­low that means it’ll be even slower (and you may have to re­duce learn­ing rates to avoid un­sta­ble train­ing). So, Nvidia 1080ti &amp; up would be good. (Re­port­ed­ly, <span class="smallcaps-auto">AMD</span>/OpenCL works <a href="https://nitter.cc/syoyo/status/1093526177891770369" class="no-popup">for run­ning Style<span class="smallcaps-auto">GAN</span> mod­els</a>⁠, and there is one re­port of suc­cess­ful train­ing with “Radeon <span class="smallcaps-auto">VII</span> with <code>tensorflow-rocm</code> 1.13.2 and <code>rocm</code> 2.3.14”.)</p>
<p class="cyxy-trs-source">The Style<span class="smallcaps-auto">GAN</span> repo pro­vide the fol­low­ing es­ti­mated train­ing times for 1–8 <span class="smallcaps-auto">GPU</span> sys­tems (which I con­vert to to­tal <span class="smallcaps-auto">GPU</span>-hours &amp; pro­vide a worst-case <span class="smallcaps-auto">AWS</span>-based cost es­ti­mate):</p>
<blockquote>
<div class="small-table table-wrapper">
<table class="tablesorter tablesorter-default tablesorterab22e7922c28f" role="grid" aria-labelledby="tablesorterab22e7922c28fcaption">
<caption id="tablesorterab22e7922c28fcaption" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">Es­ti­mated Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> wall­clock train­ing times for var­i­ous res­o­lu­tions &amp; <span class="smallcaps-auto cyxy-trs-source">GPU</span>-clusters (source: Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> re­po)</caption>
<colgroup>
<col style="width: 6%">
<col style="width: 22%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header tablesorter-headerRow" role="row">
<th style="text-align: right; user-select: none;" data-column="0" class="tablesorter-header tablesorter-headerUnSorted" tabindex="0" scope="col" role="columnheader" aria-disabled="false" unselectable="on" aria-sort="none" aria-label="GPUs: No sort applied, activate to apply an ascending sort"><div class="tablesorter-header-inner"><span class="smallcaps-auto cyxy-trs-source">GPU</span>s</div></th>
<th style="text-align: left; user-select: none;" data-column="1" class="tablesorter-header tablesorter-headerUnSorted" tabindex="0" scope="col" role="columnheader" aria-disabled="false" unselectable="on" aria-sort="none" aria-label="10242: No sort applied, activate to apply an ascending sort"><div class="tablesorter-header-inner">1024<sup>2</sup></div></th>
<th style="text-align: left; user-select: none;" data-column="2" class="tablesorter-header tablesorter-headerUnSorted" tabindex="0" scope="col" role="columnheader" aria-disabled="false" unselectable="on" aria-sort="none" aria-label="5122: No sort applied, activate to apply an ascending sort"><div class="tablesorter-header-inner">512<sup>2</sup></div></th>
<th style="text-align: left; user-select: none;" data-column="3" class="tablesorter-header tablesorter-headerUnSorted" tabindex="0" scope="col" role="columnheader" aria-disabled="false" unselectable="on" aria-sort="none" aria-label="2562: No sort applied, activate to apply an ascending sort"><div class="tablesorter-header-inner">256<sup>2</sup></div></th>
<th data-column="4" class="tablesorter-header tablesorter-headerUnSorted" tabindex="0" scope="col" role="columnheader" aria-disabled="false" unselectable="on" aria-sort="none" aria-label="[March 2019 AWS Costs19]: No sort applied, activate to apply an ascending sort" style="user-select: none;"><div class="tablesorter-header-inner cyxy-trs-source cyxy-trs-source">[March 2019 <span class="smallcaps-auto cyxy-trs-source">AWS</span> Costs<a href="https://www.gwern.net/Faces#sn19" class="footnote-ref spawns-popup" id="fnref19" role="doc-noteref"><sup>19</sup></a>]</div></th>
</tr>
</thead>
<tbody aria-live="polite" aria-relevant="all">
<tr class="odd" role="row">
<td style="text-align: right;">1</td>
<td style="text-align: left;" class="cyxy-trs-source cyxy-trs-source">41 days 4 hours [988 <span class="smallcaps-auto cyxy-trs-source">GPU</span>-hours]</td>
<td style="text-align: left;" class="cyxy-trs-source cyxy-trs-source">24 days 21 hours [597 <span class="smallcaps-auto cyxy-trs-source">GPU</span>-hours]</td>
<td style="text-align: left;" class="cyxy-trs-source cyxy-trs-source">14 days 22 hours [358 <span class="smallcaps-auto cyxy-trs-source">GPU</span>-hours]</td>
<td class="cyxy-trs-source">[$320, $194, $115]</td>
</tr>
<tr class="even" role="row">
<td style="text-align: right;">2</td>
<td style="text-align: left;" class="cyxy-trs-source">21 days 22 hours [1,052]</td>
<td style="text-align: left;" class="cyxy-trs-source">13 days 7 hours [638]</td>
<td style="text-align: left;" class="cyxy-trs-source">9 days 5 hours [442]</td>
<td class="cyxy-trs-source">[NA]</td>
</tr>
<tr class="odd" role="row">
<td style="text-align: right;">4</td>
<td style="text-align: left;" class="cyxy-trs-source">11 days 8 hours [1,088]</td>
<td style="text-align: left;" class="cyxy-trs-source">7 days 0 hours [672]</td>
<td style="text-align: left;" class="cyxy-trs-source">4 days 21 hours [468]</td>
<td class="cyxy-trs-source">[NA]</td>
</tr>
<tr class="even" role="row">
<td style="text-align: right;">8</td>
<td style="text-align: left;" class="cyxy-trs-source">6 days 14 hours [1,264]</td>
<td style="text-align: left;" class="cyxy-trs-source">4 days 10 hours [848]</td>
<td style="text-align: left;" class="cyxy-trs-source">3 days 8 hours [640]</td>
<td class="cyxy-trs-source">[$2,730, $1,831, $1,382]</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<p class="cyxy-trs-source"><span class="smallcaps-auto">AWS</span> <span class="smallcaps-auto">GPU</span> in­stances are some of the most ex­pen­sive ways to train a NN and pro­vide an up­per bound (com­pare <a href="https://vast.ai/" class="no-popup">Vast.ai</a>); 512px is often an ac­cept­able (or nec­es­sary) res­o­lu­tion; and in prac­tice, the full quoted train­ing time is not re­ally nec­es­sary—with my anime face Style<span class="smallcaps-auto">GAN</span>, the faces them­selves were high qual­ity within 48 <span class="smallcaps-auto">GPU</span>-hours, and what train­ing it for ~1000 ad­di­tional <span class="smallcaps-auto">GPU</span>-hours ac­com­plished was pri­mar­ily to im­prove de­tails like the shoul­ders &amp; back­grounds. (Pro<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span> par­tic­u­larly strug­gle with back­grounds &amp; edges of im­ages be­cause those are cut off, ob­scured, and high­ly-var­ied com­pared to the faces, whether anime or <span class="smallcaps-auto">FFHQ</span>. I hy­poth­e­size that the tell­tale blurry back­grounds are due to the im­pov­er­ish­ment of the back­ground­s/<wbr>edges in cropped face pho­tos, and they could be fixed by trans­fer­-learn­ing or pre­train­ing on a more generic dataset like Im­a­geNet, so it learns what the back­grounds even <em>are</em> in the first place; then in face train­ing, it merely has to re­mem­ber them &amp; de­fo­cus a bit to gen­er­ate cor­rect blurry back­ground­s.)</p>
<figure class="full-width" style="margin-left: calc( (-1 * (var(--GW-full-width-block-layout-left-adjustment) / 2)) + (var(--GW-full-width-block-layout-side-margin)) - ((var(--GW-full-width-block-layout-page-width) - 100%) / 2) ); margin-right: calc( (var(--GW-full-width-block-layout-left-adjustment) / 2) + (var(--GW-full-width-block-layout-side-margin)) - ((var(--GW-full-width-block-layout-page-width) - 100%) / 2) );">

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Train­ing im­prove­ments: 256px  anime faces after ~46  (top) vs 512px anime faces after 382  (bot­tom); see also the video mon­tage of first 9k it­er­a­tions" class="full-width focusable" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1179px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-10-stylegan-animefaces-1dayvs6daystraining.jpg" srcset="/images/gan/stylegan/2019-02-10-stylegan-animefaces-1dayvs6daystraining.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-10-stylegan-animefaces-1dayvs6daystraining.jpg 1179w" width="1179"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source"><em class="cyxy-trs-source">Train­ing im­prove­ments</em>: 256px Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> anime faces after ~46 <span class="smallcaps-auto cyxy-trs-source">GPU</span>-hours (top) vs 512px anime faces after 382 <span class="smallcaps-auto cyxy-trs-source">GPU</span>-hours (bot­tom); see also the <a href="https://www.gwern.net/images/gan/stylegan/2019-03-16-stylegan-facestraining.mp4" class="has-content spawns-popup cyxy-trs-source">video mon­tage of first 9k it­er­a­tions</a></figcaption></span></span></figure>
</section>
</section>
<section id="data-preparation" class="level1">
<h1><a href="https://www.gwern.net/Faces#data-preparation" title="Link to section: § &#39;Data Preparation&#39;" class="no-popup cyxy-trs-source">Data Preparation</a></h1>
<p class="cyxy-trs-source">The most diffi­cult part of run­ning Style<span class="smallcaps-auto">GAN</span> is prepar­ing the dataset prop­er­ly. Style<span class="smallcaps-auto">GAN</span> does not, un­like most <span class="smallcaps-auto">GAN</span> im­ple­men­ta­tions (par­tic­u­larly Py­Torch ones), sup­port read­ing a di­rec­tory of files as in­put; it can only read its unique <code>.tfrecord</code> for­mat which stores each im­age as raw ar­rays at every rel­e­vant res­o­lu­tion.<a href="https://www.gwern.net/Faces#sn20" class="footnote-ref spawns-popup" id="fnref20" role="doc-noteref"><sup>20</sup></a> Thus, in­put files must be per­fectly uni­form, (s­low­ly) con­verted to the <code>.tfrecord</code> for­mat by the spe­cial <code>dataset_tool.py</code> tool, and will take up ~19× more disk space.<a href="https://www.gwern.net/Faces#sn21" class="footnote-ref spawns-popup" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
<div class="admonition error">
<div class="admonition-title">
<p class="cyxy-trs-source">A Style<span class="smallcaps-auto">GAN</span> dataset must con­sist of im­ages all for­mat­ted <em>ex­actly</em> the same way.</p>
</div>
<p class="cyxy-trs-source">Im­ages must be pre­cisely 512×512px or 1024×1024px etc (any eg 512×513px im­ages will kill the en­tire run), they must all be the <em>same</em> col­or­space (you can­not have s<span class="smallcaps-auto">RGB</span> and Grayscale <span class="smallcaps-auto">JPG</span>s—and I doubt other color spaces work at al­l), they must not be trans­par­ent, the file­type must be the same as the model you in­tend to (re)­train (ie you can­not re­train a <span class="smallcaps-auto">PNG</span>-trained model on a <span class="smallcaps-auto">JPG</span> dataset, Style<span class="smallcaps-auto">GAN</span> will crash every time with in­scrutable con­vo­lu­tion/<wbr>chan­nel-re­lated er­rors)<a href="https://www.gwern.net/Faces#sn22" class="footnote-ref spawns-popup" id="fnref22" role="doc-noteref"><sup>22</sup></a>⁠, and there must be no sub­tle er­rors like <span class="smallcaps-auto">CRC</span> check­sum er­rors which im­age view­ers or li­braries like Im­ageMag­ick often ig­nore.</p>
</div>
<section id="faces-preparation" class="level2">
<h2><a href="https://www.gwern.net/Faces#faces-preparation" title="Link to section: § &#39;Faces preparation&#39;" class="no-popup cyxy-trs-source">Faces preparation</a></h2>
<p class="cyxy-trs-source">My work­flow:</p>
<ol type="1">
<li class="cyxy-trs-source cyxy-trs-source">Down­load raw im­ages from <a href="https://www.gwern.net/Danbooru2020#download" class="link-local has-content spawns-popup cyxy-trs-source">Dan­booru2018</a> if nec­es­sary</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Ex­tract from the <span class="smallcaps-auto cyxy-trs-source">JSON</span> Dan­booru2018 meta­data all the IDs of a sub­set of im­ages if a spe­cific Dan­booru tag (such as a sin­gle char­ac­ter) is de­sired, us­ing <code>jq</code> and shell script­ing</li>
<li class="cyxy-trs-source cyxy-trs-source">Crop square anime faces from raw im­ages us­ing <a href="https://github.com/nagadomi/lbpcascade_animeface" class="no-popup cyxy-trs-source">Na­gadomi’s <code>lbpcascade_animeface</code></a> (reg­u­lar face-de­tec­tion meth­ods do not work on anime im­ages)</li>
<li class="cyxy-trs-source">Delete empty files, mono­chrome or grayscale files, &amp; ex­ac­t-du­pli­cate files</li>
<li class="cyxy-trs-source">Con­vert to <span class="smallcaps-auto cyxy-trs-source">JPG</span></li>
<li class="cyxy-trs-source">Up­scale be­low the tar­get res­o­lu­tion (512px) im­ages with <a href="https://github.com/nagadomi/waifu2x" class="no-popup"><code>waifu2x</code></a></li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Con­vert all im­ages to <em class="cyxy-trs-source">ex­actly</em> 512×512 res­o­lu­tion s<span class="smallcaps-auto cyxy-trs-source">RGB</span> <span class="smallcaps-auto cyxy-trs-source">JPG</span> im­ages</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">If fea­si­ble, im­prove data qual­ity by check­ing for low-qual­ity im­ages by hand, re­mov­ing near-du­pli­cates im­ages found by <code>findimagedupes</code>, and fil­ter­ing with a pre­trained <span class="smallcaps-auto cyxy-trs-source">GAN</span>’s Dis­crim­i­na­tor</li>
<li class="cyxy-trs-source cyxy-trs-source">Con­vert to Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> for­mat us­ing <code>dataset_tool.py</code></li>
</ol>
<p class="cyxy-trs-source">The goal is to turn this:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="100 ran­dom real sam­ple im­ages from the 512px  sub­set of Dan­booru in a 10×10 grid." height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/danbooru2017-512px-samples.jpg" srcset="/images/gan/danbooru2017-512px-samples.jpg-768px.jpg 768w, /images/gan/danbooru2017-512px-samples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">100 ran­dom real sam­ple im­ages from the 512px <span class="smallcaps-auto cyxy-trs-source">SFW</span> sub­set of Dan­booru in a 10×10 grid.</figcaption></span></span></figure>
<p class="cyxy-trs-source">into this:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="36 ran­dom real sam­ple im­ages from the cropped Dan­booru faces in a 6×6 grid." height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/danbooru2017-512px-face-samples.jpg" srcset="/images/gan/danbooru2017-512px-face-samples.jpg-768px.jpg 768w, /images/gan/danbooru2017-512px-face-samples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">36 ran­dom real sam­ple im­ages from the cropped Dan­booru faces in a 6×6 grid.</figcaption></span></span></figure>
<p class="cyxy-trs-source">Be­low I use shell script­ing to pre­pare the dataset. A pos­si­ble al­ter­na­tive is <a href="https://github.com/reidsanders/danbooru-utility" class="no-popup"><code>danbooru-utility</code></a>⁠, which aims to help “ex­plore the dataset, fil­ter by tags, rat­ing, and score, de­tect faces, and re­size the im­ages”.</p>
<section id="cropping" class="level3">
<h3><a href="https://www.gwern.net/Faces#cropping" title="Link to section: § &#39;Cropping&#39;" class="no-popup cyxy-trs-source">Cropping</a></h3>
<p class="cyxy-trs-source">The <a href="https://www.gwern.net/Danbooru2020#download" class="link-local has-content spawns-popup">Dan­booru2018 down­load</a> can be done via Bit­Tor­rent or rsync, which pro­vides a <span class="smallcaps-auto">JSON</span> meta­data tar­ball which un­packs into <code>metadata/2*</code> &amp; a folder struc­ture of <code>{original,512px}/{0-999}/$ID.{png,jpg,...}</code>.</p>
<p class="cyxy-trs-source">For train­ing on <span class="smallcaps-auto">SFW</span> whole im­ages, the <code>512px/</code> ver­sion of Dan­booru2018 would work, but it is not a great idea for <em>faces</em> be­cause by scal­ing im­ages down to 512px, a lot of face de­tail has been lost, and get­ting high­-qual­ity faces is a chal­lenge. The <span class="smallcaps-auto">SFW</span> IDs can be ex­tracted from the file­names in <code>512px/</code> di­rectly or from the meta­data by ex­tract­ing the <code>id</code> &amp; <code>rating</code> fields (and sav­ing to a file):</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">find</span> ./512px/ <span class="at">-type</span> f <span class="kw">|</span> <span class="fu">sed</span> <span class="at">-e</span> <span class="st">'s/.*\/\([[:digit:]]*\)\.jpg/\1/'</span>
<span class="co"># 967769</span>
<span class="co"># 1853769</span>
<span class="co"># 2729769</span>
<span class="co"># 704769</span>
<span class="co"># 1799769</span>
<span class="co"># ...</span>
<span class="fu">tar</span> xf metadata.json.tar.xz
<span class="fu">cat</span> metadata/20180000000000<span class="pp">*</span> <span class="kw">|</span> <span class="ex">jq</span> <span class="st">'[.id, .rating]'</span> <span class="at">-c</span> <span class="kw">|</span> <span class="fu">fgrep</span> <span class="st">'"s"'</span> <span class="kw">|</span> <span class="fu">cut</span> <span class="at">-d</span> <span class="st">'"'</span> <span class="at">-f</span> 2 <span class="co"># "</span>
<span class="co"># ...</span></code></pre></div>
<p class="cyxy-trs-source">After in­stalling and test­ing <a href="https://github.com/nagadomi/lbpcascade_animeface" class="no-popup">Na­gadomi’s <code>lbpcascade_animeface</code></a> to make sure it &amp; <a href="https://en.wikipedia.org/wiki/OpenCV" class="docMetadata has-annotation spawns-popup">OpenCV</a> works, one can use <a href="https://github.com/nagadomi/lbpcascade_animeface/issues/1#issue-205363706" class="no-popup">a sim­ple script</a> which crops the face(s) from a sin­gle in­put im­age. The ac­cu­racy on Dan­booru im­ages is fairly good, per­haps 90% ex­cel­lent faces, 5% low-qual­ity faces (gen­uine but ei­ther aw­ful art or tiny lit­tle faces on the or­der of 64px which use­less), and 5% out­right er­rors—non-faces like armpits or el­bows (oddly enough). It can be im­proved by mak­ing the script more re­stric­tive, such as re­quir­ing 250×250px re­gions, which elim­i­nates most of the low-qual­ity faces &amp; mis­takes. (There is an al­ter­na­tive more-d­iffi­cult-to-run li­brary by Nakatomi which offers a face-crop­ping script, <a href="https://github.com/nagadomi/animeface-2009" class="no-popup">ani­me­face-2009</a>’s <a href="https://github.com/nagadomi/animeface-2009/blob/master/animeface-ruby/face_collector.rb" class="no-popup"><code>face_collector.rb</code></a>⁠, which Nakatomi says is bet­ter at crop­ping faces, but I was not im­pressed when I tried it out.) <code>crop.py</code>:</p>
<div class="sourceCode collapse" id="cb3"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Python"><code class="sourceCode python"><span class="im">import</span> cv2
<span class="im">import</span> sys
<span class="im">import</span> os.path

<span class="kw">def</span> detect(cascade_file, filename, outputname):
    <span class="cf">if</span> <span class="kw">not</span> os.path.isfile(cascade_file):
        <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"</span><span class="sc">%s</span><span class="st">: not found"</span> <span class="op">%</span> cascade_file)

    cascade <span class="op">=</span> cv2.CascadeClassifier(cascade_file)
    image <span class="op">=</span> cv2.imread(filename)
    gray <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    gray <span class="op">=</span> cv2.equalizeHist(gray)

    <span class="co">## </span><span class="al">NOTE</span><span class="co">: Suggested modification: increase minSize to '(250,250)' px,</span>
    <span class="co">## increasing proportion of high-quality faces &amp; reducing</span>
    <span class="co">## false positives. Faces which are only 50×50px are useless</span>
    <span class="co">## and often not faces at all.</span>
    <span class="co">## FOr my StyleGANs, I use 250 or 300px boxes</span>

    faces <span class="op">=</span> cascade.detectMultiScale(gray,
                                     <span class="co"># detector options</span>
                                     scaleFactor <span class="op">=</span> <span class="fl">1.1</span>,
                                     minNeighbors <span class="op">=</span> <span class="dv">5</span>,
                                     minSize <span class="op">=</span> (<span class="dv">50</span>, <span class="dv">50</span>))
    i<span class="op">=</span><span class="dv">0</span>
    <span class="cf">for</span> (x, y, w, h) <span class="kw">in</span> faces:
        cropped <span class="op">=</span> image[y: y <span class="op">+</span> h, x: x <span class="op">+</span> w]
        cv2.imwrite(outputname<span class="op">+</span><span class="bu">str</span>(i)<span class="op">+</span><span class="st">".png"</span>, cropped)
        i<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>

<span class="cf">if</span> <span class="bu">len</span>(sys.argv) <span class="op">!=</span> <span class="dv">4</span>:
    sys.stderr.write(<span class="st">"usage: detect.py &lt;animeface.xml file&gt;  &lt;input&gt; &lt;output prefix&gt;</span><span class="ch">\n</span><span class="st">"</span>)
    sys.exit(<span class="op">-</span><span class="dv">1</span>)

detect(sys.argv[<span class="dv">1</span>], sys.argv[<span class="dv">2</span>], sys.argv[<span class="dv">3</span>])</code></pre></div>
<p class="cyxy-trs-source">The IDs can be com­bined with the pro­vided <code>lbpcascade_animeface</code> script us­ing <code>xargs</code>, how­ever this will be far too slow and it would be bet­ter to ex­ploit par­al­lelism with <code>xargs --max-args=1 --max-procs=16</code> or <code>parallel</code>. It’s also worth not­ing that <code>lbpcascade_animeface</code> seems to use up <span class="smallcaps-auto">GPU</span> <span class="smallcaps-auto">VRAM</span> even though <span class="smallcaps-auto">GPU</span> use offers no ap­par­ent speedup (a slow­down if any­thing, given lim­ited <span class="smallcaps-auto">VRAM</span>), so I find it helps to ex­plic­itly dis­able <span class="smallcaps-auto">GPU</span> use by set­ting <code>CUDA_VISIBLE_DEVICES=""</code>. (For this step, it’s quite help­ful to have a many-core sys­tem like a Thread­rip­per.)</p>
<p class="cyxy-trs-source">Com­bin­ing every­thing, par­al­lel face-crop­ping of an en­tire Dan­booru2018 sub­set can be done like this:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">cropFaces()</span> <span class="kw">{</span>
    <span class="va">BUCKET</span><span class="op">=</span><span class="va">$(</span><span class="bu">printf</span> <span class="st">"%04d"</span> <span class="va">$((</span> <span class="va">$@</span> <span class="op">%</span> <span class="dv">1000</span> <span class="va">))</span> <span class="va">)</span>
    <span class="va">ID</span><span class="op">=</span><span class="st">"</span><span class="va">$@</span><span class="st">"</span>
    <span class="va">CUDA_VISIBLE_DEVICES</span><span class="op">=</span><span class="st">""</span> <span class="fu">nice</span> python ~/src/lbpcascade_animeface/examples/crop.py  <span class="dt">\</span>
     ~/src/lbpcascade_animeface/lbpcascade_animeface.xml <span class="dt">\</span>
     ./original/<span class="va">$BUCKET</span>/<span class="va">$ID</span>.<span class="pp">*</span> <span class="st">"./faces/</span><span class="va">$ID</span><span class="st">"</span>
<span class="kw">}</span>
<span class="bu">export</span> <span class="at">-f</span> <span class="va">cropFaces</span>

<span class="fu">mkdir</span> ./faces/
<span class="fu">cat</span> sfw-ids.txt <span class="kw">|</span> <span class="ex">parallel</span> <span class="at">--progress</span> cropFaces

<span class="co"># </span><span class="al">NOTE</span><span class="co">: because of the possibility of multiple crops from an image, the script appends a N counter;</span>
<span class="co"># remove that to get back the original ID &amp; filepath: eg</span>
<span class="co">#</span>
<span class="co">## original/0196/933196.jpg  → portrait/9331961.jpg</span>
<span class="co">## original/0669/1712669.png → portrait/17126690.jpg</span>
<span class="co">## original/0997/3093997.jpg → portrait/30939970.jpg</span></code></pre></div>
<p class="cyxy-trs-source">Nvidia Style<span class="smallcaps-auto">GAN</span>, by de­fault and like most im­age-re­lated tools, ex­pects square im­ages like 512×512px, but there is noth­ing in­her­ent to neural nets or con­vo­lu­tions that re­quires square in­puts or out­puts, and rec­tan­gu­lar con­vo­lu­tions are pos­si­ble. In the case of faces, they tend to be more rec­tan­gu­lar than square, and we’d pre­fer to use a rec­tan­gu­lar con­vo­lu­tion if pos­si­ble to fo­cus the im­age on the rel­e­vant di­men­sion rather than ei­ther pay the se­vere per­for­mance penalty of in­creas­ing to­tal di­men­sions to 1024×1024px or stick with 512×512px &amp; waste im­age out­puts on emit­ting black bars/<wbr>back­grounds. A prop­er­ly-sized rec­tan­gu­lar con­vo­lu­tion can offer a nice speedup (eg <a href="https://www.gwern.net/docs/www/www.fast.ai/2487d10f9a64eedda53c4a8456ae434227f01247.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://www.fast.ai/2018/08/10/fastai-diu-imagenet/" data-attribute-title="Now anyone can train Imagenet in 18 minutes (Original URL: http://www.fast.ai/2018/08/10/fastai-diu-imagenet/ )">Fast.ai’s train­ing Im­a­geNet in 18m</a> for $40 us­ing them among other trick­s). Nolan Ken­t’s <a href="https://github.com/nolan-dev/stylegan_reimplementation" class="no-popup">Style<span class="smallcaps-auto">GAN</span> re-im­ple­men­tion</a> (re­leased Oc­to­ber 2019) does sup­port rec­tan­gu­lar con­vo­lu­tions, and as <a href="https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e" title="Animating gAnime with StyleGAN: Part 1 - Introducing a tool for interacting with generative models" class="no-popup">he demon­strates in his blog post</a>⁠, it works nice­ly.</p>
</section>
<section id="cleaning-upscaling" class="level3">
<h3><a href="https://www.gwern.net/Faces#cleaning-upscaling" title="Link to section: § &#39;Cleaning &amp; Upscaling&#39;" class="no-popup cyxy-trs-source">Cleaning &amp; Upscaling</a></h3>
<p class="cyxy-trs-source">Mis­cel­la­neous cleanups can be done:</p>
<div class="sourceCode collapse" id="cb5"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co">## Delete failed/empty files</span>
<span class="fu">find</span> faces/ <span class="at">-size</span> 0    <span class="at">-type</span> f <span class="at">-delete</span>

<span class="co">## Delete 'too small' files which is indicative of low quality:</span>
<span class="fu">find</span> faces/ <span class="at">-size</span> <span class="at">-40k</span> <span class="at">-type</span> f <span class="at">-delete</span>

<span class="co">## Delete exact duplicates:</span>
<span class="ex">fdupes</span> <span class="at">--delete</span> <span class="at">--omitfirst</span> <span class="at">--noprompt</span> faces/

<span class="co">## Delete monochrome or minimally-colored images:</span>
<span class="co">### the heuristic of &lt;257 unique colors is imperfect but better than anything else I tried</span>
<span class="fu">deleteBW()</span> <span class="kw">{</span> <span class="cf">if</span> <span class="kw">[[</span> <span class="kw">`</span><span class="ex">identify</span> <span class="at">-format</span> <span class="st">"%k"</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span><span class="kw">`</span> <span class="ot">-lt</span> 257 <span class="kw">]];</span>
             <span class="cf">then</span> <span class="fu">rm</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span><span class="kw">;</span> <span class="cf">fi</span><span class="kw">;</span> <span class="kw">}</span>
<span class="bu">export</span> <span class="at">-f</span> <span class="va">deleteBW</span>
<span class="fu">find</span> faces <span class="at">-type</span> f <span class="kw">|</span> <span class="ex">parallel</span> <span class="at">--progress</span> deleteBW</code></pre></div>
<p class="cyxy-trs-source">I re­move black­-white or grayscale im­ages from all my <span class="smallcaps-auto">GAN</span> ex­per­i­ments be­cause in my ear­li­est ex­per­i­ments, their in­clu­sion ap­peared to in­crease in­sta­bil­i­ty: mixed datasets were ex­tremely un­sta­ble, mono­chrome datasets failed to learn at all, but col­or-only runs made some progress. It is likely that Style<span class="smallcaps-auto">GAN</span> is now pow­er­ful enough to be able to learn on mixed datasets (and some later ex­per­i­ments by other peo­ple sug­gest that Style<span class="smallcaps-auto">GAN</span> can han­dle both mono­chrome &amp; color ani­me-style faces with­out a prob­lem), but I have not risked a full mon­th-long run to in­ves­ti­gate, and so I con­tinue do­ing col­or-on­ly.</p>
<section id="discriminator-ranking" class="level4">
<h4><a href="https://www.gwern.net/Faces#discriminator-ranking" title="Link to section: § &#39;Discriminator ranking&#39;" class="no-popup cyxy-trs-source">Discriminator ranking</a></h4>
<p class="cyxy-trs-source">A good trick with <span class="smallcaps-auto">GAN</span>s is, after train­ing to rea­son­able lev­els of qual­i­ty, reusing the Dis­crim­i­na­tor to rank the real dat­a­points; im­ages the trained D as­signs the low­est prob­a­bil­i­ty/<wbr>s­core of be­ing real are often the worst-qual­ity ones and go­ing through the bot­tom decile (or delet­ing them en­tire­ly) should re­move many anom­alies and may im­prove the <span class="smallcaps-auto">GAN</span>. The <span class="smallcaps-auto">GAN</span> is then trained on the new cleaned dataset, mak­ing this a kind of “ac­tive learn­ing”.</p>
<p class="cyxy-trs-source">Since rat­ing im­ages is what the D al­ready does, no new al­go­rithms or train­ing meth­ods are nec­es­sary, and al­most no code is nec­es­sary: run the D on the whole dataset to rank each im­age (faster than it seems since the G &amp; back­prop­a­ga­tion are un­nec­es­sary, even a large dataset can be ranked in a wall­clock hour or two), then one can re­view man­u­ally the bot­tom &amp; top X%, or per­haps just delete the bot­tom X% sight un­seen if enough data is avail­able.</p>
<div class="collapse"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><div class="">
<div class="collapseSummary">
<p class="cyxy-trs-source">What is a D do­ing? I find that the <em>high­est</em> ranked im­ages often con­tain many anom­alies or low-qual­ity im­ages which need to be delet­ed. Why? The <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=6" id="brock-et-al-2019-6" class="docMetadata localArchive has-annotation spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=6" data-attribute-title="(Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=6 )">Big<span class="smallcaps-auto">GAN</span> pa­per</a> notes a well-trained D which achieves 98% real vs fake clas­si­fi­ca­tion per­for­mance on the Im­a­geNet train­ing dataset falls to 50–55% ac­cu­racy when run on the val­i­da­tion dataset, sug­gest­ing the D’s role is about mem­o­riz­ing the train­ing data rather than some mea­sure of ‘re­al­ism’.</p>
</div>
<p class="cyxy-trs-source">Per­haps be­cause the D rank­ing is not nec­es­sar­ily a ‘qual­ity’ score but sim­ply a sort of con­fi­dence rat­ing that an im­age is from the real dataset; if the real im­ages con­tain cer­tain eas­i­ly-de­tectable im­ages which the G can’t repli­cate, then the D might mem­o­rize or learn them quick­ly. For ex­am­ple, in face crops, whole fig­ure crops are com­mon mis­taken crops, mak­ing up a tiny per­cent­age of im­ages; how could a face-only G learn to gen­er­ate whole re­al­is­tic bod­ies with­out the in­ter­me­di­ate steps be­ing in­stantly de­tected &amp; de­feated as er­rors by D, while D is eas­ily able to de­tect re­al­is­tic bod­ies as defi­nitely re­al? This would ex­plain the po­lar­ized rank­ings. And given the close con­nec­tions be­tween <span class="smallcaps-auto">GAN</span>s &amp; <span class="smallcaps-auto">DRL</span>, I have to won­der if there is more mem­o­riza­tion go­ing on than sus­pected in things like <a href="https://arxiv.org/abs/1706.03741" id="christiano-et-al-2017-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Christiano et al 2017">“Deep re­in­force­ment learn­ing from hu­man pref­er­ences”</a>? In­ci­den­tal­ly, this may also ex­plain the prob­lem with us­ing Dis­crim­i­na­tors for semi­-su­per­vised rep­re­sen­ta­tion learn­ing: if the D is mem­o­riz­ing dat­a­points to force the G to gen­er­al­ize, then its in­ter­nal rep­re­sen­ta­tions would be ex­pected to be use­less. (One would in­stead want to ex­tract knowl­edge from the G, per­haps by en­cod­ing an im­age into <em>z</em> and us­ing the <em>z</em> as the rep­re­sen­ta­tion.)</p>
<p class="cyxy-trs-source">An al­ter­na­tive per­spec­tive is offered by a crop of 2020 pa­pers (<a href="https://arxiv.org/abs/2006.02595#google" id="zhao-et-al-2020-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Image Augmentations for GAN Training">Zhao&nbsp;et&nbsp;al&nbsp;2020b</a>⁠; <a href="https://arxiv.org/abs/2006.05338" id="tran-et-al-2020-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Towards Good Practices for Data Augmentation in GAN Training">Tran&nbsp;et&nbsp;al&nbsp;2020</a>⁠; <a href="https://arxiv.org/abs/2006.06676#nvidia" id="karras-et-al-2020-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="ADA/StyleGAN3: Training Generative Adversarial Networks with Limited Data">Kar­ras&nbsp;et&nbsp;al&nbsp;2020</a>⁠; <a href="https://arxiv.org/abs/2006.10738" id="zhao-et-al-2020-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Differentiable Augmentation for Data-Efficient GAN Training">Zhao&nbsp;et&nbsp;al&nbsp;2020c</a>) ex­am­in­ing how use­ful <span class="smallcaps-auto">GAN</span> data aug­men­ta­tion re­quires it to be done <em>dur­ing train­ing</em>, and one must aug­ment <em>all</em> im­ages.<a href="https://www.gwern.net/Faces#sn23" class="footnote-ref spawns-popup" id="fnref23" role="doc-noteref"><sup>23</sup></a> <a href="https://www.gwern.net/docs/www/arxiv.org/02b44c97b373b5b558255ee66a81edd997b1ea67.pdf#figure.caption.2" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2006.10738.pdf#figure.caption.2" data-attribute-title="Here we analyze the performance of BigGAN [2] with different amounts of data on CIFAR-10. As plotted in Figure 1, even given 100% data, the gap between the discriminator’s training and validation accuracy keeps increasing, suggesting that the discriminator is simply memorizing the training images...Figure 6 analyzes that stronger DiffAugment policies generally maintain a higher discriminator’s validation accuracy at the cost of a lower training accuracy, alleviate the overfitting problem, and eventually achieve better convergence. (Original URL: https://arxiv.org/pdf/2006.10738.pdf#figure.caption.2 )">Zhao&nbsp;et&nbsp;al&nbsp;2020c</a> &amp; <a href="https://arxiv.org/pdf/2006.06676#figure.caption.2" title="Figure 1a shows our baseline results for different subsets of FFHQ. Training starts the same way in each case, but eventually the progress stops and FID starts to rise. The less training data there is, the earlier this happens. Figure 1b,c shows the discriminator output distributions for real and generated images during training. The distributions overlap initially but keep drifting apart as the discriminator becomes more and more confident, and the point where FID starts to deteriorate is consistent with the loss of sufficient overlap between distributions. This is a strong indication of overfitting, evidenced further by the drop in accuracy measured for a separate validation set." class="no-popup">Kar­ras&nbsp;et&nbsp;al&nbsp;2020</a> ob­serve, with reg­u­lar <span class="smallcaps-auto">GAN</span> train­ing, there is a strik­ing steady de­cline of D per­for­mance on held­out data, and in­crease on train­ing data, through­out the course of train­ing, con­firm­ing the Big<span class="smallcaps-auto">GAN</span> ob­ser­va­tion but also show­ing it is a dy­namic phe­nom­e­non, and prob­a­bly a bad one. Adding in cor­rect data aug­men­ta­tion re­duces this over­fit­ting—and markedly im­proves sam­ple-effi­ciency &amp; fi­nal qual­i­ty. This sug­gests that the D does in­deed mem­o­rize, but that this is not a good thing. Kar­ras&nbsp;et&nbsp;al&nbsp;2020 de­scribes what hap­pens as</p>
<blockquote>
<p class="cyxy-trs-source">Con­ver­gence is now achieved [with <span class="smallcaps-auto">ADA</span>/data aug­men­ta­tion] re­gard­less of the train­ing set size and over­fit­ting no longer oc­curs. With­out aug­men­ta­tions, the gra­di­ents the gen­er­a­tor re­ceives from the dis­crim­i­na­tor be­come very sim­plis­tic over time—the dis­crim­i­na­tor starts to pay at­ten­tion to only a hand­ful of fea­tures, and the gen­er­a­tor is free to cre­ate oth­er­wise non­sen­si­cal im­ages. With <span class="smallcaps-auto">ADA</span>, the gra­di­ent field stays much more de­tailed which pre­vents such de­te­ri­o­ra­tion.</p>
</blockquote>
<p class="cyxy-trs-source">In other words, just as the G can ‘mode col­lapse’ by fo­cus­ing on gen­er­at­ing im­ages with only a few fea­tures, the D can also ‘fea­ture col­lapse’ by fo­cus­ing on a few fea­tures which hap­pen to cor­rectly split the train­ing data’s re­als from fakes, such as by mem­o­riz­ing them out­right. This tech­ni­cally works, but not well. This also ex­plains why <a href="https://arxiv.org/pdf/1809.11096.pdf&amp;org=deepmind#page=8" id="brock-et-al-2018-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;BigGAN: Large Scale GAN Training For High Fidelity Natural Image Synthesis: 5.2 Additional Evaluation On JFT-300M&#39;, Brock et al 2018">Big<span class="smallcaps-auto">GAN</span> train­ing sta­bi­lized</a> when train­ing on <span class="smallcaps-auto">JFT-300M</span>: di­ver­gence/<wbr>­col­lapse usu­ally starts with D win­ning; if D wins be­cause it mem­o­rizes, then a suffi­ciently large dataset should make mem­o­riza­tion in­fea­si­ble; and <span class="smallcaps-auto">JFT-300M</span> turns out to be suffi­ciently large. (This would pre­dict that if Brock et al had checked the <span class="smallcaps-auto">JFT-300M</span> Big<span class="smallcaps-auto">GAN</span> D’s clas­si­fi­ca­tion per­for­mance on a held-out <span class="smallcaps-auto">JFT-300M</span>, rather than just on their Im­a­geNet Big<span class="smallcaps-auto">GAN</span>, then they would have found that it clas­si­fied re­als vs fake well above chance.)</p>
<p class="cyxy-trs-source">If so, this sug­gests that for D rank­ing, it may not be too use­ful to take the D from the end of a run, if not us­ing data aug­men­ta­tion, be­cause that D be the ver­sion with the great­est de­gree of mem­o­riza­tion!</p>
</div></div>
<p class="cyxy-trs-source">Here is a sim­ple Style<span class="smallcaps-auto">GAN</span>2 script (<code>ranker.py</code>) to open a Style<span class="smallcaps-auto">GAN</span> <code>.pkl</code> and run it on a list of im­age file­names to print out the D score, <a href="https://github.com/xunings/styleganime2/blob/master/misc/ranker.py" class="no-popup">cour­tesy of Shao Xun­ing</a>:</p>
<div class="sourceCode collapse" id="cb6"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Python"><code class="sourceCode python"><span class="im">import</span> pickle
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> cv2
<span class="im">import</span> dnnlib.tflib <span class="im">as</span> tflib
<span class="im">import</span> random
<span class="im">import</span> argparse
<span class="im">import</span> PIL.Image
<span class="im">from</span> training.misc <span class="im">import</span> adjust_dynamic_range


<span class="kw">def</span> preprocess(file_path):
    <span class="co"># print(file_path)</span>
    img <span class="op">=</span> np.asarray(PIL.Image.<span class="bu">open</span>(file_path))

    <span class="co"># Preprocessing from dataset_tool.create_from_images</span>
    img <span class="op">=</span> img.transpose([<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>])  <span class="co"># HWC =&gt; CHW</span>
    <span class="co"># img = np.expand_dims(img, axis=0)</span>
    img <span class="op">=</span> img.reshape((<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">512</span>, <span class="dv">512</span>))

    <span class="co"># Preprocessing from training_loop.process_reals</span>
    img <span class="op">=</span> adjust_dynamic_range(data<span class="op">=</span>img, drange_in<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">255</span>], drange_out<span class="op">=</span>[<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>])
    <span class="cf">return</span> img


<span class="kw">def</span> main(args):
    random.seed(args.random_seed)
    minibatch_size <span class="op">=</span> args.minibatch_size
    input_shape <span class="op">=</span> (minibatch_size, <span class="dv">3</span>, <span class="dv">512</span>, <span class="dv">512</span>)
    <span class="co"># print(args.images)</span>
    images <span class="op">=</span> args.images
    images.sort()

    tflib.init_tf()
    _G, D, _Gs <span class="op">=</span> pickle.load(<span class="bu">open</span>(args.model, <span class="st">"rb"</span>))
    <span class="co"># D.print_layers()</span>

    image_score_all <span class="op">=</span> [(image, []) <span class="cf">for</span> image <span class="kw">in</span> images]

    <span class="co"># Shuffle the images and process each image in multiple minibatches.</span>
    <span class="co"># Note: networks.stylegan2.minibatch_stddev_layer</span>
    <span class="co"># calculates the standard deviation of a minibatch group as a feature channel,</span>
    <span class="co"># which means that the output of the discriminator actually depends</span>
    <span class="co"># on the companion images in the same minibatch.</span>
    <span class="cf">for</span> i_shuffle <span class="kw">in</span> <span class="bu">range</span>(args.num_shuffles):
        <span class="co"># print('shuffle: {}'.format(i_shuffle))</span>
        random.shuffle(image_score_all)
        <span class="cf">for</span> idx_1st_img <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(image_score_all), minibatch_size):
            idx_img_minibatch <span class="op">=</span> []
            images_minibatch <span class="op">=</span> []
            input_minibatch <span class="op">=</span> np.zeros(input_shape)
            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(minibatch_size):
                idx_img <span class="op">=</span> (idx_1st_img <span class="op">+</span> i) <span class="op">%</span> <span class="bu">len</span>(image_score_all)
                idx_img_minibatch.append(idx_img)
                image <span class="op">=</span> image_score_all[idx_img][<span class="dv">0</span>]
                images_minibatch.append(image)
                img <span class="op">=</span> preprocess(image)
                input_minibatch[i, :] <span class="op">=</span> img
            output <span class="op">=</span> D.run(input_minibatch, <span class="va">None</span>, resolution<span class="op">=</span><span class="dv">512</span>)
            <span class="bu">print</span>(<span class="st">'shuffle: </span><span class="sc">{}</span><span class="st">, indices: </span><span class="sc">{}</span><span class="st">, images: </span><span class="sc">{}</span><span class="st">'</span>
                  .<span class="bu">format</span>(i_shuffle, idx_img_minibatch, images_minibatch))
            <span class="bu">print</span>(<span class="st">'Output: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(output))
            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(minibatch_size):
                idx_img <span class="op">=</span> idx_img_minibatch[i]
                image_score_all[idx_img][<span class="dv">1</span>].append(output[i][<span class="dv">0</span>])

    <span class="cf">with</span> <span class="bu">open</span>(args.output, <span class="st">'a'</span>) <span class="im">as</span> fout:
        <span class="cf">for</span> image, score_list <span class="kw">in</span> image_score_all:
            <span class="bu">print</span>(<span class="st">'Image: </span><span class="sc">{}</span><span class="st">, score_list: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(image, score_list))
            avg_score <span class="op">=</span> <span class="bu">sum</span>(score_list)<span class="op">/</span><span class="bu">len</span>(score_list)
            fout.write(image <span class="op">+</span> <span class="st">' '</span> <span class="op">+</span> <span class="bu">str</span>(avg_score) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)


<span class="kw">def</span> parse_arguments():
    parser <span class="op">=</span> argparse.ArgumentParser()
    parser.add_argument(<span class="st">'--model'</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">str</span>, required<span class="op">=</span><span class="va">True</span>,
                        <span class="bu">help</span><span class="op">=</span><span class="st">'.pkl model'</span>)
    parser.add_argument(<span class="st">'--images'</span>, nargs<span class="op">=</span><span class="st">'+'</span>)
    parser.add_argument(<span class="st">'--output'</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">str</span>, default<span class="op">=</span><span class="st">'rank.txt'</span>)
    parser.add_argument(<span class="st">'--minibatch_size'</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, default<span class="op">=</span><span class="dv">4</span>)
    parser.add_argument(<span class="st">'--num_shuffles'</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, default<span class="op">=</span><span class="dv">5</span>)
    parser.add_argument(<span class="st">'--random_seed'</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, default<span class="op">=</span><span class="dv">0</span>)
    <span class="cf">return</span> parser.parse_args()


<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:
    main(parse_arguments())</code></pre></div>
<p class="cyxy-trs-source">De­pend­ing on how noisy the rank­ings are in terms of ‘qual­ity’ and avail­able sam­ple size, one can ei­ther re­view the worst-ranked im­ages by hand, or delete the bot­tom X%. One should check the top-ranked im­ages as well to make sure the or­der­ing is right; there can also be some odd im­ages in the top X% as well which should be re­moved.</p>
<p class="cyxy-trs-source">It might be pos­si­ble to use <code>ranker.py</code> to im­prove the qual­ity of gen­er­ated sam­ples as well, as a sim­ple ver­sion of <a href="https://arxiv.org/abs/1810.06758" id="azadi-et-al-2018-0" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Discriminator Rejection Sampling&#39;, Azadi et al 2018">dis­crim­i­na­tor re­jec­tion sam­pling</a>⁠.</p>
</section>
<section id="upscaling" class="level4">
<h4><a href="https://www.gwern.net/Faces#upscaling" title="Link to section: § &#39;Upscaling&#39;" class="no-popup cyxy-trs-source">Upscaling</a></h4>
<p class="cyxy-trs-source">The next ma­jor step is up­scal­ing im­ages us­ing <a href="https://github.com/nagadomi/waifu2x" class="no-popup"><code>waifu2x</code></a>⁠, which does an ex­cel­lent job on 2× up­scal­ing of anime im­ages, which are nigh-indis­tin­guish­able from a high­er-res­o­lu­tion orig­i­nal and greatly in­crease the us­able cor­pus. The down­side is that it can take 1–10s per im­age, must run on the <span class="smallcaps-auto">GPU</span> (I can re­li­ably fit ~9 in­stances on my 2×1080ti), and is writ­ten in a now-un­main­tained DL frame­work, Torch, with <a href="https://github.com/nagadomi/waifu2x/issues/231#issuecomment-381164157" class="no-popup">no cur­rent plans to port to Py­Torch</a>⁠, and is grad­u­ally be­com­ing harder to get run­ning (one hopes that by the time <span class="smallcaps-auto">CUDA</span> up­dates break it en­tire­ly, there will be an­other su­per-res­o­lu­tion <span class="smallcaps-auto">GAN</span> I or some­one else can train on Dan­booru to re­place it). If pressed for time, one can just up­scale the faces nor­mally with Im­ageMag­ick but I be­lieve there will be some qual­ity loss and it’s worth­while.</p>
<div class="sourceCode collapse" id="cb7"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="bu">.</span> ~/src/torch/install/bin/torch-activate
<span class="fu">upscaleWaifu2x()</span> <span class="kw">{</span>
    <span class="va">SIZE1</span><span class="op">=</span><span class="va">$(</span><span class="ex">identify</span> <span class="at">-format</span> <span class="st">"%h"</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span><span class="va">)</span>
    <span class="va">SIZE2</span><span class="op">=</span><span class="va">$(</span><span class="ex">identify</span> <span class="at">-format</span> <span class="st">"%w"</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span><span class="va">)</span><span class="kw">;</span>

    <span class="cf">if</span> <span class="kw">((</span> <span class="va">$SIZE1</span> <span class="op">&lt;</span> <span class="dv">512</span> <span class="kw">&amp;&amp;</span> <span class="va">$SIZE2</span> <span class="op">&lt;</span> <span class="dv">512</span>  <span class="kw">));</span> <span class="cf">then</span>
        <span class="bu">echo</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span> <span class="va">$SIZE</span>
        <span class="va">TMP</span><span class="op">=</span><span class="va">$(</span><span class="fu">mktemp</span> <span class="st">"/tmp/XXXXXX.png"</span><span class="va">)</span>
        <span class="va">CUDA_VISIBLE_DEVICES</span><span class="op">=</span><span class="st">"</span><span class="va">$((RANDOM</span> <span class="op">%</span> <span class="dv">2</span> <span class="op">&lt;</span> <span class="dv">1</span><span class="va">))</span><span class="st">"</span> <span class="fu">nice</span> th ~/src/waifu2x/waifu2x.lua <span class="at">-model_dir</span> <span class="dt">\</span>
            ~/src/waifu2x/models/upconv_7/art <span class="at">-tta</span> 1 <span class="at">-m</span> scale <span class="at">-scale</span> 2 <span class="dt">\</span>
            <span class="at">-i</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span> <span class="at">-o</span> <span class="st">"</span><span class="va">$TMP</span><span class="st">"</span>
        <span class="ex">convert</span> <span class="st">"</span><span class="va">$TMP</span><span class="st">"</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span>
        <span class="fu">rm</span> <span class="st">"</span><span class="va">$TMP</span><span class="st">"</span>
    <span class="cf">fi</span><span class="kw">;</span>  <span class="kw">}</span>

<span class="bu">export</span> <span class="at">-f</span> <span class="va">upscaleWaifu2x</span>
<span class="fu">find</span> faces/ <span class="at">-type</span> f <span class="kw">|</span> <span class="ex">parallel</span> <span class="at">--progress</span> <span class="at">--jobs</span> 9 upscaleWaifu2x</code></pre></div>
</section>
</section>
<section id="quality-checks-data-augmentation" class="level3">
<h3><a href="https://www.gwern.net/Faces#quality-checks-data-augmentation" title="Link to section: § &#39;Quality Checks &amp; Data Augmentation&#39;" class="no-popup cyxy-trs-source">Quality Checks &amp; Data Augmentation</a></h3>
<p class="cyxy-trs-source">The sin­gle most effec­tive strat­egy to im­prove a <span class="smallcaps-auto">GAN</span> is to clean the da­ta. Style<span class="smallcaps-auto">GAN</span> can­not han­dle too-di­verse datasets com­posed of mul­ti­ple ob­jects or sin­gle ob­jects shifted around, and rare or odd im­ages can­not be learned well. Kar­ras et al get such good re­sults with Style<span class="smallcaps-auto">GAN</span> on faces in part be­cause they con­structed <span class="smallcaps-auto">FFHQ</span> to be an ex­tremely clean con­sis­tent dataset of just cen­tered well-lit clear hu­man faces with­out any ob­struc­tions or other vari­a­tion. Sim­i­lar­ly, Ar­fa’s <a href="https://www.thisfursonadoesnotexist.com/" id="arfafax-2020" class="docMetadata has-annotation spawns-popup">“This Fur­sona Does Not Ex­ist”</a> (<span class="smallcaps-auto">TFDNE</span>) S2 gen­er­ates much bet­ter <a href="https://en.wikipedia.org/wiki/Furry_fandom" class="docMetadata has-annotation spawns-popup">furry</a> por­traits than my own “This Waifu Does Not Ex­ist” (<span class="smallcaps-auto">TWDNE</span>) S2 anime por­traits, due partly to train­ing longer to con­ver­gence on a <span class="smallcaps-auto">TPU</span> pod but mostly due to his in­vest­ment in data clean­ing: align­ing the faces and heavy fil­ter­ing of sam­ples—this left him with only <em>n</em> = 50k but <span class="smallcaps-auto">TFDNE</span> nev­er­the­less out­per­forms <span class="smallcaps-auto">TWDNE</span>’s <em>n</em> = 300k. (Data clean­ing/<wbr>aug­men­ta­tion is one of the more pow­er­ful ways to im­prove re­sults; if we imag­ine deep learn­ing as ‘pro­gram­ming’ or ‘Soft­ware 2.0’<a href="https://www.gwern.net/Faces#sn24" class="footnote-ref spawns-popup" id="fnref24" role="doc-noteref"><sup>24</sup></a> in An­drej Karpa­thy’s terms, data clean­ing/<wbr>aug­men­ta­tion is one of the eas­i­est ways to fine­tune the loss func­tion to­wards what we <em>re­ally</em> want by gar­den­ing our data to re­move what we don’t want and in­crease what we do.)</p>
<p class="cyxy-trs-source">At this point, one can do man­ual qual­ity checks by view­ing a few hun­dred im­ages, run­ning <code>findimagedupes -t 99%</code> to look for near-i­den­ti­cal faces, or dab­ble in fur­ther mod­i­fi­ca­tions such as do­ing “data aug­men­ta­tion”. Work­ing with Dan­booru2018, at this point one would have ~600–700,000 faces, which is more than enough to train Style<span class="smallcaps-auto">GAN</span> and one will have diffi­culty stor­ing the fi­nal Style<span class="smallcaps-auto">GAN</span> dataset be­cause of its sheer size (due to the ~18× size mul­ti­pli­er). After clean­ing etc, my fi­nal face dataset is the <a href="https://www.gwern.net/Crops#danbooru2019-portraits" id="gwern-crops-danbooru2019-portraits" class="link-local docMetadata has-annotation spawns-popup">por­trait dataset</a> with <em>n</em> = 300k.</p>
<p class="cyxy-trs-source">How­ev­er, if that is not enough or one is work­ing with a small dataset like for a sin­gle char­ac­ter, data aug­men­ta­tion may be nec­es­sary. The mir­ror/<wbr>hor­i­zon­tal flip is not nec­es­sary as Style<span class="smallcaps-auto">GAN</span> has that built-in as an op­tion<a href="https://www.gwern.net/Faces#sn25" class="footnote-ref spawns-popup" id="fnref25" role="doc-noteref"><sup>25</sup></a>⁠, but there are many other pos­si­ble data aug­men­ta­tions. One can stretch, shift col­ors, sharp­en, blur, in­crease/<wbr>de­crease con­trast/<wbr>bright­ness, crop, and so on. An ex­am­ple, ex­tremely ag­gres­sive, set of data aug­men­ta­tions could be done like this:</p>
<div class="sourceCode collapse" id="cb8"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">dataAugment ()</span> <span class="kw">{</span>
    <span class="va">image</span><span class="op">=</span><span class="st">"</span><span class="va">$@</span><span class="st">"</span>
    <span class="va">target</span><span class="op">=</span><span class="va">$(</span><span class="fu">basename</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span><span class="va">)</span>
    <span class="va">suffix</span><span class="op">=</span><span class="st">"png"</span>
    <span class="ex">convert</span> <span class="at">-deskew</span> 50                     <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.deskew.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-resize</span> 110%x100%              <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.horizstretch.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-resize</span> 100%x110%              <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.vertstretch.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-blue-shift</span> 1.1                <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.midnight.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-fill</span> red <span class="at">-colorize</span> 5%         <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.red.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-fill</span> orange <span class="at">-colorize</span> 5%      <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.orange.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-fill</span> yellow <span class="at">-colorize</span> 5%      <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.yellow.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-fill</span> green <span class="at">-colorize</span> 5%       <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.green.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-fill</span> blue <span class="at">-colorize</span> 5%        <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.blue.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-fill</span> purple <span class="at">-colorize</span> 5%      <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.purple.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-adaptive-blur</span> 3x2             <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.blur.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-adaptive-sharpen</span> 4x2          <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.sharpen.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-brightness-contrast</span> 10        <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.brighter.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-brightness-contrast</span> 10x10     <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.brightercontraster.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-brightness-contrast</span> <span class="at">-10</span>       <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.darker.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-brightness-contrast</span> <span class="at">-10x10</span>    <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.darkerlesscontrast.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> +level 5%                      <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.contraster.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
    <span class="ex">convert</span> <span class="at">-level</span> 5%<span class="dt">\!</span>                    <span class="st">"</span><span class="va">$image</span><span class="st">"</span> <span class="st">"</span><span class="va">$target</span><span class="st">"</span>.lesscontrast.<span class="st">"</span><span class="va">$suffix</span><span class="st">"</span>
  <span class="kw">}</span>
<span class="bu">export</span> <span class="at">-f</span> <span class="va">dataAugment</span>
<span class="fu">find</span> faces/ <span class="at">-type</span> f <span class="kw">|</span> <span class="ex">parallel</span> <span class="at">--progress</span> dataAugment</code></pre></div>
</section>
<section id="upscaling-conversion" class="level3">
<h3><a href="https://www.gwern.net/Faces#upscaling-conversion" title="Link to section: § &#39;Upscaling &amp; Conversion&#39;" class="no-popup cyxy-trs-source">Upscaling &amp; Conversion</a></h3>
<p class="cyxy-trs-source">Once any qual­ity fixes or data aug­men­ta­tion are done, it’d be a good idea to save a lot of disk space by con­vert­ing to <span class="smallcaps-auto">JPG</span> &amp; loss­ily re­duc­ing qual­ity (I find 33% saves a ton of space at no vis­i­ble change):</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">convertPNGToJPG()</span> <span class="kw">{</span> <span class="ex">convert</span> <span class="at">-quality</span> 33 <span class="st">"</span><span class="va">$@</span><span class="st">"</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span>.jpg <span class="kw">&amp;&amp;</span> <span class="fu">rm</span> <span class="st">"</span><span class="va">$@</span><span class="st">"</span><span class="kw">;</span> <span class="kw">}</span>
<span class="bu">export</span> <span class="at">-f</span> <span class="va">convertPNGToJPG</span>
<span class="fu">find</span> faces/ <span class="at">-type</span> f <span class="at">-name</span> <span class="st">"*.png"</span> <span class="kw">|</span> <span class="ex">parallel</span> <span class="at">--progress</span> convertPNGToJPG</code></pre></div>
<div class="admonition warning">
<p class="cyxy-trs-source">Re­mem­ber that Style<span class="smallcaps-auto">GAN</span> mod­els are only com­pat­i­ble with im­ages of the type they were trained on, so if you are us­ing a Style<span class="smallcaps-auto">GAN</span> pre­trained model which was trained on <span class="smallcaps-auto">PNG</span>s (like, <span class="smallcaps-auto">IIRC</span>, the <span class="smallcaps-auto">FFHQ</span> Style<span class="smallcaps-auto">GAN</span> mod­el­s), you will need to keep us­ing <span class="smallcaps-auto">PNG</span>s.</p>
</div>
<p class="cyxy-trs-source">Do­ing the fi­nal scal­ing to <em>ex­actly</em> 512px can be done at many points but I gen­er­ally post­pone it to the end in or­der to work with im­ages in their ‘na­tive’ res­o­lu­tions &amp; as­pec­t-ra­tios for as long as pos­si­ble. At this point we <em>care­fully</em> <a href="http://www.imagemagick.org/Usage/resize/" class="no-popup">tell Im­ageMag­ick to rescale</a> every­thing to 512×512<a href="https://www.gwern.net/Faces#sn26" class="footnote-ref spawns-popup" id="fnref26" role="doc-noteref"><sup>26</sup></a>⁠, not pre­serv­ing the as­pect ra­tio by fill­ing in with a black back­ground as nec­es­sary on ei­ther side:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">find</span> faces/ <span class="at">-type</span> f <span class="kw">|</span> <span class="fu">xargs</span> <span class="at">--max-procs</span><span class="op">=</span>16 <span class="at">-n</span> 9000 <span class="dt">\</span>
    mogrify <span class="at">-resize</span> 512x512<span class="dt">\&gt;</span> <span class="at">-extent</span> 512x512<span class="dt">\&gt;</span> <span class="at">-gravity</span> center <span class="at">-background</span> black</code></pre></div>
<p class="cyxy-trs-source">Any slight­ly-d­iffer­ent im­age could crash the im­port process. There­fore, we delete <em>any</em> im­age which is even slightly differ­ent from the 512×512 s<span class="smallcaps-auto">RGB</span> <span class="smallcaps-auto">JPG</span> they are sup­posed to be:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">find</span> faces/ <span class="at">-type</span> f <span class="kw">|</span> <span class="fu">xargs</span> <span class="at">--max-procs</span><span class="op">=</span>16 <span class="at">-n</span> 9000 identify <span class="kw">|</span> <span class="dt">\</span>
    <span class="co"># remember the warning: images must be identical, square, and sRGB/grayscale:</span>
    <span class="fu">fgrep</span> <span class="at">-v</span> <span class="st">" JPEG 512x512 512x512+0+0 8-bit sRGB"</span><span class="kw">|</span> <span class="fu">cut</span> <span class="at">-d</span> <span class="st">' '</span> <span class="at">-f</span> 1 <span class="kw">|</span> <span class="dt">\</span>
    <span class="fu">xargs</span> <span class="at">--max-procs</span><span class="op">=</span>16 <span class="at">-n</span> 10000 rm</code></pre></div>
<p class="cyxy-trs-source">Hav­ing done all this, we should have a large con­sis­tent high­-qual­ity dataset.</p>
<p class="cyxy-trs-source">Fi­nal­ly, the faces can now be con­verted to the Pro<span class="smallcaps-auto">GAN</span> or Style<span class="smallcaps-auto">GAN</span> dataset for­mat us­ing <code>dataset_tool.py</code>. It is worth re­mem­ber­ing at this point how frag­ile that is and the re­quire­ments Im­ageMag­ick’s <code>identify</code> com­mand is handy for look­ing at files in more de­tails, par­tic­u­larly their res­o­lu­tion &amp; col­or­space, which are often the prob­lem.</p>
<p class="cyxy-trs-source">Be­cause of the ex­treme fragility of <code>dataset_tool.py</code>, I strongly ad­vise that you edit it to print out the file­names of each file as they are be­ing processed so that when (not if) it crash­es, you can in­ves­ti­gate the cul­prit and check the rest. The edit could be as sim­ple as this:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode Diff"><code class="sourceCode diff"><span class="kw">diff --git a/dataset_tool.py b/dataset_tool.py</span>
index 4ddfe44..e64e40b 100755
<span class="dt">--- a/dataset_tool.py</span>
<span class="dt">+++ b/dataset_tool.py</span>
<span class="dt">@@ -519,6 +519,7 @@ def create_from_images(tfrecord_dir, image_dir, shuffle):</span>
     with TFRecordExporter(tfrecord_dir, len(image_filenames)) as tfr:
         order = tfr.choose_shuffled_order() if shuffle else np.arange(len(image_filenames))
         for idx in range(order.size):
<span class="va">+            print(image_filenames[order[idx]])</span>
             img = np.asarray(PIL.Image.open(image_filenames[order[idx]]))
             if channels == 1:
                 img = img[np.newaxis, :, :] # HW =&gt; CHW</code></pre></div>
<p class="cyxy-trs-source">There should be no is­sues if all the im­ages were thor­oughly checked ear­lier, but should any im­ages crash it, they can be checked in more de­tail by <code>identify</code>. (I ad­vise just delet­ing them and not try­ing to res­cue them.)</p>
<p class="cyxy-trs-source">Then the con­ver­sion is just (as­sum­ing Style<span class="smallcaps-auto">GAN</span> pre­req­ui­sites are in­stalled, see next sec­tion):</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="bu">source</span> activate MY_TENSORFLOW_ENVIRONMENT
<span class="ex">python</span> dataset_tool.py create_from_images datasets/faces /media/gwern/Data/danbooru2018/faces/</code></pre></div>
<p class="cyxy-trs-source">Con­grat­u­la­tions, the hard­est part is over. Most of the rest sim­ply re­quires pa­tience (and a will­ing­ness to edit Python files di­rectly in or­der to con­fig­ure Style<span class="smallcaps-auto">GAN</span>).</p>
</section>
</section>
</section>
<section id="training" class="level1">
<h1><a href="https://www.gwern.net/Faces#training" title="Link to section: § &#39;Training&#39;" class="no-popup cyxy-trs-source">Training</a></h1>
<section id="installation" class="level2">
<h2><a href="https://www.gwern.net/Faces#installation" title="Link to section: § &#39;Installation&#39;" class="no-popup cyxy-trs-source">Installation</a></h2>
<p class="cyxy-trs-source">I as­sume you have <a href="https://developer.nvidia.com/cuda-downloads" class="no-popup"><span class="smallcaps-auto">CUDA</span></a> in­stalled &amp; func­tion­ing. If not, good luck. (On my Ubuntu Bionic 18.04.2 <span class="smallcaps-auto">LTS</span> OS, I have suc­cess­fully used the Nvidia dri­ver ver­sion #410.104, <span class="smallcaps-auto">CUDA</span> 10.1, and Ten­sor­Flow 1.13.1.)</p>
<p class="cyxy-trs-source">A Python ≥3.6<a href="https://www.gwern.net/Faces#sn27" class="footnote-ref spawns-popup" id="fnref27" role="doc-noteref"><sup>27</sup></a> vir­tual en­vi­ron­ment can be set up for Style<span class="smallcaps-auto">GAN</span> to keep de­pen­den­cies tidy, <a href="https://www.tensorflow.org/install/" class="no-popup">Ten­sor­Flow</a> &amp; Style<span class="smallcaps-auto">GAN</span> de­pen­den­cies in­stalled:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="ex">conda</span> create <span class="at">-n</span> stylegan pip python=3.6
<span class="bu">source</span> activate stylegan

<span class="co">## TF:</span>
<span class="ex">pip</span> install tensorflow-gpu
<span class="co">## Test install:</span>
<span class="ex">python</span> <span class="at">-c</span> <span class="st">"import tensorflow as tf; tf.enable_eager_execution(); </span><span class="dt">\</span>
<span class="st">    print(tf.reduce_sum(tf.random_normal([1000, 1000])))"</span>
<span class="ex">pip</span> install tensorboard

<span class="co">## StyleGAN:</span>
<span class="co">## Install pre-requisites:</span>
<span class="ex">pip</span> install pillow numpy moviepy scipy opencv-python lmdb <span class="co"># requests?</span>
<span class="co">## Download:</span>
<span class="fu">git</span> clone <span class="st">'https://github.com/NVlabs/stylegan.git'</span> <span class="kw">&amp;&amp;</span> <span class="bu">cd</span> ./stylegan/
<span class="co">## Test install:</span>
<span class="ex">python</span> pretrained_example.py
<span class="co">## ./results/example.png should be a photograph of a middle-aged man</span></code></pre></div>
<p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> can also be trained on the in­ter­ac­tive <a href="https://colab.research.google.com/notebooks/welcome.ipynb" class="no-popup">Google Co­lab</a> ser­vice, which pro­vides free slices of K80 <span class="smallcaps-auto">GPU</span>s 12-<span class="smallcaps-auto">GPU</span>-hour chunks, us­ing <a href="https://github.com/ak9250/stylegan-art/blob/master/styleganportraits.ipynb" class="no-popup">this Co­lab note­book</a>⁠. Co­lab is much slower than train­ing on a lo­cal ma­chine &amp; the free in­stances are not enough to train the best Style<span class="smallcaps-auto">GAN</span>s, but this might be a use­ful op­tion for peo­ple who sim­ply want to try it a lit­tle or who are do­ing some­thing quick like ex­tremely low-res­o­lu­tion train­ing or trans­fer­-learn­ing where a few <span class="smallcaps-auto">GPU</span>-hours on a slow small <span class="smallcaps-auto">GPU</span> might be enough.</p>
</section>
<section id="configuration" class="level2">
<h2><a href="https://www.gwern.net/Faces#configuration" title="Link to section: § &#39;Configuration&#39;" class="no-popup cyxy-trs-source">Configuration</a></h2>
<p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> does­n’t ship with any sup­port for <span class="smallcaps-auto">CLI</span> op­tions; in­stead, one must edit <code>train.py</code> and <code>train/training_loop.py</code>:</p>
<ol type="1">
<li><p class="cyxy-trs-source"><code>train/training_loop.py</code></p>
<p class="cyxy-trs-source">The core con­fig­u­ra­tion is done in the func­tion de­faults to <code>training_loop</code> be­gin­ning <a href="https://github.com/NVlabs/stylegan/blob/master/training/training_loop.py#L112" class="no-popup">line 112</a>⁠.</p>
<p class="cyxy-trs-source">The key ar­gu­ments are <code>G_smoothing_kimg</code> &amp; <code>D_repeats</code> (affects the learn­ing dy­nam­ic­s), <code>network_snapshot_ticks</code> (how often to save the pickle snap­shot­s—­more fre­quent means less progress lost in crash­es, but as each one weighs 300M­B+, can quickly use up gi­ga­bytes of space), <code>resume_run_id</code> (set to <code>"latest"</code>), and <code>resume_kimg</code>.</p>
<div class="admonition warning cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">
<div class="admonition-title cyxy-trs-source">
Don’t Erase Your Model
</div>
<code>resume_kimg</code> gov­erns where in the over­all pro­gres­sive-grow­ing train­ing sched­ule Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> starts from. If it is set to 0, train­ing be­gins at the be­gin­ning of the pro­gres­sive-grow­ing sched­ule, at the low­est res­o­lu­tion, re­gard­less of how much train­ing has been pre­vi­ously done. It is vi­tally im­por­tant when do­ing trans­fer learn­ing that it is set to a suffi­ciently high num­ber (eg 10000) that train­ing <em class="cyxy-trs-source">be­gins</em> at the high­est de­sired res­o­lu­tion like 512px, as it ap­pears that lay­ers are erased when added dur­ing pro­gres­sive-grow­ing. (<code>resume_kimg</code> may also need to be set to a high value to make it skip straight to train­ing at the high­est res­o­lu­tion if you are train­ing on small datasets of small im­ages, where there’s risk of it over­fit­ting un­der the nor­mal train­ing sched­ule and never reach­ing the high­est res­o­lu­tion.) This trick is un­nec­es­sary in Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> 2, which is sim­pler in not us­ing pro­gres­sive grow­ing.
</div>
<p class="cyxy-trs-source">More ex­per­i­men­tal­ly, I sug­gest set­ting <code>minibatch_repeats = 1</code> in­stead of <code>minibatch_repeats = 5</code>; in line with the sus­pi­cious­ness of the gra­di­en­t-ac­cu­mu­la­tion im­ple­men­ta­tion in Pro<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span>, this ap­pears to make train­ing both sta­bler &amp; faster.</p>
<p class="cyxy-trs-source">Note that some of these vari­ables, like learn­ing rates, are over­rid­den in <code>train.py</code>. It’s bet­ter to set those there or else you may con­fuse your­self badly (like I did in won­der­ing why Pro<span class="smallcaps-auto">GAN</span> &amp; Style<span class="smallcaps-auto">GAN</span> seemed ex­tra­or­di­nar­ily ro­bust to large changes in the learn­ing rates…).</p></li>
<li><p class="cyxy-trs-source"><code>train.py</code> (pre­vi­ously <code>config.py</code> in Pro<span class="smallcaps-auto">GAN</span>; re­named <code>run_training.py</code> in Style<span class="smallcaps-auto">GAN</span> 2)</p>
<p class="cyxy-trs-source">Here we set the num­ber of <span class="smallcaps-auto">GPU</span>s, im­age res­o­lu­tion, dataset, learn­ing rates, hor­i­zon­tal flip­ping/<wbr>mir­ror­ing data aug­men­ta­tion, and mini­batch sizes. (This file in­cludes set­tings in­tended Pro<span class="smallcaps-auto">GAN</span>—watch out that you don’t ac­ci­den­tally turn on Pro<span class="smallcaps-auto">GAN</span> in­stead of Style<span class="smallcaps-auto">GAN</span> &amp; con­fuse your­self.) Learn­ing rate &amp; mini­batch should gen­er­ally be left alone (ex­cept to­wards the end of train­ing when one wants to lower the learn­ing rate to pro­mote con­ver­gence or re­bal­ance the G/<wbr>D), but the im­age res­o­lu­tion/<wbr>­dataset/<wbr>mir­ror­ing do need to be set, <a href="https://github.com/NVlabs/stylegan/blob/master/train.py#L37" class="no-popup">like thus</a>:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode Python"><code class="sourceCode python">desc <span class="op">+=</span> <span class="st">'-faces'</span><span class="op">;</span>     dataset <span class="op">=</span> EasyDict(tfrecord_dir<span class="op">=</span><span class="st">'faces'</span>, resolution<span class="op">=</span><span class="dv">512</span>)<span class="op">;</span>              train.mirror_augment <span class="op">=</span> <span class="va">True</span></code></pre></div>
<p class="cyxy-trs-source">This sets up the 512px face dataset which was pre­vi­ously cre­ated in <code>dataset/faces</code>, turns on mir­ror­ing (be­cause while there may be writ­ing in the back­ground, we don’t care about it for face gen­er­a­tion), and sets a ti­tle for the check­points/<wbr>logs, which will now ap­pear in <code>results/</code> with the ‘-faces’ string.</p>
<p class="cyxy-trs-source">As­sum­ing you do not have 8 <span class="smallcaps-auto">GPU</span>s (as you prob­a­bly do not), you <em>must</em> change the <code>-preset</code> to match your num­ber of <span class="smallcaps-auto">GPU</span>s, Style<span class="smallcaps-auto">GAN</span> will not au­to­mat­i­cally choose the cor­rect num­ber of <span class="smallcaps-auto">GPU</span>s. If you fail to set it cor­rectly to the ap­pro­pri­ate pre­set, Style<span class="smallcaps-auto">GAN</span> will at­tempt to use <span class="smallcaps-auto">GPU</span>s which do not ex­ist and will crash with the opaque er­ror mes­sage (note that <span class="smallcaps-auto">CUDA</span> uses ze­ro-in­dex­ing so <code>GPU:0</code> refers to the first <span class="smallcaps-auto">GPU</span>, <code>GPU:1</code> refers to my sec­ond <span class="smallcaps-auto">GPU</span>, and thus <code>/device:GPU:2</code> refers to my—nonex­is­ten­t—third <span class="smallcaps-auto">GPU</span>):</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation \
    G_synthesis_3/lod: {{node G_synthesis_3/lod}}was explicitly assigned to /device:GPU:2 but available \
    devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, \
    /job:localhost/replica:0/task:0/device:GPU:1, /job:localhost/replica:0/task:0/device:XLA_CPU:0, \
    /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1 ]. \
    Make sure the device specification refers to a valid device.
     [[{{node G_synthesis_3/lod}}]]</code></pre>
<p class="cyxy-trs-source">For my 2×1080ti <a href="https://github.com/NVlabs/stylegan/blob/master/train.py#L47" class="no-popup">I’d set</a>:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode Python"><code class="sourceCode python">desc <span class="op">+=</span> <span class="st">'-preset-v2-2gpus'</span><span class="op">;</span> submit_config.num_gpus <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> sched.minibatch_base <span class="op">=</span> <span class="dv">8</span><span class="op">;</span> sched.minibatch_dict <span class="op">=</span> <span class="op">\</span>
    {<span class="dv">4</span>: <span class="dv">256</span>, <span class="dv">8</span>: <span class="dv">256</span>, <span class="dv">16</span>: <span class="dv">128</span>, <span class="dv">32</span>: <span class="dv">64</span>, <span class="dv">64</span>: <span class="dv">32</span>, <span class="dv">128</span>: <span class="dv">16</span>, <span class="dv">256</span>: <span class="dv">8</span>}<span class="op">;</span> sched.G_lrate_dict <span class="op">=</span> {<span class="dv">512</span>: <span class="fl">0.0015</span>, <span class="dv">1024</span>: <span class="fl">0.002</span>}<span class="op">;</span> <span class="op">\</span>
    sched.D_lrate_dict <span class="op">=</span> EasyDict(sched.G_lrate_dict)<span class="op">;</span> train.total_kimg <span class="op">=</span> <span class="dv">99000</span></code></pre></div>
<p class="cyxy-trs-source">So my re­sults get saved to <code>results/00001-sgan-faces-2gpu</code> etc (the run ID in­cre­ments, ‘sgan’ be­cause Style<span class="smallcaps-auto">GAN</span> rather than Pro<span class="smallcaps-auto">GAN</span>, ‘-faces’ as the dataset be­ing trained on, and ‘2gpu’ be­cause it’s multi-<span class="smallcaps-auto">GPU</span>).</p></li>
</ol>
</section>
<section id="running" class="level2">
<h2><a href="https://www.gwern.net/Faces#running" title="Link to section: § &#39;Running&#39;" class="no-popup cyxy-trs-source">Running</a></h2>
<p class="cyxy-trs-source">I typ­i­cally run Style<span class="smallcaps-auto">GAN</span> in a <a href="https://en.wikipedia.org/wiki/GNU_Screen" class="docMetadata has-annotation spawns-popup"><code>screen</code></a> ses­sion which can be de­tached and keeps mul­ti­ple shells or­ga­nized: 1 ter­mi­nal/<wbr>shell for the Style<span class="smallcaps-auto">GAN</span> run, 1 ter­mi­nal/<wbr>shell for Ten­sor­Board, and 1 for Emacs.</p>
<p class="cyxy-trs-source">With Emacs, I keep the two key Python files open (<code>train.py</code> and <code>train/training_loop.py</code>) for ref­er­ence &amp; easy edit­ing.</p>
<p class="cyxy-trs-source">With the “lat­est” patch, Style<span class="smallcaps-auto">GAN</span> can be thrown into a while-loop to keep run­ning after crash­es, like:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="cf">while</span> <span class="fu">true</span><span class="kw">;</span> <span class="cf">do</span> <span class="fu">nice</span> py train.py <span class="kw">;</span> <span class="fu">date</span><span class="kw">;</span> <span class="kw">(</span><span class="ex">xmessage</span> <span class="st">"alert: StyleGAN crashed"</span> <span class="kw">&amp;);</span> <span class="fu">sleep</span> 10s<span class="kw">;</span> <span class="cf">done</span></code></pre></div>
<p class="cyxy-trs-source"><a href="https://www.tensorflow.org/guide/summaries_and_tensorboard" title="TensorBoard: Visualizing Learning" class="no-popup">Ten­sor­Board</a> is a log­ging util­ity which dis­plays lit­tle time-series of recorded vari­ables which one views in a web browser, eg:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="ex">tensorboard</span> <span class="at">--logdir</span> results/02022-sgan-faces-2gpu/
<span class="co"># TensorBoard 1.13.0 at http://127.0.0.1:6006 (Press CTRL+C to quit)</span></code></pre></div>
<p class="cyxy-trs-source">Note that Ten­sor­Board can be back­ground­ed, but needs to be up­dated every time a new run is started as the re­sults will then be in a differ­ent fold­er.</p>
<p class="cyxy-trs-source">Train­ing Style<span class="smallcaps-auto">GAN</span> is much eas­ier &amp; more re­li­able than other <span class="smallcaps-auto">GAN</span>s, but it is still more of an art than a sci­ence. (We put up with it be­cause while <span class="smallcaps-auto">GAN</span>s suck, every­thing else sucks more.) Notes on train­ing:</p>
<ul>
<li><p class="cyxy-trs-source"><strong>Crash­proofing</strong>:</p>
<p class="cyxy-trs-source">The ini­tial re­lease of Style<span class="smallcaps-auto">GAN</span> was prone to crash­ing when I ran it, seg­fault­ing at ran­dom. Up­dat­ing Ten­sor­Flow ap­peared to re­duce this but the root cause is still un­known. Seg­fault­ing or crash­ing is also re­port­edly com­mon if run­ning on mixed <span class="smallcaps-auto">GPU</span>s (eg a 1080ti + Ti­tan V).</p>
<p class="cyxy-trs-source">Un­for­tu­nate­ly, Style<span class="smallcaps-auto">GAN</span> has no set­ting for sim­ply re­sum­ing from the lat­est snap­shot after crash­ing/<wbr>ex­it­ing (which is what one usu­ally wants), and one must man­u­ally edit the <a href="https://github.com/NVlabs/stylegan/blob/master/training/training_loop.py#L136" class="no-popup"><code>resume_run_id</code> line in <code>training_loop.py</code></a> to set it to the lat­est run ID. This is te­dious and er­ror-prone—at one point I re­al­ized I had wasted 6 <span class="smallcaps-auto">GPU</span>-days of train­ing by restart­ing from a 3-day-old snap­shot be­cause I had not up­dated the <code>resume_run_id</code> after a seg­fault!</p>
<p class="cyxy-trs-source">If you are do­ing any runs longer than a few wall­clock hours, I strongly ad­vise use of <a href="https://www.gwern.net/docs/www/paste.laravel.io/3f45f7cddccb572bfa5cdc49616cf766b4764977.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://paste.laravel.io/f2419e15-ea7d-408a-8ff2-b8ee6d00ddd1/raw" data-attribute-title="(Original URL: https://paste.laravel.io/f2419e15-ea7d-408a-8ff2-b8ee6d00ddd1/raw )">nshep­perd’s patch</a> to au­to­mat­i­cally restart from the lat­est snap­shot by set­ting <code>resume_run_id = "latest"</code>:</p>
<div class="sourceCode collapse" id="cb20"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Diff"><code class="sourceCode diff"><span class="kw">diff --git a/training/misc.py b/training/misc.py</span>
index 50ae51c..d906a2d 100755
<span class="dt">--- a/training/misc.py</span>
<span class="dt">+++ b/training/misc.py</span>
<span class="dt">@@ -119,6 +119,14 @@ def list_network_pkls(run_id_or_run_dir, include_final=True):</span>
         del pkls[0]
     return pkls

<span class="va">+def locate_latest_pkl():</span>
<span class="va">+    allpickles = sorted(glob.glob(os.path.join(config.result_dir, '0*', 'network-*.pkl')))</span>
<span class="va">+    latest_pickle = allpickles[-1]</span>
<span class="va">+    resume_run_id = os.path.basename(os.path.dirname(latest_pickle))</span>
<span class="va">+    RE_KIMG = re.compile('network-snapshot-(\d+).pkl')</span>
<span class="va">+    kimg = int(RE_KIMG.match(os.path.basename(latest_pickle)).group(1))</span>
<span class="va">+    return (locate_network_pkl(resume_run_id), float(kimg))</span>
<span class="va">+</span>
 def locate_network_pkl(run_id_or_run_dir_or_network_pkl, snapshot_or_network_pkl=None):
     for candidate in [snapshot_or_network_pkl, run_id_or_run_dir_or_network_pkl]:
         if isinstance(candidate, str):
<span class="kw">diff --git a/training/training_loop.py b/training/training_loop.py</span>
index 78d6fe1..20966d9 100755
<span class="dt">--- a/training/training_loop.py</span>
<span class="dt">+++ b/training/training_loop.py</span>
<span class="dt">@@ -148,7 +148,10 @@ def training_loop(</span>
     # Construct networks.
     with tf.device('/gpu:0'):
         if resume_run_id is not None:
<span class="st">-            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)</span>
<span class="va">+            if resume_run_id == 'latest':</span>
<span class="va">+                network_pkl, resume_kimg = misc.locate_latest_pkl()</span>
<span class="va">+            else:</span>
<span class="va">+                network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)</span>
             print('Loading networks from "%s"...' % network_pkl)
             G, D, Gs = misc.load_pkl(network_pkl)
         else:</code></pre></div>
<p class="cyxy-trs-source"><!-- ) --></p>
<p class="cyxy-trs-source">(The diff can be edited by hand, or copied into the repo as a file like <code>latest.patch</code> &amp; then ap­plied with <code>git apply latest.patch</code>.)</p></li>
<li><p class="cyxy-trs-source"><strong>Tun­ing Learn­ing Rates</strong></p>
<p class="cyxy-trs-source">The LR is one of the most crit­i­cal hy­per­pa­ra­me­ters: too-large up­dates based on too-s­mall mini­batches are dev­as­tat­ing to <span class="smallcaps-auto">GAN</span> sta­bil­ity &amp; fi­nal qual­i­ty. The LR also seems to in­ter­act with the in­trin­sic diffi­culty or di­ver­sity of an im­age do­main; Kar­ras&nbsp;et&nbsp;al&nbsp;2019 use 0.003 G/<wbr>D LRs on their <span class="smallcaps-auto">FFHQ</span> dataset (which has been care­fully cu­rated and the faces aligned to put land­marks like eye­s/<wbr>­mouth in the same lo­ca­tions in every im­age) when train­ing on 8-<span class="smallcaps-auto">GPU</span> ma­chines with mini­batches of <em>n</em> = 32, but I find lower to be bet­ter on my anime face/<wbr>­por­trait datasets where I can only do <em>n</em> = 8. From look­ing at train­ing videos of whole-Dan­booru2018 Style<span class="smallcaps-auto">GAN</span> runs, I sus­pect that the nec­es­sary LRs would be lower still. Learn­ing rates are closely re­lated to mini­batch size (a com­mon rule of thumb in su­per­vised learn­ing of <span class="smallcaps-auto">CNN</span>s is that the re­la­tion­ship of biggest us­able LR fol­lows a square-root curve in mini­batch size) and the Big<span class="smallcaps-auto">GAN</span> re­search ar­gues that mini­batch size it­self strongly in­flu­ences how bad mode drop­ping is, which sug­gests that smaller LRs may be more nec­es­sary the more di­verse/<wbr>d­iffi­cult a dataset is.</p></li>
<li><p class="cyxy-trs-source"><strong>Bal­anc­ing G/<wbr>D</strong>:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Screen­shot of Ten­sor­Board  losses for an anime face  mak­ing progress to­wards con­ver­gence" class="invertible focusable" height="234" loading="lazy" sizes="(max-width: 768px) 100vw, 979px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-19-stylegan-faces-trainingloss-convergence.webp" srcset="/images/gan/stylegan/2019-02-19-stylegan-faces-trainingloss-convergence.png-768px.png 768w, /images/gan/stylegan/2019-02-19-stylegan-faces-trainingloss-convergence.png 979w" width="979"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Screen­shot of Ten­sor­Board G/<wbr>D losses for an anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> mak­ing progress to­wards con­ver­gence</figcaption></span></span></figure>
<p class="cyxy-trs-source">Later in train­ing, if the G is not mak­ing good progress to­wards the ul­ti­mate goal of a 0.5 loss (and the D’s loss grad­u­ally de­creas­ing to­wards 0.5), and has a loss stub­bornly stuck around −1 or some­thing, it may be nec­es­sary to change the bal­ance of G/<wbr>D. This can be done sev­eral ways but the eas­i­est is to ad­just <a href="https://github.com/NVlabs/stylegan/blob/master/train.py#L54" class="no-popup">the LRs in <code>train.py</code></a>⁠, <code>sched.G_lrate_dict</code> &amp; <code>sched.D_lrate_dict</code>.</p>
<p class="cyxy-trs-source">One needs to keep an eye on the G/<wbr>D losses and also the per­cep­tual qual­ity of the faces (s­ince we don’t have any good <span class="smallcaps-auto">FID</span> equiv­a­lent yet for anime faces, which re­quires a good open-source Dan­booru tag­ger to cre­ate em­bed­dings), and re­duce both LRs (or usu­ally just the D’s LR) based on the face qual­ity and whether the G/<wbr>D losses are ex­plod­ing or oth­er­wise look im­bal­anced. What you want, I think, is for the G/<wbr>D losses to be sta­ble at a cer­tain ab­solute amount for a long time while the qual­ity vis­i­bly im­proves, re­duc­ing D’s LR as nec­es­sary to keep it bal­anced with G; and then once you’ve run out of time/<wbr>­pa­tience or ar­ti­facts are show­ing up, then you can de­crease both LRs to con­verge onto a lo­cal op­ti­ma.</p>
<p class="cyxy-trs-source">I find the de­fault of 0.003 can be too high once qual­ity reaches a high level with both faces &amp; por­traits, and it helps to re­duce it by a third to 0.001 or a tenth to 0.0003. If there still is­n’t con­ver­gence, the D may be too strong and it can be turned down sep­a­rate­ly, to a tenth or a fifti­eth even. (Given the sto­chas­tic­ity of train­ing &amp; the rel­a­tiv­ity of the loss­es, one should wait sev­eral wall­clock hours or days after each mod­i­fi­ca­tion to see if it made a differ­ence.)</p></li>
<li><p class="cyxy-trs-source"><strong>Skip­ping <span class="smallcaps-auto">FID</span> met­rics</strong>:</p>
<p class="cyxy-trs-source">Some met­rics are com­puted for log­ging/<wbr>re­port­ing. The <span class="smallcaps-auto">FID</span> met­rics are cal­cu­lated us­ing an old Im­a­geNet <span class="smallcaps-auto">CNN</span>; what is re­al­is­tic on Im­a­geNet may have lit­tle to do with your par­tic­u­lar do­main and while a large <span class="smallcaps-auto">FID</span> like 100 is con­cern­ing, <span class="smallcaps-auto">FID</span>s like 20 or even in­creas­ing are not nec­es­sar­ily a prob­lem or use­ful guid­ance com­pared to just look­ing at the gen­er­ated sam­ples or the loss curves. Given that com­put­ing <span class="smallcaps-auto">FID</span> met­rics is not free &amp; po­ten­tially ir­rel­e­vant or mis­lead­ing on many im­age do­mains, I sug­gest dis­abling them en­tire­ly. (They are not used in the train­ing for any­thing, and dis­abling them is safe.)</p>
<p class="cyxy-trs-source">They can be edited out of the main train­ing loop by com­ment­ing out the call to <code>metrics.run</code> like so:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode Python"><code class="sourceCode python"><span class="op">@@</span> <span class="op">-</span><span class="dv">261</span>,<span class="dv">7</span> <span class="op">+</span><span class="dv">265</span>,<span class="dv">7</span> <span class="op">@@</span> <span class="kw">def</span> training_loop()
        <span class="cf">if</span> cur_tick <span class="op">%</span> network_snapshot_ticks <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> done <span class="kw">or</span> cur_tick <span class="op">==</span> <span class="dv">1</span>:
            pkl <span class="op">=</span> os.path.join(submit_config.run_dir, <span class="st">'network-snapshot-</span><span class="sc">%06d</span><span class="st">.pkl'</span> <span class="op">%</span> (cur_nimg <span class="op">//</span> <span class="dv">1000</span>))
            misc.save_pkl((G, D, Gs), pkl)
            <span class="co"># metrics.run(pkl, run_dir=submit_config.run_dir, num_gpus=submit_config.num_gpus, tf_config=tf_config)</span></code></pre></div></li>
<li><p class="cyxy-trs-source"><strong>‘Blob’ &amp; ‘Crack’ Ar­ti­facts</strong>:</p>
<p class="cyxy-trs-source">Dur­ing train­ing, ‘blobs’ often show up or move around. These blobs ap­pear even late in train­ing on oth­er­wise high­-qual­ity im­ages and are unique to Style<span class="smallcaps-auto">GAN</span> (at least, I’ve never seen an­other <span class="smallcaps-auto">GAN</span> whose train­ing ar­ti­facts look like the blob­s). That they are so large &amp; glar­ing sug­gests a weak­ness in Style<span class="smallcaps-auto">GAN</span> some­where. The source of the blobs was un­clear. If you watch train­ing videos, these blobs seem to grad­u­ally morph into new fea­tures such as eyes or hair or glass­es. I sus­pect they are part of how Style<span class="smallcaps-auto">GAN</span> ‘cre­ates’ new fea­tures, start­ing with a fea­ture-less blob su­per­im­posed at ap­prox­i­mately the right lo­ca­tion, and grad­u­ally re­fined into some­thing use­ful. The <a href="https://arxiv.org/abs/1912.04958" id="karras-et-al-2019-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Analyzing and Improving the Image Quality of StyleGAN&#39;, Karras et al 2019">Style<span class="smallcaps-auto">GAN</span> 2 pa­per</a> in­ves­ti­gated the blob ar­ti­facts &amp; found it to be due to the Gen­er­a­tor work­ing around a flaw in Style<span class="smallcaps-auto">GAN</span>’s use of AdaIN nor­mal­iza­tion. Kar­ras&nbsp;et&nbsp;al&nbsp;2019 note that im­ages <em>with­out</em> a blob some­where are se­verely cor­rupt­ed; be­cause the blobs are in fact do­ing some­thing use­ful, it is un­sur­pris­ing that the Dis­crim­i­na­tor does­n’t fix the Gen­er­a­tor. Style<span class="smallcaps-auto">GAN</span> 2 changes the AdaIN nor­mal­iza­tion to elim­i­nate this prob­lem, im­prov­ing over­all qual­i­ty.<a href="https://www.gwern.net/Faces#sn28" class="footnote-ref spawns-popup" id="fnref28" role="doc-noteref"><sup>28</sup></a></p>
<p class="cyxy-trs-source">If blobs are ap­pear­ing too often or one wants a fi­nal model with­out any new in­tru­sive blobs, it may help to lower the LR to try to con­verge to a lo­cal op­tima where the nec­es­sary blob is hid­den away some­where un­ob­tru­sive.</p>
<p class="cyxy-trs-source">In train­ing anime faces, I have seen ad­di­tional ar­ti­facts, which look like ‘cracks’ or ‘waves’ or ele­phant skin wrin­kles or the sort of fine craz­ing seen in old paint­ings or ce­ram­ics, which ap­pear to­ward the end of train­ing on pri­mar­ily skin or ar­eas of flat col­or; they hap­pen par­tic­u­larly fast when trans­fer learn­ing on a small dataset. The only so­lu­tion I have found so far is to ei­ther stop train­ing or get more da­ta. In con­trast to the blob ar­ti­facts (i­den­ti­fied as an ar­chi­tec­tural prob­lem &amp; fixed in Style<span class="smallcaps-auto">GAN</span> 2), I cur­rently sus­pect the cracks are a sign of over­fit­ting rather than a pe­cu­liar­ity of nor­mal Style<span class="smallcaps-auto">GAN</span> train­ing, where the G has started try­ing to mem­o­rize noise in the fine de­tail of pix­e­la­tion/<wbr>­li­nes, and so these are a kind of over­fit­ting/<wbr>­mode col­lapse. (More spec­u­la­tive­ly: an­other pos­si­ble ex­pla­na­tion is that the cracks are caused by the Style<span class="smallcaps-auto">GAN</span> D be­ing sin­gle-s­cale rather than mul­ti­-s­cale—as in <span class="smallcaps-auto">MSG-GAN</span> and a num­ber of oth­er­s—and the ‘cracks’ are ac­tu­ally high­-fre­quency noise cre­ated by the G in spe­cific patches as ad­ver­sar­ial ex­am­ples to fool the D. They re­port­edly do not ap­pear in <span class="smallcaps-auto">MSG-GAN</span> or <a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-down has-content spawns-popup">Style<span class="smallcaps-auto">GAN</span> 2</a>⁠, which both use mul­ti­-s­cale Ds.)</p></li>
<li><p class="cyxy-trs-source"><strong>Gra­di­ent Ac­cu­mu­la­tion</strong>:</p>
<p class="cyxy-trs-source">Pro<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span>’s code­base claims to sup­port gra­di­ent ac­cu­mu­la­tion, which is a way to fake <em>large mini­batch train­ing</em> (eg <em>n</em> = 2048) by not do­ing the back­prop­a­ga­tion up­date every mini­batch, but in­stead sum­ming the gra­di­ents over many mini­batches and ap­ply­ing them all at once. This is a use­ful trick for sta­bi­liz­ing train­ing, and large mini­batch NN train­ing can differ qual­i­ta­tively from small mini­batch NN training—Big<span class="smallcaps-auto">GAN</span> per­for­mance in­creased with in­creas­ingly large mini­batches (<em>n</em> = 2048) and the au­thors spec­u­late that this is be­cause such large mini­batches mean that the full di­ver­sity of the dataset is rep­re­sented in each ‘mini­batch’ so the Big<span class="smallcaps-auto">GAN</span> mod­els can­not sim­ply ‘for­get’ rarer dat­a­points which would oth­er­wise not ap­pear for many mini­batches in a row, re­sult­ing in the <span class="smallcaps-auto">GAN</span> pathol­ogy of ‘mode drop­ping’ where some kinds of data just get ig­nored by both G/<wbr>D.</p>
<p class="cyxy-trs-source">How­ev­er, the Pro<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span> im­ple­men­ta­tion of gra­di­ent ac­cu­mu­la­tion does not re­sem­ble that of any other im­ple­men­ta­tion I’ve seen in Ten­sor­Flow or Py­Torch, and in my own ex­per­i­ments with up to <em>n</em> = 4096, I did­n’t ob­serve any sta­bi­liza­tion or qual­i­ta­tive differ­ences, so I am sus­pi­cious the im­ple­men­ta­tion is wrong.</p></li>
</ul>
<p class="cyxy-trs-source">Here is what a suc­cess­ful train­ing pro­gres­sion looks like for the anime face Style<span class="smallcaps-auto">GAN</span>:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-16-stylegan-facestraining.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-03-16-stylegan-facestraining.mp4" class="has-content spawns-popup cyxy-trs-source">Train­ing mon­tage video</a> of the first 9k it­er­a­tions of the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>.
</figcaption></span></span></figure>
<div class="admonition note">
<p class="cyxy-trs-source">The anime face model is ob­so­leted by the <a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-down has-content spawns-popup">Style<span class="smallcaps-auto">GAN</span> 2 por­trait model</a>⁠.</p>
</div>
<p class="cyxy-trs-source">The anime face model as of 2019-03-08, trained for 21,980 it­er­a­tions or ~21m im­ages or ~38 <span class="smallcaps-auto">GPU</span>-days, is <a href="https://mega.nz/#!vawjXISI!F7s13yRicxDA3QYqYDL2kjnc2K7Zk3DwCIYETREmBP4" title="2019-03-08-stylegan-animefaces-network-02051-021980.pkl.xz" class="no-popup">avail­able for down­load</a>⁠. (It is still not ful­ly-con­verged, but the qual­ity is good.)</p>
</section>
</section>
<section id="sampling" class="level1">
<h1><a href="https://www.gwern.net/Faces#sampling" title="Link to section: § &#39;Sampling&#39;" class="no-popup cyxy-trs-source">Sampling</a></h1>
<p class="cyxy-trs-source">Hav­ing suc­cess­fully trained a Style<span class="smallcaps-auto">GAN</span>, now the fun part—­gen­er­at­ing sam­ples!</p>
<section id="psitruncation-trick" class="level2">
<h2><a href="https://www.gwern.net/Faces#psitruncation-trick" title="Link to section: § &#39;Psi/truncation trick&#39;" class="no-popup cyxy-trs-source">Psi/“truncation trick”</a></h2>
<p class="cyxy-trs-source">The Ψ/<wbr>“trun­ca­tion trick” (<a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=4" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=4" data-attribute-title="(Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=4 )">Big<span class="smallcaps-auto">GAN</span> dis­cus­sion</a>⁠, <a href="https://www.gwern.net/docs/www/arxiv.org/4cb3118987e4ea896320737fe1a5bf959c722d04.pdf#page=3" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1812.04948.pdf#page=3" data-attribute-title="(Original URL: https://arxiv.org/pdf/1812.04948.pdf#page=3 )">Style<span class="smallcaps-auto">GAN</span> dis­cus­sion</a>⁠; ap­par­ently first in­tro­duced by <a href="https://arxiv.org/abs/1706.00082" id="marchesi-2017-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Megapixel Size Image Creation using Generative Adversarial Networks">March­esi 2017</a>) is the most im­por­tant hy­per­pa­ra­me­ter for all Style<span class="smallcaps-auto">GAN</span> gen­er­a­tion.</p>
<p class="cyxy-trs-source">The trun­ca­tion trick is used at sam­ple gen­er­a­tion time but <em>not</em> train­ing time. The idea is to edit the la­tent vec­tor <em>z</em>, which is a vec­tor of 𝒩(0,1), to re­move any vari­ables which are above a cer­tain size like 0.5 or 1.0, and re­sam­ple those.<a href="https://www.gwern.net/Faces#sn29" class="footnote-ref spawns-popup" id="fnref29" role="doc-noteref"><sup>29</sup></a> This seems to help by avoid­ing ‘ex­treme’ la­tent val­ues or com­bi­na­tions of la­tent val­ues which the G is not as good at—a G will not have gen­er­ated many data points with each la­tent vari­able at, say, +1.5SD. The trade­off is that those are still le­git­i­mate ar­eas of the over­all la­tent space which were be­ing used dur­ing train­ing to cover parts of the data dis­tri­b­u­tion; so while the la­tent vari­ables close to the mean of 0 may be the most ac­cu­rately mod­eled, they are also only a small part of the space of all pos­si­ble im­ages. So one can gen­er­ate la­tent vari­ables from the full un­re­stricted 𝒩(0,1) dis­tri­b­u­tion for each one, or one can trun­cate them at some­thing like +1SD or +0.7SD. (Like the dis­cus­sion of the best dis­tri­b­u­tion for the orig­i­nal la­tent dis­tri­b­u­tion, there’s no good rea­son to think that this is an op­ti­mal method of do­ing trun­ca­tion; there are many al­ter­na­tives, such as ones pe­nal­iz­ing the sum of the vari­ables, ei­ther re­ject­ing them or scal­ing them down, and <a href="https://arxiv.org/abs/1904.06991" id="kynkäänniemi-et-al-2019-4" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Improved Precision and Recall Metric for Assessing Generative Models&#39;, Kynkäänniemi et al 2019">some ap­pear to work much bet­ter</a> than the cur­rent trun­ca­tion trick.)</p>
<p class="cyxy-trs-source">At Ψ = 0, di­ver­sity is nil and all faces are a sin­gle global av­er­age face (a brown-eyed brown-haired school­girl, un­sur­pris­ing­ly); at ±0.5 you have a broad range of faces, and by ±1.2, you’ll see tremen­dous di­ver­sity in faces/<wbr>styles/<wbr>­con­sis­tency but also tremen­dous ar­ti­fact­ing &amp; dis­tor­tion. Where you set your Ψ will heav­ily in­flu­ence how ‘orig­i­nal’ out­puts look. At Ψ = 1.2, they are tremen­dously orig­i­nal but ex­tremely hit or miss. At Ψ = 0.5 they are con­sis­tent but bor­ing. For most of my sam­pling, I set Ψ = 0.7 which strikes the best bal­ance be­tween crazi­ness/<wbr>ar­ti­fact­ing and qual­i­ty/<wbr>­di­ver­si­ty. (Per­son­al­ly, I pre­fer to look at Ψ = 1.2 sam­ples be­cause they are so much more in­ter­est­ing, but if I re­leased those sam­ples, it would give a mis­lead­ing im­pres­sion to read­er­s.)</p>
</section>
<section id="random-samples" class="level2">
<h2><a href="https://www.gwern.net/Faces#random-samples" title="Link to section: § &#39;Random Samples&#39;" class="no-popup cyxy-trs-source">Random Samples</a></h2>
<p class="cyxy-trs-source">The Style<span class="smallcaps-auto">GAN</span> repo has a sim­ple script <a href="https://github.com/NVlabs/stylegan/blob/master/pretrained_example.py" class="no-popup"><code>pretrained_example.py</code></a> to down­load &amp; gen­er­ate a sin­gle face; in the in­ter­ests of re­pro­ducibil­i­ty, it hard­wires the model and the <span class="smallcaps-auto">RNG</span> seed so it will only gen­er­ate 1 par­tic­u­lar face. How­ev­er, it can be eas­ily adapted to use a lo­cal model and (s­lowly<a href="https://www.gwern.net/Faces#sn30" class="footnote-ref spawns-popup" id="fnref30" role="doc-noteref"><sup>30</sup></a>) gen­er­ate, say, 1000 sam­ple im­ages with the hy­per­pa­ra­me­ter Ψ = 0.6 (which gives high­-qual­ity but not high­ly-di­verse im­ages) which are saved to <code>results/example-{0-999}.png</code>:</p>
<div class="sourceCode collapse" id="cb22"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Python"><code class="sourceCode python"><span class="im">import</span> os
<span class="im">import</span> pickle
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> PIL.Image
<span class="im">import</span> dnnlib
<span class="im">import</span> dnnlib.tflib <span class="im">as</span> tflib
<span class="im">import</span> config

<span class="kw">def</span> main():
    tflib.init_tf()
    _G, _D, Gs <span class="op">=</span> pickle.load(<span class="bu">open</span>(<span class="st">"results/02051-sgan-faces-2gpu/network-snapshot-021980.pkl"</span>, <span class="st">"rb"</span>))
    Gs.print_layers()

    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">1000</span>):
        rnd <span class="op">=</span> np.random.RandomState(<span class="va">None</span>)
        latents <span class="op">=</span> rnd.randn(<span class="dv">1</span>, Gs.input_shape[<span class="dv">1</span>])
        fmt <span class="op">=</span> <span class="bu">dict</span>(func<span class="op">=</span>tflib.convert_images_to_uint8, nchw_to_nhwc<span class="op">=</span><span class="va">True</span>)
        images <span class="op">=</span> Gs.run(latents, <span class="va">None</span>, truncation_psi<span class="op">=</span><span class="fl">0.6</span>, randomize_noise<span class="op">=</span><span class="va">True</span>, output_transform<span class="op">=</span>fmt)
        os.makedirs(config.result_dir, exist_ok<span class="op">=</span><span class="va">True</span>)
        png_filename <span class="op">=</span> os.path.join(config.result_dir, <span class="st">'example-'</span><span class="op">+</span><span class="bu">str</span>(i)<span class="op">+</span><span class="st">'.png'</span>)
        PIL.Image.fromarray(images[<span class="dv">0</span>], <span class="st">'RGB'</span>).save(png_filename)

<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:
    main()</code></pre></div>
</section>
<section id="karras-et-al-2018-figures" class="level2">
<h2><a href="https://www.gwern.net/Faces#karras-et-al-2018-figures" title="Link to section: § &#39;Karras et al 2018 Figures&#39;" class="no-popup cyxy-trs-source">Karras&nbsp;et&nbsp;al&nbsp;2018 Figures</a></h2>
<p class="cyxy-trs-source">The fig­ures in Kar­ras&nbsp;et&nbsp;al&nbsp;2018, demon­strat­ing ran­dom sam­ples and as­pects of the style noise us­ing the 1024px <span class="smallcaps-auto">FFHQ</span> face model (as well as the oth­er­s), were gen­er­ated by <a href="https://github.com/NVlabs/stylegan/blob/master/generate_figures.py" class="no-popup"><code>generate_figures.py</code></a>⁠. This script needs ex­ten­sive mod­i­fi­ca­tions to work with my 512px anime face; go­ing through the file:</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">the code uses Ψ = 1 trun­ca­tion, but faces look bet­ter with Ψ = 0.7 (sev­eral of the func­tions have <code>truncation_psi=</code> set­tings but, trick­i­ly, the Fig­ure 3 <code>draw_style_mixing_figure</code> has its Ψ set­ting hid­den away in the <code>synthesis_kwargs</code> global vari­able)</li>
<li class="cyxy-trs-source">the loaded model needs to be switched to the anime face mod­el, of course</li>
<li class="cyxy-trs-source">di­men­sions must be re­duced 1024→512 as ap­pro­pri­ate; some ranges are hard­coded and must be re­duced for 512px im­ages as well</li>
<li class="cyxy-trs-source cyxy-trs-source">the trun­ca­tion trick fig­ure 8 does­n’t show enough faces to give in­sight into what the la­tent space is do­ing so it needs to be ex­panded to show both more ran­dom seed­s/<wbr>­faces, and more Ψ val­ues</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">the bed­room/<wbr>­car/<wbr>­cat sam­ples should be dis­abled</li>
</ul>
<p class="cyxy-trs-source">The changes I make are as fol­lows:</p>
<div class="sourceCode collapse" id="cb23"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Diff"><code class="sourceCode diff"><span class="kw">diff --git a/generate_figures.py b/generate_figures.py</span>
index 45b68b8..f27af9d 100755
<span class="dt">--- a/generate_figures.py</span>
<span class="dt">+++ b/generate_figures.py</span>
<span class="dt">@@ -24,16 +24,13 @@ url_bedrooms    = 'https://drive.google.com/uc?id=1MOSKeGF0FJcivpBI7s63V9YHloUTO</span>
 url_cars        = 'https://drive.google.com/uc?id=1MJ6iCfNtMIRicihwRorsM3b7mmtmK9c3' # karras2019stylegan-cars-512x384.pkl
 url_cats        = 'https://drive.google.com/uc?id=1MQywl0FNt6lHu8E_EUqnRbviagS7fbiJ' # karras2019stylegan-cats-256x256.pkl

<span class="st">-synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True), minibatch_size=8)</span>
<span class="va">+synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True), minibatch_size=8, truncation_psi=0.7)</span>

 _Gs_cache = dict()

 def load_Gs(url):
<span class="st">-    if url not in _Gs_cache:</span>
<span class="st">-        with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:</span>
<span class="st">-            _G, _D, Gs = pickle.load(f)</span>
<span class="st">-        _Gs_cache[url] = Gs</span>
<span class="st">-    return _Gs_cache[url]</span>
<span class="va">+    _G, _D, Gs = pickle.load(open("results/02051-sgan-faces-2gpu/network-snapshot-021980.pkl", "rb"))</span>
<span class="va">+    return Gs</span>

 #----------------------------------------------------------------------------
 # Figures 2, 3, 10, 11, 12: Multi-resolution grid of uncurated result images.
<span class="dt">@@ -85,7 +82,7 @@ def draw_noise_detail_figure(png, Gs, w, h, num_samples, seeds):</span>
     canvas = PIL.Image.new('RGB', (w * 3, h * len(seeds)), 'white')
     for row, seed in enumerate(seeds):
         latents = np.stack([np.random.RandomState(seed).randn(Gs.input_shape[1])] * num_samples)
<span class="st">-        images = Gs.run(latents, None, truncation_psi=1, **synthesis_kwargs)</span>
<span class="va">+        images = Gs.run(latents, None, **synthesis_kwargs)</span>
         canvas.paste(PIL.Image.fromarray(images[0], 'RGB'), (0, row * h))
         for i in range(4):
             crop = PIL.Image.fromarray(images[i + 1], 'RGB')
<span class="dt">@@ -109,7 +106,7 @@ def draw_noise_components_figure(png, Gs, w, h, seeds, noise_ranges, flips):</span>
     all_images = []
     for noise_range in noise_ranges:
         tflib.set_vars({var: val * (1 if i in noise_range else 0) for i, (var, val) in enumerate(noise_pairs)})
<span class="st">-        range_images = Gsc.run(latents, None, truncation_psi=1, randomize_noise=False, **synthesis_kwargs)</span>
<span class="va">+        range_images = Gsc.run(latents, None, randomize_noise=False, **synthesis_kwargs)</span>
         range_images[flips, :, :] = range_images[flips, :, ::-1]
         all_images.append(list(range_images))

<span class="dt">@@ -144,14 +141,11 @@ def draw_truncation_trick_figure(png, Gs, w, h, seeds, psis):</span>
 def main():
     tflib.init_tf()
     os.makedirs(config.result_dir, exist_ok=True)
<span class="st">-    draw_uncurated_result_figure(os.path.join(config.result_dir, 'figure02-uncurated-ffhq.png'), load_Gs(url_ffhq), cx=0, cy=0, cw=1024, ch=1024, rows=3, lods=[0,1,2,2,3,3], seed=5)</span>
<span class="st">-    draw_style_mixing_figure(os.path.join(config.result_dir, 'figure03-style-mixing.png'), load_Gs(url_ffhq), w=1024, h=1024, src_seeds=[639,701,687,615,2268], dst_seeds=[888,829,1898,1733,1614,845], style_ranges=[range(0,4)]*3+[range(4,8)]*2+[range(8,18)])</span>
<span class="st">-    draw_noise_detail_figure(os.path.join(config.result_dir, 'figure04-noise-detail.png'), load_Gs(url_ffhq), w=1024, h=1024, num_samples=100, seeds=[1157,1012])</span>
<span class="st">-    draw_noise_components_figure(os.path.join(config.result_dir, 'figure05-noise-components.png'), load_Gs(url_ffhq), w=1024, h=1024, seeds=[1967,1555], noise_ranges=[range(0, 18), range(0, 0), range(8, 18), range(0, 8)], flips=[1])</span>
<span class="st">-    draw_truncation_trick_figure(os.path.join(config.result_dir, 'figure08-truncation-trick.png'), load_Gs(url_ffhq), w=1024, h=1024, seeds=[91,388], psis=[1, 0.7, 0.5, 0, -0.5, -1])</span>
<span class="st">-    draw_uncurated_result_figure(os.path.join(config.result_dir, 'figure10-uncurated-bedrooms.png'), load_Gs(url_bedrooms), cx=0, cy=0, cw=256, ch=256, rows=5, lods=[0,0,1,1,2,2,2], seed=0)</span>
<span class="st">-    draw_uncurated_result_figure(os.path.join(config.result_dir, 'figure11-uncurated-cars.png'), load_Gs(url_cars), cx=0, cy=64, cw=512, ch=384, rows=4, lods=[0,1,2,2,3,3], seed=2)</span>
<span class="st">-    draw_uncurated_result_figure(os.path.join(config.result_dir, 'figure12-uncurated-cats.png'), load_Gs(url_cats), cx=0, cy=0, cw=256, ch=256, rows=5, lods=[0,0,1,1,2,2,2], seed=1)</span>
<span class="va">+    draw_uncurated_result_figure(os.path.join(config.result_dir, 'figure02-uncurated-ffhq.png'), load_Gs(url_ffhq), cx=0, cy=0, cw=512, ch=512, rows=3, lods=[0,1,2,2,3,3], seed=5)</span>
<span class="va">+    draw_style_mixing_figure(os.path.join(config.result_dir, 'figure03-style-mixing.png'), load_Gs(url_ffhq), w=512, h=512, src_seeds=[639,701,687,615,2268], dst_seeds=[888,829,1898,1733,1614,845], style_ranges=[range(0,4)]*3+[range(4,8)]*2+[range(8,16)])</span>
<span class="va">+    draw_noise_detail_figure(os.path.join(config.result_dir, 'figure04-noise-detail.png'), load_Gs(url_ffhq), w=512, h=512, num_samples=100, seeds=[1157,1012])</span>
<span class="va">+    draw_noise_components_figure(os.path.join(config.result_dir, 'figure05-noise-components.png'), load_Gs(url_ffhq), w=512, h=512, seeds=[1967,1555], noise_ranges=[range(0, 18), range(0, 0), range(8, 18), range(0, 8)], flips=[1])</span>
<span class="va">+    draw_truncation_trick_figure(os.path.join(config.result_dir, 'figure08-truncation-trick.png'), load_Gs(url_ffhq), w=512, h=512, seeds=[91,388, 389, 390, 391, 392, 393, 394, 395, 396], psis=[1, 0.7, 0.5, 0.25, 0, -0.25, -0.5, -1])</span></code></pre></div>
<p class="cyxy-trs-source">All this done, we get some fun anime face sam­ples to par­al­lel Kar­ras&nbsp;et&nbsp;al&nbsp;2018’s fig­ures:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Anime face  Fig­ure 2, un­cu­rated sam­ples" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1152px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-17-stylegan-animeface-figure02-randomsamples.jpg" srcset="/images/gan/stylegan/2019-03-17-stylegan-animeface-figure02-randomsamples.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-17-stylegan-animeface-figure02-randomsamples.jpg 1152w" width="1152" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, Fig­ure 2, un­cu­rated sam­ples</figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Fig­ure 3, “style mix­ing” of  faces, demon­strat­ing con­trol &amp; in­ter­po­la­tion (top row = style, left colum­n=­tar­get to be styled)" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1200px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-17-stylegan-animeface-figure03-stylemixing.jpg" srcset="/images/gan/stylegan/2019-03-17-stylegan-animeface-figure03-stylemixing.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-17-stylegan-animeface-figure03-stylemixing.jpg 1200w" width="1200" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Fig­ure 3, “style mix­ing” of source/<wbr>­trans­fer faces, demon­strat­ing con­trol &amp; in­ter­po­la­tion (top row = style, left colum­n=­tar­get to be styled)</figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Fig­ure 8, the “trun­ca­tion trick” vi­su­al­ized: 10 ran­dom faces, with the range Ψ = [1, 0.7, 0.5, 0.25, 0, −0.25, −0.5, −1]—demon­strat­ing the trade­off be­tween di­ver­sity &amp; qual­i­ty, and the global av­er­age face." height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1120px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-17-stylegan-animeface-figure08-truncationtrick.jpg" srcset="/images/gan/stylegan/2019-03-17-stylegan-animeface-figure08-truncationtrick.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-17-stylegan-animeface-figure08-truncationtrick.jpg 1120w" width="1120" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">Fig­ure 8, the “trun­ca­tion trick” vi­su­al­ized: 10 ran­dom faces, with the range Ψ = [1, 0.7, 0.5, 0.25, 0, −0.25, −0.5, −1]—demon­strat­ing the trade­off be­tween di­ver­sity &amp; qual­i­ty, and the global av­er­age face.</figcaption></span></span></figure>
</section>
<section id="videos" class="level2">
<h2><a href="https://www.gwern.net/Faces#videos" title="Link to section: § &#39;Videos&#39;" class="no-popup cyxy-trs-source">Videos</a></h2>
<section id="training-montage" class="level3">
<h3><a href="https://www.gwern.net/Faces#training-montage" title="Link to section: § &#39;Training Montage&#39;" class="no-popup cyxy-trs-source">Training Montage</a></h3>
<p class="cyxy-trs-source">The eas­i­est sam­ples are the progress snap­shots gen­er­ated dur­ing train­ing. Over the course of train­ing, their size in­creases as the effec­tive res­o­lu­tion in­creases &amp; finer de­tails are gen­er­at­ed, and at the end can be quite large (often 14MB each for the anime faces) so do­ing lossy com­pres­sion with a tool like <code>pngnq</code>+<code>advpng</code> or con­vert­ing them to <span class="smallcaps-auto">JPG</span> with low­ered qual­ity is a good idea. To turn the many snap­shots into a train­ing mon­tage video like above, I use <a href="https://en.wikipedia.org/wiki/FFmpeg" class="docMetadata has-annotation spawns-popup">FFm­peg</a> on the <span class="smallcaps-auto">PNG</span>s:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">cat</span> <span class="va">$(</span><span class="fu">ls</span> ./results/<span class="pp">*</span>faces<span class="pp">*</span>/fakes<span class="pp">*</span>.png <span class="kw">|</span> <span class="fu">sort</span> <span class="at">--numeric-sort</span><span class="va">)</span> <span class="kw">|</span> <span class="ex">ffmpeg</span> <span class="at">-framerate</span> 10 <span class="dt">\ </span># show 10 inputs per second
    <span class="ex">-i</span> <span class="at">-</span> <span class="co"># stdin</span>
    <span class="ex">-r</span> 25 <span class="co"># output frame-rate; frames will be duplicated to pad out to 25FPS</span>
    <span class="ex">-c:v</span> libx264 <span class="co"># x264 for compatibility</span>
    <span class="ex">-pix_fmt</span> yuv420p <span class="co"># force ffmpeg to use a standard colorspace - otherwise PNG colorspace is kept, breaking browsers (!)</span>
    <span class="ex">-crf</span> 33 <span class="co"># adequate high quality</span>
    <span class="ex">-vf</span> <span class="st">"scale=iw/2:ih/2"</span> <span class="dt">\ </span># shrink the image by 2×, the full detail is not necessary <span class="kw">&amp;</span> <span class="ex">saves</span> space
    <span class="ex">-preset</span> veryslow <span class="at">-tune</span> animation <span class="dt">\ </span># aim for smallest binary possible with animation-tuned settings
    <span class="ex">./stylegan-facestraining.mp4</span></code></pre></div>
</section>
<section id="interpolations" class="level3">
<h3><a href="https://www.gwern.net/Faces#interpolations" title="Link to section: § &#39;Interpolations&#39;" class="no-popup cyxy-trs-source">Interpolations</a></h3>
<p class="cyxy-trs-source">The orig­i­nal Pro<span class="smallcaps-auto">GAN</span> repo pro­vided a con­fig for gen­er­at­ing in­ter­po­la­tion videos, but that was re­moved in Style<span class="smallcaps-auto">GAN</span>. <a href="https://www.gwern.net/docs/www/cyrildiagne.com/d00815952159013917b207b1f6e6998ed6aedc16.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://cyrildiagne.com/" data-attribute-title="(Original URL: http://cyrildiagne.com/ )">Cyril Di­agne</a> (<code>@kikko_fr</code>) <a href="https://colab.research.google.com/gist/kikko/d48c1871206fc325fa6f7372cf58db87/stylegan-experiments.ipynb" class="no-popup">im­ple­mented a re­place­ment</a>⁠, pro­vid­ing 3 kinds of videos:</p>
<ol type="1">
<li><p class="cyxy-trs-source"><code>random_grid_404.mp4</code>: a stan­dard in­ter­po­la­tion video, which is sim­ply a ran­dom walk through the la­tent space, mod­i­fy­ing all the vari­ables smoothly and an­i­mat­ing it; by de­fault it makes 4 of them arranged 2×2 in the video. Sev­eral in­ter­po­la­tion videos are show in the <a href="https://www.gwern.net/Faces#examples" class="link-self identifier-link-up has-content spawns-popup">ex­am­ples sec­tion</a>⁠.</p></li>
<li><p class="cyxy-trs-source"><code>interpolate.mp4</code>: a ‘coarse’ “style mix­ing” video; a sin­gle ‘source’ face is gen­er­ated &amp; held con­stant; a sec­ondary in­ter­po­la­tion video, a ran­dom walk as be­fore is gen­er­at­ed; at each step of the ran­dom walk, the ‘coarse’/<wbr>high­-level ‘style’ noise is copied from the ran­dom walk to over­write the source face’s orig­i­nal style noise. For faces, this means that the orig­i­nal face will be mod­i­fied with all sorts of ori­en­ta­tions &amp; fa­cial ex­pres­sions while still re­main­ing rec­og­niz­ably the orig­i­nal char­ac­ter. (It is the video ana­log of Kar­ras&nbsp;et&nbsp;al&nbsp;2018’s Fig­ure 3.)</p>
<p class="cyxy-trs-source">A copy of Di­ag­ne’s <code>video.py</code>:</p>
<div class="sourceCode collapse" id="cb25"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Python"><code class="sourceCode python"><span class="im">import</span> os
<span class="im">import</span> pickle
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> PIL.Image
<span class="im">import</span> dnnlib
<span class="im">import</span> dnnlib.tflib <span class="im">as</span> tflib
<span class="im">import</span> config
<span class="im">import</span> scipy

<span class="kw">def</span> main():

    tflib.init_tf()

    <span class="co"># Load pre-trained network.</span>
    <span class="co"># url = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ'</span>
    <span class="co"># with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:</span>
    <span class="co">## </span><span class="al">NOTE</span><span class="co">: insert model here:</span>
    _G, _D, Gs <span class="op">=</span> pickle.load(<span class="bu">open</span>(<span class="st">"results/02047-sgan-faces-2gpu/network-snapshot-013221.pkl"</span>, <span class="st">"rb"</span>))
    <span class="co"># _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.</span>
    <span class="co"># _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.</span>
    <span class="co"># Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.</span>

    grid_size <span class="op">=</span> [<span class="dv">2</span>,<span class="dv">2</span>]
    image_shrink <span class="op">=</span> <span class="dv">1</span>
    image_zoom <span class="op">=</span> <span class="dv">1</span>
    duration_sec <span class="op">=</span> <span class="fl">60.0</span>
    smoothing_sec <span class="op">=</span> <span class="fl">1.0</span>
    mp4_fps <span class="op">=</span> <span class="dv">20</span>
    mp4_codec <span class="op">=</span> <span class="st">'libx264'</span>
    mp4_bitrate <span class="op">=</span> <span class="st">'5M'</span>
    random_seed <span class="op">=</span> <span class="dv">404</span>
    mp4_file <span class="op">=</span> <span class="st">'results/random_grid_</span><span class="sc">%s</span><span class="st">.mp4'</span> <span class="op">%</span> random_seed
    minibatch_size <span class="op">=</span> <span class="dv">8</span>

    num_frames <span class="op">=</span> <span class="bu">int</span>(np.rint(duration_sec <span class="op">*</span> mp4_fps))
    random_state <span class="op">=</span> np.random.RandomState(random_seed)

    <span class="co"># Generate latent vectors</span>
    shape <span class="op">=</span> [num_frames, np.prod(grid_size)] <span class="op">+</span> Gs.input_shape[<span class="dv">1</span>:] <span class="co"># [frame, image, channel, component]</span>
    all_latents <span class="op">=</span> random_state.randn(<span class="op">*</span>shape).astype(np.float32)
    <span class="im">import</span> scipy
    all_latents <span class="op">=</span> scipy.ndimage.gaussian_filter(all_latents,
                   [smoothing_sec <span class="op">*</span> mp4_fps] <span class="op">+</span> [<span class="dv">0</span>] <span class="op">*</span> <span class="bu">len</span>(Gs.input_shape), mode<span class="op">=</span><span class="st">'wrap'</span>)
    all_latents <span class="op">/=</span> np.sqrt(np.mean(np.square(all_latents)))


    <span class="kw">def</span> create_image_grid(images, grid_size<span class="op">=</span><span class="va">None</span>):
        <span class="cf">assert</span> images.ndim <span class="op">==</span> <span class="dv">3</span> <span class="kw">or</span> images.ndim <span class="op">==</span> <span class="dv">4</span>
        num, img_h, img_w, channels <span class="op">=</span> images.shape

        <span class="cf">if</span> grid_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:
            grid_w, grid_h <span class="op">=</span> <span class="bu">tuple</span>(grid_size)
        <span class="cf">else</span>:
            grid_w <span class="op">=</span> <span class="bu">max</span>(<span class="bu">int</span>(np.ceil(np.sqrt(num))), <span class="dv">1</span>)
            grid_h <span class="op">=</span> <span class="bu">max</span>((num <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> grid_w <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)

        grid <span class="op">=</span> np.zeros([grid_h <span class="op">*</span> img_h, grid_w <span class="op">*</span> img_w, channels], dtype<span class="op">=</span>images.dtype)
        <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(num):
            x <span class="op">=</span> (idx <span class="op">%</span> grid_w) <span class="op">*</span> img_w
            y <span class="op">=</span> (idx <span class="op">//</span> grid_w) <span class="op">*</span> img_h
            grid[y : y <span class="op">+</span> img_h, x : x <span class="op">+</span> img_w] <span class="op">=</span> images[idx]
        <span class="cf">return</span> grid

    <span class="co"># Frame generation func for moviepy.</span>
    <span class="kw">def</span> make_frame(t):
        frame_idx <span class="op">=</span> <span class="bu">int</span>(np.clip(np.<span class="bu">round</span>(t <span class="op">*</span> mp4_fps), <span class="dv">0</span>, num_frames <span class="op">-</span> <span class="dv">1</span>))
        latents <span class="op">=</span> all_latents[frame_idx]
        fmt <span class="op">=</span> <span class="bu">dict</span>(func<span class="op">=</span>tflib.convert_images_to_uint8, nchw_to_nhwc<span class="op">=</span><span class="va">True</span>)
        images <span class="op">=</span> Gs.run(latents, <span class="va">None</span>, truncation_psi<span class="op">=</span><span class="fl">0.7</span>,
                              randomize_noise<span class="op">=</span><span class="va">False</span>, output_transform<span class="op">=</span>fmt)

        grid <span class="op">=</span> create_image_grid(images, grid_size)
        <span class="cf">if</span> image_zoom <span class="op">&gt;</span> <span class="dv">1</span>:
            grid <span class="op">=</span> scipy.ndimage.zoom(grid, [image_zoom, image_zoom, <span class="dv">1</span>], order<span class="op">=</span><span class="dv">0</span>)
        <span class="cf">if</span> grid.shape[<span class="dv">2</span>] <span class="op">==</span> <span class="dv">1</span>:
            grid <span class="op">=</span> grid.repeat(<span class="dv">3</span>, <span class="dv">2</span>) <span class="co"># grayscale =&gt; RGB</span>
        <span class="cf">return</span> grid

    <span class="co"># Generate video.</span>
    <span class="im">import</span> moviepy.editor
    video_clip <span class="op">=</span> moviepy.editor.VideoClip(make_frame, duration<span class="op">=</span>duration_sec)
    video_clip.write_videofile(mp4_file, fps<span class="op">=</span>mp4_fps, codec<span class="op">=</span>mp4_codec, bitrate<span class="op">=</span>mp4_bitrate)

    <span class="co"># import scipy</span>
    <span class="co"># coarse</span>
    duration_sec <span class="op">=</span> <span class="fl">60.0</span>
    smoothing_sec <span class="op">=</span> <span class="fl">1.0</span>
    mp4_fps <span class="op">=</span> <span class="dv">20</span>

    num_frames <span class="op">=</span> <span class="bu">int</span>(np.rint(duration_sec <span class="op">*</span> mp4_fps))
    random_seed <span class="op">=</span> <span class="dv">500</span>
    random_state <span class="op">=</span> np.random.RandomState(random_seed)


    w <span class="op">=</span> <span class="dv">512</span>
    h <span class="op">=</span> <span class="dv">512</span>
    <span class="co">#src_seeds = [601]</span>
    dst_seeds <span class="op">=</span> [<span class="dv">700</span>]
    style_ranges <span class="op">=</span> ([<span class="dv">0</span>] <span class="op">*</span> <span class="dv">7</span> <span class="op">+</span> [<span class="bu">range</span>(<span class="dv">8</span>,<span class="dv">16</span>)]) <span class="op">*</span> <span class="bu">len</span>(dst_seeds)

    fmt <span class="op">=</span> <span class="bu">dict</span>(func<span class="op">=</span>tflib.convert_images_to_uint8, nchw_to_nhwc<span class="op">=</span><span class="va">True</span>)
    synthesis_kwargs <span class="op">=</span> <span class="bu">dict</span>(output_transform<span class="op">=</span>fmt, truncation_psi<span class="op">=</span><span class="fl">0.7</span>, minibatch_size<span class="op">=</span><span class="dv">8</span>)

    shape <span class="op">=</span> [num_frames] <span class="op">+</span> Gs.input_shape[<span class="dv">1</span>:] <span class="co"># [frame, image, channel, component]</span>
    src_latents <span class="op">=</span> random_state.randn(<span class="op">*</span>shape).astype(np.float32)
    src_latents <span class="op">=</span> scipy.ndimage.gaussian_filter(src_latents,
                                                smoothing_sec <span class="op">*</span> mp4_fps,
                                                mode<span class="op">=</span><span class="st">'wrap'</span>)
    src_latents <span class="op">/=</span> np.sqrt(np.mean(np.square(src_latents)))

    dst_latents <span class="op">=</span> np.stack(np.random.RandomState(seed).randn(Gs.input_shape[<span class="dv">1</span>]) <span class="cf">for</span> seed <span class="kw">in</span> dst_seeds)


    src_dlatents <span class="op">=</span> Gs.components.mapping.run(src_latents, <span class="va">None</span>) <span class="co"># [seed, layer, component]</span>
    dst_dlatents <span class="op">=</span> Gs.components.mapping.run(dst_latents, <span class="va">None</span>) <span class="co"># [seed, layer, component]</span>
    src_images <span class="op">=</span> Gs.components.synthesis.run(src_dlatents, randomize_noise<span class="op">=</span><span class="va">False</span>, <span class="op">**</span>synthesis_kwargs)
    dst_images <span class="op">=</span> Gs.components.synthesis.run(dst_dlatents, randomize_noise<span class="op">=</span><span class="va">False</span>, <span class="op">**</span>synthesis_kwargs)


    canvas <span class="op">=</span> PIL.Image.new(<span class="st">'RGB'</span>, (w <span class="op">*</span> (<span class="bu">len</span>(dst_seeds) <span class="op">+</span> <span class="dv">1</span>), h <span class="op">*</span> <span class="dv">2</span>), <span class="st">'white'</span>)

    <span class="cf">for</span> col, dst_image <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">list</span>(dst_images)):
        canvas.paste(PIL.Image.fromarray(dst_image, <span class="st">'RGB'</span>), ((col <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> h, <span class="dv">0</span>))

    <span class="kw">def</span> make_frame(t):
        frame_idx <span class="op">=</span> <span class="bu">int</span>(np.clip(np.<span class="bu">round</span>(t <span class="op">*</span> mp4_fps), <span class="dv">0</span>, num_frames <span class="op">-</span> <span class="dv">1</span>))
        src_image <span class="op">=</span> src_images[frame_idx]
        canvas.paste(PIL.Image.fromarray(src_image, <span class="st">'RGB'</span>), (<span class="dv">0</span>, h))

        <span class="cf">for</span> col, dst_image <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">list</span>(dst_images)):
            col_dlatents <span class="op">=</span> np.stack([dst_dlatents[col]])
            col_dlatents[:, style_ranges[col]] <span class="op">=</span> src_dlatents[frame_idx, style_ranges[col]]
            col_images <span class="op">=</span> Gs.components.synthesis.run(col_dlatents, randomize_noise<span class="op">=</span><span class="va">False</span>, <span class="op">**</span>synthesis_kwargs)
            <span class="cf">for</span> row, image <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">list</span>(col_images)):
                canvas.paste(PIL.Image.fromarray(image, <span class="st">'RGB'</span>), ((col <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> h, (row <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> w))
        <span class="cf">return</span> np.array(canvas)

    <span class="co"># Generate video.</span>
    <span class="im">import</span> moviepy.editor
    mp4_file <span class="op">=</span> <span class="st">'results/interpolate.mp4'</span>
    mp4_codec <span class="op">=</span> <span class="st">'libx264'</span>
    mp4_bitrate <span class="op">=</span> <span class="st">'5M'</span>

    video_clip <span class="op">=</span> moviepy.editor.VideoClip(make_frame, duration<span class="op">=</span>duration_sec)
    video_clip.write_videofile(mp4_file, fps<span class="op">=</span>mp4_fps, codec<span class="op">=</span>mp4_codec, bitrate<span class="op">=</span>mp4_bitrate)

    <span class="im">import</span> scipy

    duration_sec <span class="op">=</span> <span class="fl">60.0</span>
    smoothing_sec <span class="op">=</span> <span class="fl">1.0</span>
    mp4_fps <span class="op">=</span> <span class="dv">20</span>

    num_frames <span class="op">=</span> <span class="bu">int</span>(np.rint(duration_sec <span class="op">*</span> mp4_fps))
    random_seed <span class="op">=</span> <span class="dv">503</span>
    random_state <span class="op">=</span> np.random.RandomState(random_seed)


    w <span class="op">=</span> <span class="dv">512</span>
    h <span class="op">=</span> <span class="dv">512</span>
    style_ranges <span class="op">=</span> [<span class="bu">range</span>(<span class="dv">6</span>,<span class="dv">16</span>)]

    fmt <span class="op">=</span> <span class="bu">dict</span>(func<span class="op">=</span>tflib.convert_images_to_uint8, nchw_to_nhwc<span class="op">=</span><span class="va">True</span>)
    synthesis_kwargs <span class="op">=</span> <span class="bu">dict</span>(output_transform<span class="op">=</span>fmt, truncation_psi<span class="op">=</span><span class="fl">0.7</span>, minibatch_size<span class="op">=</span><span class="dv">8</span>)

    shape <span class="op">=</span> [num_frames] <span class="op">+</span> Gs.input_shape[<span class="dv">1</span>:] <span class="co"># [frame, image, channel, component]</span>
    src_latents <span class="op">=</span> random_state.randn(<span class="op">*</span>shape).astype(np.float32)
    src_latents <span class="op">=</span> scipy.ndimage.gaussian_filter(src_latents,
                                                smoothing_sec <span class="op">*</span> mp4_fps,
                                                mode<span class="op">=</span><span class="st">'wrap'</span>)
    src_latents <span class="op">/=</span> np.sqrt(np.mean(np.square(src_latents)))

    dst_latents <span class="op">=</span> np.stack([random_state.randn(Gs.input_shape[<span class="dv">1</span>])])


    src_dlatents <span class="op">=</span> Gs.components.mapping.run(src_latents, <span class="va">None</span>) <span class="co"># [seed, layer, component]</span>
    dst_dlatents <span class="op">=</span> Gs.components.mapping.run(dst_latents, <span class="va">None</span>) <span class="co"># [seed, layer, component]</span>


    <span class="kw">def</span> make_frame(t):
        frame_idx <span class="op">=</span> <span class="bu">int</span>(np.clip(np.<span class="bu">round</span>(t <span class="op">*</span> mp4_fps), <span class="dv">0</span>, num_frames <span class="op">-</span> <span class="dv">1</span>))
        col_dlatents <span class="op">=</span> np.stack([dst_dlatents[<span class="dv">0</span>]])
        col_dlatents[:, style_ranges[<span class="dv">0</span>]] <span class="op">=</span> src_dlatents[frame_idx, style_ranges[<span class="dv">0</span>]]
        col_images <span class="op">=</span> Gs.components.synthesis.run(col_dlatents, randomize_noise<span class="op">=</span><span class="va">False</span>, <span class="op">**</span>synthesis_kwargs)
        <span class="cf">return</span> col_images[<span class="dv">0</span>]

    <span class="co"># Generate video.</span>
    <span class="im">import</span> moviepy.editor
    mp4_file <span class="op">=</span> <span class="st">'results/fine_</span><span class="sc">%s</span><span class="st">.mp4'</span> <span class="op">%</span> (random_seed)
    mp4_codec <span class="op">=</span> <span class="st">'libx264'</span>
    mp4_bitrate <span class="op">=</span> <span class="st">'5M'</span>

    video_clip <span class="op">=</span> moviepy.editor.VideoClip(make_frame, duration<span class="op">=</span>duration_sec)
    video_clip.write_videofile(mp4_file, fps<span class="op">=</span>mp4_fps, codec<span class="op">=</span>mp4_codec, bitrate<span class="op">=</span>mp4_bitrate)

<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:
    main()</code></pre></div>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-17-stylegan-animeface-interpolation-coarse.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption>
<p class="cyxy-trs-source"><a href="https://www.gwern.net/images/gan/stylegan/2019-03-17-stylegan-animeface-interpolation-coarse.mp4" class="has-content spawns-popup">‘Coarse’ style-trans­fer­/<wbr>in­ter­po­la­tion video</a></p>
</figcaption></span></span></figure></li>
<li><p class="cyxy-trs-source"><code>fine_503.mp4</code>: a ‘fine’ style mix­ing video; in this case, the style noise is taken from later on and in­stead of affect­ing the global ori­en­ta­tion or ex­pres­sion, it affects sub­tler de­tails like the pre­cise shape of hair strands or hair color or mouths.</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-17-stylegan-animeface-interpolation-fine.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption>
<p class="cyxy-trs-source"><a href="https://www.gwern.net/images/gan/stylegan/2019-03-17-stylegan-animeface-interpolation-fine.mp4" class="has-content spawns-popup">‘Fine’ style-trans­fer­/<wbr>in­ter­po­la­tion video</a></p>
</figcaption></span></span></figure></li>
</ol>
<p class="cyxy-trs-source"><em>Cir­cu­lar in­ter­po­la­tions</em> are an­other in­ter­est­ing kind of in­ter­po­la­tion, writ­ten by <a href="https://nitter.cc/halcy/status/1098223180454477824" class="no-popup">snowy halcy</a>⁠, which in­stead of ran­dom walk­ing around the la­tent space freely, with large or awk­ward tran­si­tions, in­stead tries to move around a fixed high­-di­men­sional point do­ing: “bi­nary search to get the <span class="smallcaps-auto">MSE</span> to be roughly the same be­tween frames (s­lightly brute force, but it looks nicer), and then did that for what is prob­a­bly close to a sphere or cir­cle in the la­tent space.” A later ver­sion of cir­cu­lar in­ter­po­la­tion is in snowy hal­cy’s <a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images" class="link-self identifier-link-down has-content spawns-popup">face ed­i­tor</a> re­po, but here is the orig­i­nal ver­sion cleaned up into a stand-alone pro­gram:</p>
<div class="sourceCode collapse" id="cb26"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Python"><code class="sourceCode python"><span class="im">import</span> dnnlib.tflib <span class="im">as</span> tflib
<span class="im">import</span> math
<span class="im">import</span> moviepy.editor
<span class="im">from</span> numpy <span class="im">import</span> linalg
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> pickle

<span class="kw">def</span> main():
    tflib.init_tf()
    _G, _D, Gs <span class="op">=</span> pickle.load(<span class="bu">open</span>(<span class="st">"results/02051-sgan-faces-2gpu/network-snapshot-021980.pkl"</span>, <span class="st">"rb"</span>))

    rnd <span class="op">=</span> np.random
    latents_a <span class="op">=</span> rnd.randn(<span class="dv">1</span>, Gs.input_shape[<span class="dv">1</span>])
    latents_b <span class="op">=</span> rnd.randn(<span class="dv">1</span>, Gs.input_shape[<span class="dv">1</span>])
    latents_c <span class="op">=</span> rnd.randn(<span class="dv">1</span>, Gs.input_shape[<span class="dv">1</span>])

    <span class="kw">def</span> circ_generator(latents_interpolate):
        radius <span class="op">=</span> <span class="fl">40.0</span>

        latents_axis_x <span class="op">=</span> (latents_a <span class="op">-</span> latents_b).flatten() <span class="op">/</span> linalg.norm(latents_a <span class="op">-</span> latents_b)
        latents_axis_y <span class="op">=</span> (latents_a <span class="op">-</span> latents_c).flatten() <span class="op">/</span> linalg.norm(latents_a <span class="op">-</span> latents_c)

        latents_x <span class="op">=</span> math.sin(math.pi <span class="op">*</span> <span class="fl">2.0</span> <span class="op">*</span> latents_interpolate) <span class="op">*</span> radius
        latents_y <span class="op">=</span> math.cos(math.pi <span class="op">*</span> <span class="fl">2.0</span> <span class="op">*</span> latents_interpolate) <span class="op">*</span> radius

        latents <span class="op">=</span> latents_a <span class="op">+</span> latents_x <span class="op">*</span> latents_axis_x <span class="op">+</span> latents_y <span class="op">*</span> latents_axis_y
        <span class="cf">return</span> latents

    <span class="kw">def</span> mse(x, y):
        <span class="cf">return</span> (np.square(x <span class="op">-</span> y)).mean()

    <span class="kw">def</span> generate_from_generator_adaptive(gen_func):
        max_step <span class="op">=</span> <span class="fl">1.0</span>
        current_pos <span class="op">=</span> <span class="fl">0.0</span>

        change_min <span class="op">=</span> <span class="fl">10.0</span>
        change_max <span class="op">=</span> <span class="fl">11.0</span>

        fmt <span class="op">=</span> <span class="bu">dict</span>(func<span class="op">=</span>tflib.convert_images_to_uint8, nchw_to_nhwc<span class="op">=</span><span class="va">True</span>)

        current_latent <span class="op">=</span> gen_func(current_pos)
        current_image <span class="op">=</span> Gs.run(current_latent, <span class="va">None</span>, truncation_psi<span class="op">=</span><span class="fl">0.7</span>, randomize_noise<span class="op">=</span><span class="va">False</span>, output_transform<span class="op">=</span>fmt)[<span class="dv">0</span>]
        array_list <span class="op">=</span> []

        video_length <span class="op">=</span> <span class="fl">1.0</span>
        <span class="cf">while</span>(current_pos <span class="op">&lt;</span> video_length):
            array_list.append(current_image)

            lower <span class="op">=</span> current_pos
            upper <span class="op">=</span> current_pos <span class="op">+</span> max_step
            current_pos <span class="op">=</span> (upper <span class="op">+</span> lower) <span class="op">/</span> <span class="fl">2.0</span>

            current_latent <span class="op">=</span> gen_func(current_pos)
            current_image <span class="op">=</span> images <span class="op">=</span> Gs.run(current_latent, <span class="va">None</span>, truncation_psi<span class="op">=</span><span class="fl">0.7</span>, randomize_noise<span class="op">=</span><span class="va">False</span>, output_transform<span class="op">=</span>fmt)[<span class="dv">0</span>]
            current_mse <span class="op">=</span> mse(array_list[<span class="op">-</span><span class="dv">1</span>], current_image)

            <span class="cf">while</span> current_mse <span class="op">&lt;</span> change_min <span class="kw">or</span> current_mse <span class="op">&gt;</span> change_max:
                <span class="cf">if</span> current_mse <span class="op">&lt;</span> change_min:
                    lower <span class="op">=</span> current_pos
                    current_pos <span class="op">=</span> (upper <span class="op">+</span> lower) <span class="op">/</span> <span class="fl">2.0</span>

                <span class="cf">if</span> current_mse <span class="op">&gt;</span> change_max:
                    upper <span class="op">=</span> current_pos
                    current_pos <span class="op">=</span> (upper <span class="op">+</span> lower) <span class="op">/</span> <span class="fl">2.0</span>


                current_latent <span class="op">=</span> gen_func(current_pos)
                current_image <span class="op">=</span> images <span class="op">=</span> Gs.run(current_latent, <span class="va">None</span>, truncation_psi<span class="op">=</span><span class="fl">0.7</span>, randomize_noise<span class="op">=</span><span class="va">False</span>, output_transform<span class="op">=</span>fmt)[<span class="dv">0</span>]
                current_mse <span class="op">=</span> mse(array_list[<span class="op">-</span><span class="dv">1</span>], current_image)
            <span class="bu">print</span>(current_pos, current_mse)
        <span class="cf">return</span> array_list

    frames <span class="op">=</span> generate_from_generator_adaptive(circ_generator)
    frames <span class="op">=</span> moviepy.editor.ImageSequenceClip(frames, fps<span class="op">=</span><span class="dv">30</span>)

    <span class="co"># Generate video.</span>
    mp4_file <span class="op">=</span> <span class="st">'results/circular.mp4'</span>
    mp4_codec <span class="op">=</span> <span class="st">'libx264'</span>
    mp4_bitrate <span class="op">=</span> <span class="st">'3M'</span>
    mp4_fps <span class="op">=</span> <span class="dv">20</span>

    frames.write_videofile(mp4_file, fps<span class="op">=</span>mp4_fps, codec<span class="op">=</span>mp4_codec, bitrate<span class="op">=</span>mp4_bitrate)

<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:
    main()</code></pre></div>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-17-stylegan-animeface-interpolation-circular.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption>
<a href="https://www.gwern.net/images/gan/stylegan/2019-03-17-stylegan-animeface-interpolation-circular.mp4" class="has-content spawns-popup cyxy-trs-source">‘Cir­cu­lar’ in­ter­po­la­tion video</a>
</figcaption></span></span></figure>
<p class="cyxy-trs-source">An in­ter­est­ing use of in­ter­po­la­tions is Kyle McLean’s <a href="https://www.gwern.net/docs/www/everyoneishappy.com/17c80492252323f59ac82996be2c5d617c545a58.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://everyoneishappy.com/portfolio/waifu-synthesis-real-time-generative-anime/" data-attribute-title="(Original URL: http://everyoneishappy.com/portfolio/waifu-synthesis-real-time-generative-anime/ )">“Waifu Syn­the­sis”</a> video: a singing anime video mash­ing up Style<span class="smallcaps-auto">GAN</span> anime faces + <a href="https://www.gwern.net/GPT-2" id="gwern-gpt-2" class="link-local docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">GPT-2</span></a> lyrics + <a href="https://www.gwern.net/docs/www/magenta.tensorflow.org/a8843ee870969c8945083ffdfe60c60e6885e2ec.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://magenta.tensorflow.org/" data-attribute-title="(Original URL: https://magenta.tensorflow.org/ )">Project Ma­genta</a> mu­sic.</p>
</section>
</section>
</section>
<section id="models" class="level1">
<h1><a href="https://www.gwern.net/Faces#models" title="Link to section: § &#39;Models&#39;" class="no-popup cyxy-trs-source">Models</a></h1>
<section id="anime-faces" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces" title="Link to section: § &#39;Anime Faces&#39;" class="no-popup cyxy-trs-source">Anime Faces</a></h2>
<p class="cyxy-trs-source">The pri­mary model I’ve trained, the anime face model is de­scribed in the data pro­cess­ing &amp; train­ing sec­tion. It is a 512px Style<span class="smallcaps-auto">GAN</span> model trained on <em>n</em> = 218,794 faces cropped from all of Dan­booru2017, cleaned, &amp; up­scaled, and trained for 21,980 it­er­a­tions or ~21m im­ages or ~38 <span class="smallcaps-auto">GPU</span>-days.</p>
<p class="cyxy-trs-source">Down­loads (I rec­om­mend us­ing the more-re­cent <a href="https://www.gwern.net/Crops#danbooru2019-portraits" id="gwern-crops-danbooru2019-portraits" class="link-local docMetadata has-annotation spawns-popup">por­trait Style<span class="smallcaps-auto">GAN</span></a> un­less cropped faces are specifi­cally de­sired):</p>
<ul>
<li><p class="cyxy-trs-source"><a href="https://mega.nz/#!2DRDQIjJ!JKQ_DhEXCzeYJXjliUSWRvE-_rfrvWv_cq3pgRuFadw" title="2019-02-14-stylegan-faces-02021-010483.tar" class="no-popup">ran­dom sam­ples</a> gen­er­ated on 2091-02-14 with an ex­treme Ψ = 1.2 (165MB, <span class="smallcaps-auto">JPG</span>)</p></li>
<li><p class="cyxy-trs-source">the <a href="https://mega.nz/#!aPRFDKaC!FDpQi_FEPK443JoRBEOEDOmlLmJSblKFlqZ1A1XPt2Y" title="2019-02-26-stylegan-faces-network-02048-016041.pkl" class="no-popup">Style<span class="smallcaps-auto">GAN</span> model used for <span class="smallcaps-auto">TWDNE</span>v1 sam­ples</a> as of 2019-02-26 (294MB, <code>.pkl</code>)</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source">all <a href="https://www.gwern.net/TWDNE#downloads" class="link-local has-content spawns-popup cyxy-trs-source"><span class="smallcaps-auto cyxy-trs-source">TWDNE</span> faces</a> via rsync</li>
</ul></li>
<li><p class="cyxy-trs-source"><a href="https://mega.nz/#!vawjXISI!F7s13yRicxDA3QYqYDL2kjnc2K7Zk3DwCIYETREmBP4" title="2019-03-08-stylegan-animefaces-network-02051-021980.pkl.xz" class="no-popup">the anime face Style<span class="smallcaps-auto">GAN</span> model</a>⁠, fur­ther trained, as of 2019-03-8</p>
<div class="admonition note cyxy-trs-source cyxy-trs-source">
The anime face model is ob­so­leted by the <a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-down has-content spawns-popup cyxy-trs-source cyxy-trs-source">Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> 2 por­trait model</a>⁠.
</div></li>
</ul>
<section id="twdne" class="level3">
<h3><a href="https://www.gwern.net/Faces#twdne" title="Link to section: § &#39;TWDNE&#39;" class="no-popup cyxy-trs-source">TWDNE</a></h3>
<p class="cyxy-trs-source">To show off the anime faces, and as a joke, on 2019-02-14, I set up <a href="https://www.thiswaifudoesnotexist.net/" id="gwern-www-thiswaifudoesnotexist-net" class="docMetadata has-annotation spawns-popup">“This Waifu Does Not Ex­ist”</a>⁠, a stand­alone sta­tic web­site which dis­plays a ran­dom anime face (out of 100,000), gen­er­ated with var­i­ous Ψ, and paired with <span class="smallcaps-auto">GPT-2-117M</span> text snip­pets prompted on anime plot sum­maries. <a href="https://www.gwern.net/TWDNE" id="gwern-twdne" class="link-local docMetadata has-annotation spawns-popup">The de­tails of the site im­ple­men­ta­tion &amp; gen­er­at­ing the faces/<wbr>­text</a> are too length to go into here</p>
<p class="cyxy-trs-source">But the site was amus­ing &amp; an enor­mous suc­cess. It went vi­ral overnight and by the end of March 2019, ~1 mil­lion unique vis­i­tors (most from Chi­na) had vis­ited <span class="smallcaps-auto">TWDNE</span>, spend­ing over 2 min­utes each look­ing at the NN-gen­er­ated faces &amp; text; peo­ple be­gan hunt­ing for hi­lar­i­ous­ly-de­formed faces, us­ing <span class="smallcaps-auto">TWDNE</span> as a screen­saver, pick­ing out faces as avatars, cre­at­ing packs of faces for video games, paint­ing their own col­lages of faces, us­ing it as a char­ac­ter de­signer for in­spi­ra­tion, etc.</p>
</section>
</section>
<section id="anime-bodies" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-bodies" title="Link to section: § &#39;Anime Bodies&#39;" class="no-popup cyxy-trs-source">Anime Bodies</a></h2>
<p class="cyxy-trs-source"><a href="https://nitter.cc/SkyLi0n" class="no-popup">Aaron Gokaslan</a> ex­per­i­mented with a cus­tom 256px anime game im­age dataset which has in­di­vid­ual char­ac­ters posed in whole-per­son im­ages to see how Style<span class="smallcaps-auto">GAN</span> coped with more com­plex geome­tries. Progress re­quired ad­di­tional data clean­ing and low­er­ing the learn­ing rate but, trained on a 4-<span class="smallcaps-auto">GPU</span> sys­tem for week or two, the re­sults are promis­ing (even down to re­pro­duc­ing the copy­right state­ments in the im­ages), pro­vid­ing pre­lim­i­nary ev­i­dence that Style<span class="smallcaps-auto">GAN</span> can scale:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Whole-body anime im­ages, ran­dom sam­ples, Aaron Gokaslan" height="754" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-13-skylion-wholebody-randomsamples.jpg" srcset="/images/gan/stylegan/2019-03-13-skylion-wholebody-randomsamples.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-13-skylion-wholebody-randomsamples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">Whole-body anime im­ages, ran­dom sam­ples, Aaron Gokaslan</figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Whole-body anime im­ages, style trans­fer among sam­ples, Aaron Gokaslan" height="725" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-13-skylion-wholebody-styletransfers.jpg" srcset="/images/gan/stylegan/2019-03-13-skylion-wholebody-styletransfers.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-13-skylion-wholebody-styletransfers.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">Whole-body anime im­ages, style trans­fer among sam­ples, Aaron Gokaslan</figcaption></span></span></figure>
</section>
<section id="conditional-anime-faces-arfafax" class="level2">
<h2><a href="https://www.gwern.net/Faces#conditional-anime-faces-arfafax" title="Link to section: § &#39;Conditional Anime Faces, Arfafax&#39;" class="no-popup cyxy-trs-source">Conditional Anime Faces, Arfafax</a></h2>
<p class="cyxy-trs-source">In March 2020, <a href="https://nitter.cc/arfafax/status/1348052573106757636" class="no-popup">Ar­fafax trained</a> a con­di­tional anime face Style<span class="smallcaps-auto">GAN</span>: it takes a list of tags (a sub­set of Dan­booru2019 tags rel­e­vant to faces), processes them via <a href="https://arxiv.org/abs/1405.4053#google" id="le-mikolov-2014-5" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Distributed Representations of Sentences and Documents&#39;, Le &amp; Mikolov 2014">doc2vec</a> into a fixed-size in­put, and feeds them into a con­di­tional Style<span class="smallcaps-auto">GAN</span>.<a href="https://www.gwern.net/Faces#sn31" class="footnote-ref spawns-popup" id="fnref31" role="doc-noteref"><sup>31</sup></a> (While al­most all uses of Style<span class="smallcaps-auto">GAN</span> are un­con­di­tion­al—you just dump in a big pile of im­ages and it learns to gen­er­ate ran­dom sam­ples—the code ac­tu­ally sup­ports con­di­tional use where a cat­e­gory or set of vari­ables is turned into an out­put im­age, and a few other peo­ple have ex­per­i­mented with it.)</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Sam­ple gen­er­ated for the tags: [&#39;0:white_hair&#39;, &#39;0:blue_eyes&#39;, &#39;0:long_hair&#39;]" height="512" loading="lazy" src="./一个StyleGAN动漫脸编辑详细教程_files/arfafax-conditionalstylegan-animefaces-blueeyeswhitehair.webp" width="512" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">Sam­ple gen­er­ated for the tags: <code>['0:white_hair', '0:blue_eyes', '0:long_hair']</code></figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Tags: [&#39;0:blonde_hair&#39;, &#39;0:blue_eyes&#39;, &#39;0:long_hair&#39;]" height="512" loading="lazy" src="./一个StyleGAN动漫脸编辑详细教程_files/arfafax-conditionalstylegan-animefaces-blueeyesblondehair.webp" width="512" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">Tags: <code>['0:blonde_hair', '0:blue_eyes', '0:long_hair']</code></figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Tags: [&#39;4:hirasawa_yui&#39;]" height="512" loading="lazy" src="./一个StyleGAN动漫脸编辑详细教程_files/arfafax-conditionalstylegan-animefaces-hirasawayui.webp" width="512" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">Tags: <code>['4:hirasawa_yui']</code></figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/arfafax-conditionalstylegan-animefaces-interpolation-redeyesblondehair2greeneyesbluehair.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption>
<a href="https://www.gwern.net/images/gan/stylegan/arfafax-conditionalstylegan-animefaces-interpolation-redeyesblondehair2greeneyesbluehair.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video: from <code>['blonde_hair red_eyes long_hair blush']</code> → <code>['blue_hair green_eyes long_hair blush']</code>.</a>
</figcaption></span></span></figure>
<section id="conditional-gan-problems" class="level3">
<h3><a href="https://www.gwern.net/Faces#conditional-gan-problems" title="Link to section: § &#39;Conditional GAN Problems&#39;" class="no-popup cyxy-trs-source">Conditional GAN Problems</a></h3>
<p class="cyxy-trs-source">In the­o­ry, a con­di­tional anime face <span class="smallcaps-auto">GAN</span> would have two ma­jor ben­e­fits over the reg­u­lar kind: be­cause ad­di­tional in­for­ma­tion is sup­plied by the hu­man-writ­ten tags de­scrib­ing each dat­a­point, the model should be able to learn high­er-qual­ity faces; and be­cause the faces are gen­er­ated based on a spe­cific de­scrip­tion, one can di­rectly con­trol the out­put with­out any <a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images" class="link-self identifier-link-down has-content spawns-popup">com­plex en­cod­ing/<wbr>edit­ing tricks</a>⁠. The fi­nal model was eas­ier to con­trol, but the qual­ity was only OK.</p>
<p class="cyxy-trs-source">What went wrong? In prac­tice, Ar­fafax ran into chal­lenges with over­fit­ting, qual­i­ty, and the model seem­ing to ig­nore many tags (and fo­cus­ing in­stead on on a few di­men­sions like hair col­or); my sus­pi­cion is that he re­dis­cov­ered the same conditional-<span class="smallcaps-auto">GAN</span> is­sues that the <a href="https://arxiv.org/abs/1612.03242" id="zhang-et-al-2016-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks&#39;, Zhang et al 2016">Stack<span class="smallcaps-auto">GAN</span></a> au­thors en­coun­tered, where the tag em­bed­ding is so high­-di­men­sional that each face is effec­tively unique (un­like cat­e­gor­i­cal con­di­tion­ing where there will be hun­dreds or thou­sands of other im­ages with the same cat­e­gory la­bel), lead­ing to Dis­crim­i­na­tor mem­o­riza­tion &amp; train­ing col­lapse. (Stack<span class="smallcaps-auto">GAN</span>’s rem­edy was <a href="https://www.gwern.net/docs/www/arxiv.org/967f7bc4f80872e15f847d3e1e02d6d1e2873abb.pdf#page=3" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1612.03242.pdf#page=3" data-attribute-title="StackGAN: Section 3.2. Conditioning Augmentation (Original URL: https://arxiv.org/pdf/1612.03242.pdf#page=3 )">“con­di­tion­ing aug­men­ta­tion”</a> : reg­u­lar­iz­ing D’s use of the em­bed­ding by adding ran­dom Gauss­ian noise to each use of an em­bed­ding, and this is a trick which has sur­faced re­peat­edly in con­di­tional <span class="smallcaps-auto">GAN</span>s since, such as <a href="https://www.gwern.net/docs/www/arxiv.org/b582b084b497e9c35ecc488629d27724f7fba5c7.pdf#page=3" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2005.04909.pdf#page=3" data-attribute-title="&#39;Conditional Image Generation and Manipulation for User-Specified Content&#39;, Stap et al 2020 (Original URL: https://arxiv.org/pdf/2005.04909.pdf#page=3 )">textStyle<span class="smallcaps-auto">GAN</span></a> so it seems no one has a bet­ter idea how to fix it. The data aug­men­ta­tion <span class="smallcaps-auto">GAN</span>s ap­pear to do this sort of reg­u­lar­iza­tion im­plic­itly by mod­i­fy­ing the <em>im­ages</em> in­stead, and some­times by adding <a href="https://www.gwern.net/docs/www/arxiv.org/a7c5151cccdc7da936c9e83a8fcdc850ecd69196.pdf#page=2&amp;org=google" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2002.04724.pdf#page=2&amp;org=google" data-attribute-title="&#39;Improved Consistency Regularization for GANs: 2.1 Balanced Consistency Regularization (bCR)&#39;, Zhao et al 2020 (Original URL: https://arxiv.org/pdf/2002.04724.pdf#page=2&amp;org=google )">a ‘con­sis­tency loss’ on <em>z</em></a>⁠, which re­quires nois­ing it. Also of in­ter­est is <a href="https://www.gwern.net/docs/www/cdn.openai.com/6642adae6833929825dbd3c8953379973ed86c9f.pdf#page=4" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf#page=4" data-attribute-title="(Original URL: https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf#page=4 )"><span class="smallcaps-auto">CLIP</span>’s ob­ser­va­tion</a> that train­ing by pre­dict­ing the ex­act text of a cap­tion from the im­age did­n’t work well, and it was much bet­ter to in­stead do <a href="https://arxiv.org/abs/2010.05113" id="lekhac-et-al-2020-0" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Contrastive Representation Learning: A Framework and Review&#39;, Le-Khac et al 2020">con­trastive learn­ing</a> within a mini­batch: in effect, in­stead of try­ing to pre­dict <em>the</em> cap­tion text for <em>a</em> im­age, try­ing to pre­dict <em>which</em> of the im­ages in­side the mini­batch is most likely to match <em>which</em> of the cap­tion texts com­pared to the oth­er­s.)</p>
<p class="cyxy-trs-source">To test this the­o­ry, Ar­fafax ran an ad­di­tional Style<span class="smallcaps-auto">GAN</span> ex­per­i­ment (<a href="https://colab.research.google.com/drive/1WLU1dIWJ4YeNlMk3Jz9q-1dhLfL23-r-" class="no-popup">Co­lab note­book</a>), us­ing the <a href="https://google.github.io/cartoonset/" id="royer-et-al-2018" class="docMetadata has-annotation spawns-popup">“Car­toon Set”</a> 100K dataset: a col­lec­tion of sim­ple emo­ji-like car­toon faces which were de­fined us­ing 18 (10 vs 4 vs 4 cat­e­gories = 10<sup>13</sup> unique faces) vari­ables &amp; a large sub­set gen­er­at­ed. Be­cause the dataset is syn­thet­ic, the vari­ables can be en­coded both as cat­e­gor­i­cal vari­ables and as doc2vec em­bed­dings, the set of vari­ables should uniquely spec­ify each im­age, and it is easy to de­ter­mine the amount of mod­e-drop­ping by look­ing. Ar­fafax com­pared the stan­dard un­con­di­tional Style<span class="smallcaps-auto">GAN</span>, a mul­ti­-cat­e­gor­i­cal em­bed­ding Style<span class="smallcaps-auto">GAN</span>, and a doc2vec Style<span class="smallcaps-auto">GAN</span>. The un­con­di­tional did fine as usu­al, the cat­e­gor­i­cal one did some­what poor­ly, and the doc2vec one col­lapsed bad­ly—­fail­ing to gen­er­ate en­tire swaths of Car­toon Set face-space. So the prob­lem does ap­pear to be the em­bed­ding.</p>
</section>
<section id="tag-face-usage" class="level3">
<h3><a href="https://www.gwern.net/Faces#tag-face-usage" title="Link to section: § &#39;Tag → Face Usage&#39;" class="no-popup cyxy-trs-source">Tag → Face Usage</a></h3>
<p class="cyxy-trs-source">Ar­fafax has pro­vided a <a href="https://colab.research.google.com/drive/1_k3TD-wKvJmYXNSU8Iz027P-ryx9nWHJ" title="Tag-Based Anime Generation: This model uses doc2vec embeddings of danbooru tags, combined with a conditional StyleGAN2 model, to generate anime characters based on tag inputs." class="no-popup">Google Co­lab note­book</a> (<a href="https://github.com/arfafax/StyleGAN2_experiments/blob/master/Preprocess%20Danbooru%20Vectors%20-%20StyleGAN%20Conditional.ipynb" class="no-popup">code</a>) for gen­er­at­ing anime faces from tag in­puts and for gen­er­at­ing in­ter­po­la­tions.</p>
<p class="cyxy-trs-source">Com­plete sup­ported tag list (ex­clud­ing in­di­vid­ual char­ac­ters, which are tagged like <code>miyu_(vampire_princess_miyu)</code> or <code>morrigan_aensland</code>): <!-- {[ --></p>
<div class="sourceCode collapse" id="cb27"><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into."><pre class="sourceCode Python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> face_tags
[<span class="st">'angry'</span>, <span class="st">'anger_vein'</span>, <span class="st">'annoyed'</span>, <span class="st">'clenched_teeth'</span>, <span class="st">'blush'</span>, <span class="st">'blush_stickers'</span>, <span class="st">'embarrassed'</span>, <span class="st">'bored'</span>,
    <span class="st">'closed_eyes'</span>, <span class="st">'confused'</span>, <span class="st">'crazy'</span>, <span class="st">'disdain'</span>, <span class="st">'disgust'</span>, <span class="st">'drunk'</span>, <span class="st">'envy'</span>, <span class="st">'expressionless'</span>, <span class="st">'evil'</span>, <span class="st">'facepalm'</span>,
    <span class="st">'flustered'</span>, <span class="st">'frustrated'</span>, <span class="st">'grimace'</span>, <span class="st">'guilt'</span>, <span class="st">'happy'</span>, <span class="st">'kubrick_stare'</span>, <span class="st">'lonely'</span>, <span class="st">'nervous'</span>, <span class="st">'nosebleed'</span>,
    <span class="st">'one_eye_closed'</span>, <span class="st">'open_mouth'</span>, <span class="st">'closed_mouth'</span>, <span class="st">'parted_lips'</span>, <span class="st">'pain'</span>, <span class="st">'pout'</span>, <span class="st">'raised_eyebrow'</span>, <span class="st">'rape_face'</span>,
    <span class="st">'rolling_eyes'</span>, <span class="st">'sad'</span>, <span class="st">'depressed'</span>, <span class="st">'frown'</span>, <span class="st">'gloom_(expression)'</span>, <span class="st">'tears'</span>, <span class="st">'horns'</span>, <span class="st">'scared'</span>, <span class="st">'panicking'</span>,
    <span class="st">'worried'</span>, <span class="st">'serious'</span>, <span class="st">'sigh'</span>, <span class="st">'sleepy'</span>, <span class="st">'tired'</span>, <span class="st">'sulking'</span>, <span class="st">'thinking'</span>, <span class="st">'pensive'</span>, <span class="st">'wince'</span>, <span class="st">'afterglow'</span>,
    <span class="st">'ahegao'</span>, <span class="st">'fucked_silly'</span>, <span class="st">'naughty_face'</span>, <span class="st">'torogao'</span>, <span class="st">'smile'</span>, <span class="st">'crazy_smile'</span>, <span class="st">'evil_smile'</span>, <span class="st">'fingersmile'</span>,
    <span class="st">'forced_smile'</span>, <span class="st">'glasgow_smile'</span>, <span class="st">'grin'</span>, <span class="st">'fang'</span>, <span class="st">'evil_grin'</span>, <span class="st">'light_smile'</span>, <span class="st">'sad_smile'</span>, <span class="st">'seductive_smile'</span>,
    <span class="st">'stifled_laugh'</span>, <span class="st">'smug'</span>, <span class="st">'doyagao'</span>, <span class="st">'smirk'</span>, <span class="st">'smug'</span>, <span class="st">'troll_face'</span>, <span class="st">'surprised'</span>, <span class="st">'scared'</span>, <span class="st">'/</span><span class="ch">\\</span><span class="st">/</span><span class="ch">\\</span><span class="st">/</span><span class="ch">\\</span><span class="st">'</span>,
    <span class="st">'color_drain'</span>, <span class="st">'horror_(expression)'</span>, <span class="st">'screaming'</span>, <span class="st">'turn_pale'</span>, <span class="st">'trembling'</span>, <span class="st">'wavy_mouth'</span>, <span class="st">';)'</span>, <span class="st">':d'</span>,
    <span class="st">';d'</span>, <span class="st">'xd'</span>, <span class="st">'d:'</span>, <span class="st">':}'</span>, <span class="st">':{'</span>, <span class="st">':3'</span>, <span class="st">';3'</span>, <span class="st">'x3'</span>, <span class="st">'3:'</span>, <span class="st">'uwu'</span>, <span class="st">'=.w.='</span>, <span class="st">':p'</span>, <span class="st">';p'</span>, <span class="st">':q'</span>, <span class="st">';q'</span>, <span class="st">'&gt;:)'</span>, <span class="st">'&gt;:('</span>,
    <span class="st">':t'</span>, <span class="st">':i'</span>, <span class="st">':/'</span>, <span class="st">':x'</span>, <span class="st">':c'</span>, <span class="st">'c:'</span>, <span class="st">':&lt;'</span>, <span class="st">';&lt;'</span>, <span class="st">':&lt;&gt;'</span>, <span class="st">':&gt;'</span>, <span class="st">':&gt;='</span>, <span class="st">';&gt;='</span>, <span class="st">':o'</span>, <span class="st">';o'</span>, <span class="st">'='</span>, <span class="st">'=)'</span>, <span class="st">'=d'</span>,
    <span class="st">'=o'</span>, <span class="st">'=v'</span>, <span class="st">'|3'</span>, <span class="st">'|d'</span>, <span class="st">'|o'</span>, <span class="st">'o3o'</span>, <span class="st">'(-3-)'</span>, <span class="st">'&gt;3&lt;'</span>, <span class="st">'o_o'</span>, <span class="st">'0_0'</span>, <span class="st">'._.'</span>, <span class="st">'•_•'</span>, <span class="st">'solid_circle_eyes'</span>,
    <span class="st">'♥_♥'</span>, <span class="st">'heart_eyes'</span>, <span class="st">'^_^'</span>, <span class="st">'^o^'</span>, <span class="st">'</span><span class="ch">\\</span><span class="st">(^o^)/'</span>, <span class="st">'└(^o^)┐≡'</span>, <span class="st">'^q^'</span>, <span class="st">'&gt;_&lt;'</span>, <span class="st">'xd'</span>, <span class="st">'x3'</span>, <span class="st">'&gt;o&lt;'</span>, <span class="st">'&lt;_&gt;'</span>, <span class="st">';_;'</span>,
    <span class="st">'@_@'</span>, <span class="st">'&gt;_@'</span>, <span class="st">'+_+'</span>, <span class="st">'+_-'</span>, <span class="st">'-_-'</span>, <span class="st">'</span><span class="ch">\\</span><span class="st">_/'</span>, <span class="st">'=_='</span>, <span class="st">'=^='</span>, <span class="st">'=v='</span>, <span class="st">'&lt;o&gt;_&lt;o&gt;'</span>, <span class="st">'constricted_pupils'</span>, <span class="st">'cross_eyed'</span>,
    <span class="st">'rectangular_mouth'</span>, <span class="st">'sideways_mouth'</span>, <span class="st">'no_nose'</span>, <span class="st">'no_mouth'</span>, <span class="st">'wavy_mouth'</span>, <span class="st">'wide-eyed'</span>, <span class="st">'mouth_drool'</span>,
    <span class="st">'awesome_face'</span>, <span class="st">'foodgasm'</span>, <span class="st">'henohenomoheji'</span>, <span class="st">'nonowa'</span>, <span class="st">'portrait'</span>, <span class="st">'profile'</span>, <span class="st">'smiley_face'</span>, <span class="st">'uso_da'</span>,
    <span class="st">'food_awe'</span>, <span class="st">'breast_awe'</span>, <span class="st">'penis_awe'</span>]
<span class="op">&gt;&gt;&gt;</span> eye_tags
[<span class="st">'aqua_eyes'</span>, <span class="st">'black_eyes'</span>, <span class="st">'blue_eyes'</span>, <span class="st">'brown_eyes'</span>, <span class="st">'green_eyes'</span>, <span class="st">'grey_eyes'</span>, <span class="st">'orange_eyes'</span>, <span class="st">'lavender_eyes'</span>,
    <span class="st">'pink_eyes'</span>, <span class="st">'purple_eyes'</span>, <span class="st">'red_eyes'</span>, <span class="st">'silver_eyes'</span>, <span class="st">'white_eyes'</span>, <span class="st">'yellow_eyes'</span>, <span class="st">'heterochromia'</span>, <span class="st">'multicolored_eyes'</span>,
    <span class="st">'al_bhed_eyes'</span>, <span class="st">'pac-man_eyes'</span>, <span class="st">'ringed_eyes'</span>, <span class="st">'constricted_pupils'</span>, <span class="st">'dilated_pupils'</span>, <span class="st">'horizontal_pupils'</span>,
    <span class="st">'no_pupils'</span>, <span class="st">'slit_pupils'</span>, <span class="st">'symbol-shaped_pupils'</span>, <span class="st">'+_+'</span>, <span class="st">'heart-shaped_pupils'</span>, <span class="st">'star-shaped_pupils'</span>,
    <span class="st">'blue_sclera'</span>, <span class="st">'black_sclera'</span>, <span class="st">'blank_eyes'</span>, <span class="st">'bloodshot_eyes'</span>, <span class="st">'green_sclera'</span>, <span class="st">'mismatched_sclera'</span>, <span class="st">'orange_sclera'</span>,
    <span class="st">'red_sclera'</span>, <span class="st">'yellow_sclera'</span>, <span class="st">'bags_under_eyes'</span>, <span class="st">'bruised_eye'</span>, <span class="st">'flaming_eyes'</span>, <span class="st">'glowing_eyes'</span>, <span class="st">'glowing_eye'</span>,
    <span class="st">'mako_eyes'</span>, <span class="st">'amphibian_eyes'</span>, <span class="st">'button_eyes'</span>, <span class="st">'cephalopod_eyes'</span>, <span class="st">'compound_eyes'</span>, <span class="st">'frog_eyes'</span>, <span class="st">'crazy_eyes'</span>,
    <span class="st">'empty_eyes'</span>, <span class="st">'heart_eyes'</span>, <span class="st">'nonowa'</span>, <span class="st">'solid_circle_eyes'</span>, <span class="st">'o_o'</span>, <span class="st">'0_0'</span>, <span class="st">'jitome'</span>, <span class="st">'tareme'</span>, <span class="st">'tsurime'</span>,
    <span class="st">'sanpaku'</span>, <span class="st">'sharingan'</span>, <span class="st">'mangekyou_sharingan'</span>, <span class="st">'eye_reflection'</span>, <span class="st">'text_in_eyes'</span>, <span class="st">'missing_eye'</span>, <span class="st">'one-eyed'</span>,
    <span class="st">'third_eye'</span>, <span class="st">'extra_eyes'</span>, <span class="st">'no_eyes'</span>]
<span class="op">&gt;&gt;&gt;</span> eye_expressions
[<span class="st">'&gt;_&lt;'</span>, <span class="st">'x3'</span>, <span class="st">'xd'</span>, <span class="st">'o_o'</span>, <span class="st">'0_0'</span>, <span class="st">'3_3'</span>, <span class="st">'6_9'</span>, <span class="st">'@_@'</span>, <span class="st">'^_^'</span>, <span class="st">'^o^'</span>, <span class="st">'9848'</span>, <span class="st">'26237'</span>, <span class="st">'=_='</span>, <span class="st">'+_+'</span>, <span class="st">'._.'</span>,
    <span class="st">'&lt;o&gt;_&lt;o&gt;'</span>, <span class="st">'blinking'</span>, <span class="st">'closed_eyes'</span>, <span class="st">'wince'</span>, <span class="st">'one_eye_closed'</span>, <span class="st">';&lt;'</span>, <span class="st">';&gt;'</span>, <span class="st">';p'</span>]
<span class="op">&gt;&gt;&gt;</span> eye_other
[<span class="st">'covering_eyes'</span>, <span class="st">'hair_over_eyes'</span>, <span class="st">'hair_over_one_eye'</span>, <span class="st">'bandage_over_one_eye'</span>, <span class="st">'blindfold'</span>, <span class="st">'hat_over_eyes'</span>,
    <span class="st">'eyepatch'</span>, <span class="st">'eyelashes'</span>, <span class="st">'colored_eyelashes'</span>, <span class="st">'fake_eyelashes'</span>, <span class="st">'eyes_visible_through_hair'</span>, <span class="st">'glasses'</span>,
    <span class="st">'makeup'</span>, <span class="st">'eyeliner'</span>, <span class="st">'eyeshadow'</span>, <span class="st">'mascara'</span>, <span class="st">'eye_contact'</span>, <span class="st">'looking_afar'</span>, <span class="st">'looking_at_another'</span>, <span class="st">'looking_at_breasts'</span>,
    <span class="st">'looking_at_hand'</span>, <span class="st">'looking_at_mirror'</span>, <span class="st">'looking_at_phone'</span>, <span class="st">'looking_at_viewer'</span>, <span class="st">'looking_away'</span>, <span class="st">'looking_back'</span>,
    <span class="st">'looking_down'</span>, <span class="st">'looking_out_window'</span>, <span class="st">'looking_over_glasses'</span>, <span class="st">'looking_through_legs'</span>, <span class="st">'looking_to_the_side'</span>,
    <span class="st">'looking_up'</span>, <span class="st">'akanbe'</span>, <span class="st">'blind'</span>, <span class="st">'cross-eyed'</span>, <span class="st">'drawn_on_eyes'</span>, <span class="st">'eyeball'</span>, <span class="st">'eye_beam'</span>, <span class="st">'eye_poke'</span>, <span class="st">'eye_pop'</span>,
    <span class="st">'persona_eyes'</span>, <span class="st">'shading_eyes'</span>, <span class="st">'squinting'</span>, <span class="st">'staring'</span>, <span class="st">'uneven_eyes'</span>, <span class="st">'upturned_eyes'</span>, <span class="st">'wall-eyed'</span>, <span class="st">'wide-eyed'</span>, <span class="st">'wince'</span>]
<span class="op">&gt;&gt;&gt;</span> ears_tags
[<span class="st">'animal_ears'</span>, <span class="st">'bear_ears'</span>, <span class="st">'bunny_ears'</span>, <span class="st">'cat_ears'</span>, <span class="st">'dog_ears'</span>, <span class="st">'fake_animal_ears'</span>, <span class="st">'fox_ears'</span>, <span class="st">'horse_ears'</span>,
    <span class="st">'kemonomimi_mode'</span>, <span class="st">'lion_ears'</span>, <span class="st">'monkey_ears'</span>, <span class="st">'mouse_ears'</span>, <span class="st">'raccoon_ears'</span>, <span class="st">'sheep_ears'</span>, <span class="st">'tiger_ears'</span>,
    <span class="st">'wolf_ears'</span>, <span class="st">'pointy_ears'</span>, <span class="st">'robot_ears'</span>, <span class="st">'extra_ears'</span>, <span class="st">'ear_piercing'</span>, <span class="st">'ear_protection'</span>, <span class="st">'earrings'</span>,
    <span class="st">'single_earring'</span>, <span class="st">'headphones'</span>, <span class="st">'covering_ears'</span>, <span class="st">'ear_biting'</span>, <span class="st">'ear_licking'</span>, <span class="st">'ear_grab'</span>]
<span class="op">&gt;&gt;&gt;</span> hair_tags
[<span class="st">'heartbreak_haircut'</span>, <span class="st">'hand_in_hair'</span>, <span class="st">'adjusting_hair'</span>, <span class="st">'bunching_hair'</span>, <span class="st">'hair_flip'</span>, <span class="st">'hair_grab'</span>, <span class="st">'hair_pull'</span>,
    <span class="st">'hair_tucking'</span>, <span class="st">'hair_tousle'</span>, <span class="st">'hair_twirling'</span>, <span class="st">'hair_sex'</span>, <span class="st">'hair_brush'</span>, <span class="st">'hair_dryer'</span>, <span class="st">'shampoo'</span>, <span class="st">'bun_cover'</span>,
    <span class="st">'hairpods'</span>, <span class="st">'chopsticks'</span>, <span class="st">'comb'</span>, <span class="st">'hair_ornament'</span>, <span class="st">'hair_bell'</span>, <span class="st">'hair_bobbles'</span>, <span class="st">'hair_bow'</span>, <span class="st">'hair_ribbon'</span>,
    <span class="st">'hairclip'</span>, <span class="st">'hairpin'</span>, <span class="st">'hair_flower'</span>, <span class="st">'hair_tubes'</span>, <span class="st">'kanzashi'</span>, <span class="st">'hair_tie'</span>, <span class="st">'hairband'</span>, <span class="st">'hair_weapon'</span>,
    <span class="st">'headband'</span>, <span class="st">'scrunchie'</span>, <span class="st">'wig'</span>, <span class="st">'facial_hair'</span>, <span class="st">'beard'</span>, <span class="st">'bearded_girl'</span>, <span class="st">'goatee'</span>, <span class="st">'mustache'</span>, <span class="st">'fake_mustache'</span>,
    <span class="st">'stubble'</span>, <span class="st">'fiery_hair'</span>, <span class="st">'prehensile_hair'</span>, <span class="st">'helicopter_hair'</span>, <span class="st">'tentacle_hair'</span>, <span class="st">'living_hair'</span>, <span class="st">'detached_hair'</span>,
    <span class="st">'severed_hair'</span>, <span class="st">'floating_hair'</span>, <span class="st">'hair_spread_out'</span>, <span class="st">'wet_hair'</span>]
<span class="op">&gt;&gt;&gt;</span> hair_color_tags
[<span class="st">'aqua_hair'</span>, <span class="st">'black_hair'</span>, <span class="st">'blonde_hair'</span>, <span class="st">'blue_hair'</span>, <span class="st">'light_blue_hair'</span>, <span class="st">'brown_hair'</span>, <span class="st">'light_brown_hair'</span>,
    <span class="st">'green_hair'</span>, <span class="st">'grey_hair'</span>, <span class="st">'magenta_hair'</span>, <span class="st">'orange_hair'</span>, <span class="st">'pink_hair'</span>, <span class="st">'purple_hair'</span>, <span class="st">'lavender_hair'</span>,
    <span class="st">'red_hair'</span>, <span class="st">'auburn_hair'</span>, <span class="st">'maroon_hair'</span>, <span class="st">'silver_hair'</span>, <span class="st">'white_hair'</span>, <span class="st">'multicolored_hair'</span>, <span class="st">'colored_inner_hair'</span>,
    <span class="st">'gradient_hair'</span>, <span class="st">'rainbow_hair'</span>, <span class="st">'streaked_hair'</span>, <span class="st">'two-tone_hair'</span>, <span class="st">'highlights'</span>, <span class="st">'colored_tips'</span>, <span class="st">'alternate_hair_color'</span>]
<span class="op">&gt;&gt;&gt;</span> hair_style_tags
[<span class="st">'very_short_hair'</span>, <span class="st">'short_hair'</span>, <span class="st">'medium_hair'</span>, <span class="st">'long_hair'</span>, <span class="st">'very_long_hair'</span>, <span class="st">'absurdly_long_hair'</span>,
    <span class="st">'big_hair'</span>, <span class="st">'bald'</span>, <span class="st">'bald_girl'</span>, <span class="st">'alternate_hairstyle'</span>, <span class="st">'hair_down'</span>, <span class="st">'hair_up'</span>, <span class="st">'curly_hair'</span>, <span class="st">'drill_hair'</span>,
    <span class="st">'twin_drills'</span>, <span class="st">'flipped_hair'</span>, <span class="st">'hair_flaps'</span>, <span class="st">'messy_hair'</span>, <span class="st">'pointy_hair'</span>, <span class="st">'ringlets'</span>, <span class="st">'spiked_hair'</span>, <span class="st">'wavy_hair'</span>,
    <span class="st">'bangs'</span>, <span class="st">'asymmetrical_bangs'</span>, <span class="st">'blunt_bangs'</span>, <span class="st">'hair_over_eyes'</span>, <span class="st">'hair_over_one_eye'</span>, <span class="st">'parted_bangs'</span>, <span class="st">'swept_bangs'</span>,
    <span class="st">'hair_between_eyes'</span>, <span class="st">'hair_intakes'</span>, <span class="st">'sidelocks'</span>, <span class="st">"widow's_peak"</span>, <span class="st">'ahoge'</span>, <span class="st">'heart_ahoge'</span>, <span class="st">'huge_ahoge'</span>,
    <span class="st">'antenna_hair'</span>, <span class="st">'comb_over'</span>, <span class="st">'hair_pulled_back'</span>, <span class="st">'hair_slicked_back'</span>, <span class="st">'mohawk'</span>, <span class="st">'hair_bikini'</span>, <span class="st">'hair_censor'</span>,
    <span class="st">'hair_in_mouth'</span>, <span class="st">'hair_over_breasts'</span>, <span class="st">'hair_over_one_breast'</span>, <span class="st">'hair_over_crotch'</span>, <span class="st">'hair_over_shoulder'</span>,
    <span class="st">'hair_scarf'</span>, <span class="st">'bow_by_hair'</span>, <span class="st">'braid'</span>, <span class="st">'braided_bangs'</span>, <span class="st">'front_braid'</span>, <span class="st">'side_braid'</span>, <span class="st">'french_braid'</span>, <span class="st">'crown_braid'</span>,
    <span class="st">'single_braid'</span>, <span class="st">'multiple_braids'</span>, <span class="st">'twin_braids'</span>, <span class="st">'tri_braids'</span>, <span class="st">'quad_braids'</span>, <span class="st">'hair_bun'</span>, <span class="st">'braided_bun'</span>,
    <span class="st">'double_bun'</span>, <span class="st">'triple_bun'</span>, <span class="st">'hair_rings'</span>, <span class="st">'half_updo'</span>, <span class="st">'one_side_up'</span>, <span class="st">'two_side_up'</span>, <span class="st">'low-braided_long_hair'</span>,
    <span class="st">'low-tied_long_hair'</span>, <span class="st">'mizura'</span>, <span class="st">'multi-tied_hair'</span>, <span class="st">'nihongami'</span>, <span class="st">'ponytail'</span>, <span class="st">'folded_ponytail'</span>, <span class="st">'front_ponytail'</span>,
    <span class="st">'high_ponytail'</span>, <span class="st">'short_ponytail'</span>, <span class="st">'side_ponytail'</span>, <span class="st">'split_ponytail'</span>, <span class="st">'topknot'</span>, <span class="st">'twintails'</span>, <span class="st">'low_twintails'</span>,
    <span class="st">'short_twintails'</span>, <span class="st">'uneven_twintails'</span>, <span class="st">'tri_tails'</span>, <span class="st">'quad_tails'</span>, <span class="st">'quin_tails'</span>, <span class="st">'bob_cut'</span>, <span class="st">'bowl_cut'</span>,
    <span class="st">'buzz_cut'</span>, <span class="st">'chonmage'</span>, <span class="st">'crew_cut'</span>, <span class="st">'flattop'</span>, <span class="st">'pixie_cut'</span>, <span class="st">'undercut'</span>, <span class="st">'cornrows'</span>, <span class="st">'hairlocs'</span>, <span class="st">'hime_cut'</span>,
    <span class="st">'mullet'</span>, <span class="st">'afro'</span>, <span class="st">'huge_afro'</span>, <span class="st">'beehive_hairdo'</span>, <span class="st">'pompadour'</span>, <span class="st">'quiff'</span>, <span class="st">'shouten_pegasus_mix_mori'</span>]
<span class="op">&gt;&gt;&gt;</span> skin_color_tags
[<span class="st">'dark_skin'</span>, <span class="st">'pale_skin'</span>, <span class="st">'tan'</span>, <span class="st">'tanlines'</span>, <span class="st">'sun_tattoo'</span>, <span class="st">'black_skin'</span>, <span class="st">'blue_skin'</span>, <span class="st">'green_skin'</span>, <span class="st">'grey_skin'</span>,
    <span class="st">'orange_skin'</span>, <span class="st">'pink_skin'</span>, <span class="st">'purple_skin'</span>, <span class="st">'red_skin'</span>, <span class="st">'white_skin'</span>, <span class="st">'yellow_skin'</span>, <span class="st">'shiny_skin'</span>]
<span class="op">&gt;&gt;&gt;</span> headwear_tags
[<span class="st">'crown'</span>, <span class="st">'hat'</span>, <span class="st">'helmet'</span>, <span class="st">'black_headwear'</span>, <span class="st">'blue_headwear'</span>, <span class="st">'brown_headwear'</span>, <span class="st">'green_headwear'</span>, <span class="st">'grey_headwear'</span>,
    <span class="st">'orange_headwear'</span>, <span class="st">'pink_headwear'</span>, <span class="st">'purple_headwear'</span>, <span class="st">'red_headwear'</span>, <span class="st">'white_headwear'</span>, <span class="st">'yellow_headwear'</span>,
    <span class="st">'ajirogasa'</span>, <span class="st">'animal_hat'</span>, <span class="st">'cat_hat'</span>, <span class="st">'penguin_hat'</span>, <span class="st">'baseball_cap'</span>, <span class="st">'beanie'</span>, <span class="st">'beret'</span>, <span class="st">'bicorne'</span>, <span class="st">'boater_hat'</span>,
    <span class="st">'bowl_hat'</span>, <span class="st">'bowler_hat'</span>, <span class="st">'bucket_hat'</span>, <span class="st">'cabbie_hat'</span>, <span class="st">'chef_hat'</span>, <span class="st">'toque_blanche'</span>, <span class="st">'flat_top_chef_hat'</span>,
    <span class="st">'cloche_hat'</span>, <span class="st">'cowboy_hat'</span>, <span class="st">'deerstalker'</span>, <span class="st">'deviruchi_hat'</span>, <span class="st">'dixie_cup_hat'</span>, <span class="st">'eggshell_hat'</span>, <span class="st">'fedora'</span>,
    <span class="st">'female_service_cap'</span>, <span class="st">'flat_cap'</span>, <span class="st">'fur_hat'</span>, <span class="st">'garrison_cap'</span>, <span class="st">'jester_cap'</span>, <span class="st">'kepi'</span>, <span class="st">'mian_guan'</span>, <span class="st">'mitre'</span>,
    <span class="st">'mob_cap'</span>, <span class="st">'mortarboard'</span>, <span class="st">'nightcap'</span>, <span class="st">'nurse_cap'</span>, <span class="st">'party_hat'</span>, <span class="st">'peaked_cap'</span>, <span class="st">'pillow_hat'</span>, <span class="st">'pirate_hat'</span>,
    <span class="st">'porkpie_hat'</span>, <span class="st">'pumpkin_hat'</span>, <span class="st">'rice_hat'</span>, <span class="st">'robe_and_wizard_hat'</span>, <span class="st">'sailor_hat'</span>, <span class="st">'santa_hat'</span>, <span class="st">'mini_santa_hat'</span>,
    <span class="st">'shako_cap'</span>, <span class="st">'shampoo_hat'</span>, <span class="st">'sombrero'</span>, <span class="st">'sun_hat'</span>, <span class="st">"tam_o'_shanter"</span>, <span class="st">'tate_eboshi'</span>, <span class="st">'tokin_hat'</span>, <span class="st">'top_hat'</span>,
    <span class="st">'mini_top_hat'</span>, <span class="st">'tricorne'</span>, <span class="st">'ushanka'</span>, <span class="st">'witch_hat'</span>, <span class="st">'mini_witch_hat'</span>, <span class="st">'wizard_hat'</span>, <span class="st">'veil'</span>, <span class="st">'zun_hat'</span>,
    <span class="st">'baseball_helmet'</span>, <span class="st">'bicycle_helmet'</span>, <span class="st">'brodie_helmet'</span>, <span class="st">'diving_helmet'</span>, <span class="st">'football_helmet'</span>, <span class="st">'hardhat'</span>, <span class="st">'horned_helmet'</span>,
    <span class="st">'helm'</span>, <span class="st">'kabuto'</span>, <span class="st">'motorcycle_helmet'</span>, <span class="st">'pickelhaube'</span>, <span class="st">'pith_helmet'</span>, <span class="st">'stahlhelm'</span>, <span class="st">'tank_helmet'</span>, <span class="st">'winged_helmet'</span>,
    <span class="st">'circlet'</span>, <span class="st">'diadem'</span>, <span class="st">'mini_crown'</span>, <span class="st">'saishi'</span>, <span class="st">'tiara'</span>, <span class="st">'aviator_cap'</span>, <span class="st">'bandana'</span>, <span class="st">'bonnet'</span>, <span class="st">'dalachi_(headdress)'</span>,
    <span class="st">'habit'</span>, <span class="st">'hijab'</span>, <span class="st">'keffiyeh'</span>, <span class="st">'shower_cap'</span>, <span class="st">'visor_cap'</span>, <span class="st">'checkered_hat'</span>, <span class="st">'frilled_hat'</span>, <span class="st">'military_hat'</span>,
    <span class="st">'mini_hat'</span>, <span class="st">'multicolored_hat'</span>, <span class="st">'police_hat'</span>, <span class="st">'print_hat'</span>, <span class="st">'school_hat'</span>, <span class="st">'straw_hat'</span>, <span class="st">'adjusting_hat'</span>,
    <span class="st">'hand_on_headwear'</span>, <span class="st">'hands_on_headwear'</span>, <span class="st">'hat_basket'</span>, <span class="st">'hat_loss'</span>, <span class="st">'hat_on_chest'</span>, <span class="st">'hat_over_eyes'</span>, <span class="st">'hat_over_one_eye'</span>,
    <span class="st">'hat_removed'</span>, <span class="st">'hat_tip'</span>, <span class="st">'holding_hat'</span>, <span class="st">'torn_hat'</span>, <span class="st">'no_hat'</span>, <span class="st">'hat_bow'</span>, <span class="st">'hat_feather'</span>, <span class="st">'hat_flower'</span>,
    <span class="st">'hat_ribbon'</span>, <span class="st">'hat_with_ears'</span>, <span class="st">'adjusting_hat'</span>, <span class="st">'backwards_hat'</span>, <span class="st">'hat_removed'</span>, <span class="st">'holding_hat'</span>, <span class="st">'torn_hat'</span>,
    <span class="st">'hair_bow'</span>, <span class="st">'hair_ribbon'</span>, <span class="st">'hairband'</span>, <span class="st">'headband'</span>, <span class="st">'forehead_protector'</span>, <span class="st">'sweatband'</span>, <span class="st">'hachimaki'</span>, <span class="st">'nejiri_hachimaki'</span>,
    <span class="st">'mongkhon'</span>, <span class="st">'headdress'</span>, <span class="st">'maid_headdress'</span>, <span class="st">'veil'</span>, <span class="st">'hood'</span>]
<span class="op">&gt;&gt;&gt;</span> eyewear_tags
[<span class="st">'glasses'</span>, <span class="st">'monocle'</span>, <span class="st">'sunglasses'</span>, <span class="st">'aqua-framed_eyewear'</span>, <span class="st">'black-framed_eyewear'</span>, <span class="st">'blue-framed_eyewear'</span>,
    <span class="st">'brown-framed_eyewear'</span>, <span class="st">'green-framed_eyewear'</span>, <span class="st">'grey-framed_eyewear'</span>, <span class="st">'orange-framed_eyewear'</span>, <span class="st">'pink-framed_eyewear'</span>,
    <span class="st">'purple-framed_eyewear'</span>, <span class="st">'red-framed_eyewear'</span>, <span class="st">'white-framed_eyewear'</span>, <span class="st">'yellow-framed_eyewear'</span>, <span class="st">'blue-tinted_eyewear'</span>,
    <span class="st">'brown-tinted_eyewear'</span>, <span class="st">'green-tinted_eyewear'</span>, <span class="st">'orange-tinted_eyewear'</span>, <span class="st">'pink-tinted_eyewear'</span>, <span class="st">'purple-tinted_eyewear'</span>,
    <span class="st">'red-tinted_eyewear'</span>, <span class="st">'yellow-tinted_eyewear'</span>, <span class="st">'heart-shaped_eyewear'</span>, <span class="st">'round_eyewear'</span>, <span class="st">'over-rim_eyewear'</span>,
    <span class="st">'rimless_eyewear'</span>, <span class="st">'semi-rimless_eyewear'</span>, <span class="st">'under-rim_eyewear'</span>, <span class="st">'adjusting_eyewear'</span>, <span class="st">'eyewear_on_head'</span>,
    <span class="st">'eyewear_removed'</span>, <span class="st">'eyewear_hang'</span>, <span class="st">'eyewear_in_mouth'</span>, <span class="st">'holding_eyewear'</span>, <span class="st">'eyewear_strap'</span>, <span class="st">'eyewear_switch'</span>,
    <span class="st">'looking_over_eyewear'</span>, <span class="st">'no_eyewear'</span>, <span class="st">'3d_glasses'</span>, <span class="st">'coke-bottle_glasses'</span>, <span class="st">'diving_mask'</span>, <span class="st">'fancy_glasses'</span>,
    <span class="st">'heart-shaped_eyewear'</span>, <span class="st">'funny_glasses'</span>, <span class="st">'goggles'</span>, <span class="st">'nodoka_glasses'</span>, <span class="st">'opaque_glasses'</span>, <span class="st">'pince-nez'</span>, <span class="st">'safety_glasses'</span>,
    <span class="st">'shooting_glasses'</span>, <span class="st">'ski_goggles'</span>, <span class="st">'x-ray_glasses'</span>, <span class="st">'bespectacled'</span>, <span class="st">'kamina_shades'</span>, <span class="st">'star_shades'</span>]
<span class="op">&gt;&gt;&gt;</span> piercings_tags
[<span class="st">'ear_piercing'</span>, <span class="st">'eyebrow_piercing'</span>, <span class="st">'anti-eyebrow_piercing'</span>, <span class="st">'eyelid_piercing'</span>, <span class="st">'lip_piercing'</span>, <span class="st">'labret_piercing'</span>,
    <span class="st">'nose_piercing'</span>, <span class="st">'bridge_piercing'</span>, <span class="st">'tongue_piercing'</span>]
<span class="op">&gt;&gt;&gt;</span> format_tags
[<span class="st">'3d'</span>, <span class="st">'animated'</span>, <span class="st">'animated_png'</span>, <span class="st">'flash'</span>, <span class="st">'music_video'</span>, <span class="st">'song'</span>, <span class="st">'video'</span>, <span class="st">'animated_gif'</span>, <span class="st">'non-looping_animation'</span>,
    <span class="st">'archived_file'</span>, <span class="st">'artbook'</span>, <span class="st">'bmp'</span>, <span class="st">'calendar_(medium)'</span>, <span class="st">'card_(medium)'</span>, <span class="st">'comic'</span>, <span class="st">'2koma'</span>, <span class="st">'3koma'</span>, <span class="st">'4koma'</span>,
    <span class="st">'multiple_4koma'</span>, <span class="st">'5koma'</span>, <span class="st">'borderless_panels'</span>, <span class="st">'doujinshi'</span>, <span class="st">'eromanga'</span>, <span class="st">'left-to-right_manga'</span>, <span class="st">'right-to-left_comic'</span>,
    <span class="st">'silent_comic'</span>, <span class="st">'corrupted_image'</span>, <span class="st">'cover'</span>, <span class="st">'album_cover'</span>, <span class="st">'character_single'</span>, <span class="st">'cover_page'</span>, <span class="st">'doujin_cover'</span>,
    <span class="st">'dvd_cover'</span>, <span class="st">'fake_cover'</span>, <span class="st">'game_cover'</span>, <span class="st">'magazine_cover'</span>, <span class="st">'manga_cover'</span>, <span class="st">'fake_screenshot'</span>, <span class="st">'game_cg'</span>,
    <span class="st">'gyotaku_(medium)'</span>, <span class="st">'highres'</span>, <span class="st">'absurdres'</span>, <span class="st">'incredibly_absurdres'</span>, <span class="st">'lowres'</span>, <span class="st">'thumbnail'</span>, <span class="st">'huge_filesize'</span>,
    <span class="st">'icon'</span>, <span class="st">'logo'</span>, <span class="st">'kirigami'</span>, <span class="st">'lineart'</span>, <span class="st">'no_lineart'</span>, <span class="st">'outline'</span>, <span class="st">'long_image'</span>, <span class="st">'tall_image'</span>, <span class="st">'wide_image'</span>,
    <span class="st">'mosaic_art'</span>, <span class="st">'photomosaic'</span>, <span class="st">'oekaki'</span>, <span class="st">'official_art'</span>, <span class="st">'phonecard'</span>, <span class="st">'photo'</span>, <span class="st">'papercraft'</span>, <span class="st">'paper_child'</span>,
    <span class="st">'paper_cutout'</span>, <span class="st">'pixel_art'</span>, <span class="st">'postcard'</span>, <span class="st">'poster'</span>, <span class="st">'revision'</span>, <span class="st">'bad_revision'</span>, <span class="st">'artifacted_revision'</span>,
    <span class="st">'censored_revision'</span>, <span class="st">'corrupted_revision'</span>, <span class="st">'lossy_revision'</span>, <span class="st">'watermarked_revision'</span>, <span class="st">'scan'</span>, <span class="st">'screencap'</span>,
    <span class="st">'shitajiki'</span>, <span class="st">'tegaki'</span>, <span class="st">'transparent_background'</span>, <span class="st">'triptych_(art)'</span>, <span class="st">'vector_trace'</span>, <span class="st">'wallpaper'</span>, <span class="st">'dual_monitor'</span>,
    <span class="st">'ios_wallpaper'</span>, <span class="st">'official_wallpaper'</span>, <span class="st">'phone_wallpaper'</span>, <span class="st">'psp_wallpaper'</span>, <span class="st">'tileable'</span>, <span class="st">'wallpaper_forced'</span>, <span class="st">'widescreen'</span>]
<span class="op">&gt;&gt;&gt;</span> style_tags
[<span class="st">'abstract'</span>, <span class="st">'art_deco'</span>, <span class="st">'art_nouveau'</span>, <span class="st">'fine_art_parody'</span>, <span class="st">'flame_painter'</span>, <span class="st">'impressionism'</span>, <span class="st">'nihonga'</span>,
    <span class="st">'sumi-e'</span>, <span class="st">'ukiyo-e'</span>, <span class="st">'minimalism'</span>, <span class="st">'realistic'</span>, <span class="st">'photorealistic'</span>, <span class="st">'sketch'</span>, <span class="st">'style_parody'</span>, <span class="st">'list_of_style_parodies'</span>,
    <span class="st">'surreal'</span>, <span class="st">'traditional_media'</span>, <span class="st">'faux_traditional_media'</span>, <span class="st">'work_in_progress'</span>, <span class="st">'backlighting'</span>, <span class="st">'blending'</span>,
    <span class="st">'bloom'</span>, <span class="st">'bokeh'</span>, <span class="st">'caustics'</span>, <span class="st">'chiaroscuro'</span>, <span class="st">'chromatic_aberration'</span>, <span class="st">'chromatic_aberration_abuse'</span>, <span class="st">'diffraction_spikes'</span>,
    <span class="st">'depth_of_field'</span>, <span class="st">'dithering'</span>, <span class="st">'drop_shadow'</span>, <span class="st">'emphasis_lines'</span>, <span class="st">'foreshortening'</span>, <span class="st">'gradient'</span>, <span class="st">'halftone'</span>,
    <span class="st">'lens_flare'</span>, <span class="st">'lens_flare_abuse'</span>, <span class="st">'motion_blur'</span>, <span class="st">'motion_lines'</span>, <span class="st">'multiple_monochrome'</span>, <span class="st">'optical_illusion'</span>,
    <span class="st">'anaglyph'</span>, <span class="st">'exif_thumbnail_surprise'</span>, <span class="st">'open_in_internet_explorer'</span>, <span class="st">'open_in_winamp'</span>, <span class="st">'stereogram'</span>, <span class="st">'scanlines'</span>,
    <span class="st">'silhouette'</span>, <span class="st">'speed_lines'</span>, <span class="st">'vignetting'</span>]</code></pre></div>
<p class="cyxy-trs-source">Down­load:</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source"><code>rsync --verbose --recursive rsync://78.46.86.149:873/biggan/2021-01-09-arfafax-stylegan-danbooruportraits-tagconditional.tar.xz ./</code> (all files: doc2vec, Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> mod­el, note­book, videos)</li>
</ul>
</section>
</section>
<section id="extended-stylegan2-danbooru2019-aydao" class="level2">
<h2><a href="https://www.gwern.net/Faces#extended-stylegan2-danbooru2019-aydao" title="Link to section: § &#39;Extended StyleGAN2 Danbooru2019, Aydao&#39;" class="no-popup cyxy-trs-source">Extended StyleGAN2 Danbooru2019, Aydao</a></h2>
<div id="this-anime-does-not-exist">

</div>
<div id="tadne">

</div>
<p class="cyxy-trs-source"><a href="https://aydao.ai/experiment/2020/12/05/dbstylegan512.html" class="no-popup">Ay­dao</a> (<a href="https://twitter.com/aydaoai" class="no-popup">Twit­ter</a>), in par­al­lel with <a href="https://www.gwern.net/Faces#danbooru2019e621-256px-biggan" class="link-self identifier-link-down has-content spawns-popup">the Big<span class="smallcaps-auto">GAN</span> ex­per­i­ments</a>⁠, worked on grad­u­ally ex­tend­ing Style<span class="smallcaps-auto">GAN</span>2’s mod­el­ing pow­ers to cover Dan­booru2019 <span class="smallcaps-auto">SFW</span>. His re­sults were ex­cel­lent and power <a href="https://nearcyan.com/this-anime-does-not-exist/" class="no-popup">nearcyan’s</a> (<a href="https://twitter.com/nearcyan" class="no-popup">Twit­ter</a>) <a href="https://thisanimedoesnotexist.ai/" id="nearcyan-et-al-2021" class="docMetadata has-annotation spawns-popup">This Anime Does Not Ex­ist.ai (<span class="smallcaps-auto">TADNE</span>)</a>⁠. Sam­ples:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="30 neu­ral-net-gen­er­ated anime sam­ples from Ay­dao’s Dan­booru2019  model (ad­di­tional sets: 1, 2, 3); sam­ples are hand-s­e­lected for be­ing pret­ty, in­ter­est­ing, or demon­strat­ing some­thing." height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1548px" src="./一个StyleGAN动漫脸编辑详细教程_files/2021-01-19-stylegan2ext-danbooru2019-3x10montage-0.webp" srcset="/images/gan/stylegan/2021-01-19-stylegan2ext-danbooru2019-3x10montage-0.png-768px.png 768w, /images/gan/stylegan/2021-01-19-stylegan2ext-danbooru2019-3x10montage-0.png 1548w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">30 neu­ral-net-gen­er­ated anime sam­ples from Ay­dao’s Dan­booru2019 Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-ext model (ad­di­tional sets: <a href="https://www.gwern.net/images/gan/stylegan/2021-01-19-stylegan2ext-danbooru2019-3x10montage-1.png" data-image-height="5160" data-image-width="1548" class="has-content spawns-popup">1</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-19-stylegan2ext-danbooru2019-3x10montage-2.png" data-image-height="5160" data-image-width="1548" class="has-content spawns-popup">2</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-19-stylegan2ext-danbooru2019-3x10montage-3.png" data-image-height="4648" data-image-width="1548" class="has-content spawns-popup">3</a>); sam­ples are hand-s­e­lected for be­ing pret­ty, in­ter­est­ing, or demon­strat­ing some­thing.</figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt=" in­cludes ad­di­tional fea­tures for vi­su­al­iz­ing the  effects on each ran­dom seed" height="771" loading="lazy" sizes="(max-width: 768px) 100vw, 1534px" src="./一个StyleGAN动漫脸编辑详细教程_files/tadne-psitruncation-example-blondeschoolgirl.webp" srcset="/images/gan/stylegan/tadne-psitruncation-example-blondeschoolgirl.png-768px.png 768w, /images/gan/stylegan/tadne-psitruncation-example-blondeschoolgirl.png 1534w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source"><span class="smallcaps-auto cyxy-trs-source">TADNE</span> in­cludes ad­di­tional fea­tures for vi­su­al­iz­ing the <a href="https://www.gwern.net/Faces#psitruncation-trick" class="link-self identifier-link-up has-content spawns-popup cyxy-trs-source cyxy-trs-source">ψ/<wbr>trun­ca­tion</a> effects on each ran­dom seed</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2021-01-18-tadne-interpolation-9x-random_grid_9876_3.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">
3×3 <span class="smallcaps-auto cyxy-trs-source">TADNE</span> in­ter­po­la­tion video (<a href="https://www.gwern.net/images/gan/stylegan/2021-01-18-tadne-interpolation-9x-random_grid_9876_3.mp4" class="has-content spawns-popup cyxy-trs-source">MP4</a>); ad­di­tional in­ter­po­la­tion videos: <a href="https://www.gwern.net/images/gan/stylegan/2021-01-18-tadne-interpolation-1x-aydao_random_grid_99999.mp4" class="has-content spawns-popup">1</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-18-tadne-interpolation-1x-random_grid_999999_notruncation.mp4" class="has-content spawns-popup">2</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-18-tadne-interpolation-4x-random_grid_1524299.mp4" class="has-content spawns-popup">3</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-18-tadne-interpolation-4x-random_grid_77777.mp4" class="has-content spawns-popup">4</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-18-tadne-interpolation-4x-random_grid_77778.mp4" class="has-content spawns-popup">5</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-18-tadne-interpolation-9x-random_grid_9876_3.mp4" class="has-content spawns-popup">6</a>⁠, <a href="https://www.gwern.net/images/gan/stylegan/2021-01-20-tadne-interpolation-4x-random_grid_231231.mp4" class="has-content spawns-popup">7</a>
</figcaption></span></span></figure>
<section id="stylegan2-ext-modifications" class="level3">
<h3><a href="https://www.gwern.net/Faces#stylegan2-ext-modifications" title="Link to section: § &#39;StyleGAN2-ext Modifications&#39;" class="no-popup cyxy-trs-source">StyleGAN2-ext Modifications</a></h3>
<p class="cyxy-trs-source">Broad­ly, Ay­dao’s Style<span class="smallcaps-auto">GAN</span>2-ext in­creases the model size and dis­ables reg­u­lar­iza­tions, which are use­ful for re­stricted do­mains like faces but fail badly on more com­plex and mul­ti­modal do­mains. His con­fig, avail­able in <a href="https://github.com/aydao/stylegan2-surgery/tree/model-release" class="no-popup">his Style<span class="smallcaps-auto">GAN</span>2-ext repo</a> (<a href="https://colab.research.google.com/drive/1gbqukfE5f4yYOuHWFW-85zuXW8JtWS09?usp=sharing" class="no-popup"><code>.ckpt</code> → <code>.pkl</code></a>⁠/ <a href="https://github.com/HighCWu/stylegan2-pytorch2paddle/blob/tadne/convert_weight.py" class="no-popup"><code>.pt</code></a>⁠; <a href="https://colab.research.google.com/drive/1oxcJ1tbG77hlggdKd_d8h22nBcIZsLTL" title="This Anime Does Not Exist - Interpolation Videos: This notebook generates interpolation videos from the model used for https://thisanimedoesnotexist.ai by @aydao" class="no-popup">Co­lab for gen­er­at­ing im­ages &amp; in­ter­po­la­tion videos</a>⁠, <a href="https://colab.research.google.com/drive/1QzttnjpQiVHJ8bnhEP0JaSwBX62V1ieG" class="no-popup">Gan­space</a>⁠, <a href="https://openai.com/blog/clip/" id="radford-et-al-2021" class="docMetadata has-annotation spawns-popup" data-attribute-title="CLIP: Connecting Text and Images"><span class="smallcaps-auto">CLIP</span></a>-based <a href="https://github.com/nagolinc/notebooks/blob/main/TADNE_and_CLIP.ipynb" id="nagolinc-2021" class="docMetadata has-annotation spawns-popup">text → anime gen­er­a­tor</a> eg <a href="https://nitter.cc/advadnoun/status/1353453719510163459" title="This is great! Now that the model can be used in PyTorch, I&#39;ve starting playing with @AydaoAI&#39;s anime StyleGAN directly guided by CLIP. Starting slow by searching for Asuka by name in the latent space." class="no-popup">text in­put “Asuka”</a>⁠, <a href="https://colab.research.google.com/github/Skylion007/StyleGAN-notebooks/blob/main/StyleGAN_of_Anime_Sliders_by_Skyli0n.ipynb" title="StyleGAN Anime Sliders: This notebook demonstrate how to learn and extract controllable directions from ThisAnimeDoesNotExist. This takes a pretrained StyleGAN and uses DeepDanbooru to extract various labels from a number of samples. It then uses those labels to learn various attributes which are controllable with sliders" class="no-popup">slider <span class="smallcaps-auto">GUI</span> edit­ing note­book</a> ):</p>
<ul>
<li><p class="cyxy-trs-source">dis­ables <strong><a href="https://www.gwern.net/docs/www/arxiv.org/4cb3118987e4ea896320737fe1a5bf959c722d04.pdf#page=6" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1812.04948.pdf#page=6" data-attribute-title="(Original URL: https://arxiv.org/pdf/1812.04948.pdf#page=6 )">per­cep­tual path</a> <a href="https://www.gwern.net/docs/www/arxiv.org/5f5f7259127e04a87e267af05f902ebab6b34bf7.pdf#page=5" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1912.04958.pdf#page=5" data-attribute-title="(Original URL: https://arxiv.org/pdf/1912.04958.pdf#page=5 )">length reg­u­lar­iza­tion</a></strong>, re­vert­ing to the sim­ple G lo­gis­tic loss</p></li>
<li><p class="cyxy-trs-source">dis­ables <strong><a href="https://www.gwern.net/docs/www/arxiv.org/4cb3118987e4ea896320737fe1a5bf959c722d04.pdf#page=3" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1812.04948.pdf#page=3" data-attribute-title="3.1. Style mixing (Original URL: https://arxiv.org/pdf/1812.04948.pdf#page=3 )">style mix­ing</a></strong></p></li>
<li><p class="cyxy-trs-source">dis­ables <strong>nor­mal­iz­ing</strong> la­tents</p></li>
<li><p class="cyxy-trs-source">dis­ables <strong>mir­ror­ing/<wbr>flip­ping</strong> data aug­men­ta­tion</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">does not use <span class="smallcaps cyxy-trs-source cyxy-trs-source cyxy-trs-source">Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-<span class="smallcaps-auto cyxy-trs-source">ADA</span> data aug­men­ta­tion</span> train­ing: we have strug­gled to im­ple­ment them on <span class="smallcaps-auto cyxy-trs-source">TPU</span>s. (It’s un­clear how much this mat­ters, as most of ther 2020 flurry of data aug­men­ta­tion <span class="smallcaps-auto cyxy-trs-source">GAN</span> re­search, and <a href="https://arxiv.org/abs/2006.06676#nvidia" id="karras-et-al-2020-6" class="docMetadata has-annotation spawns-popup cyxy-trs-source" data-attribute-title="ADA/StyleGAN3: Training Generative Adversarial Networks with Limited Data">Kar­ras&nbsp;et&nbsp;al&nbsp;2020</a>’s Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-<span class="smallcaps-auto cyxy-trs-source">ADA</span> re­sults, shows the best re­sults in the small <em>n</em> regime: we and other Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-<span class="smallcaps-auto cyxy-trs-source">ADA</span> users have seen it make a huge differ­ence when work­ing with <em>n</em> &lt; 5000, but no one has re­ported more than sub­tle im­prove­ments on Im­a­geNet &amp; be­yond.)</li>
</ul></li>
<li><p class="cyxy-trs-source">weak­ens the <strong><a href="https://www.gwern.net/docs/www/arxiv.org/8d8a40eca72bf3d56b2012bad9d2694208b77fb1.pdf#page=7" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1801.04406.pdf#page=7" data-attribute-title="4.1. Simplified gradient penalties (Original URL: https://arxiv.org/pdf/1801.04406.pdf#page=7 )">R1 gra­di­ent</a> <a href="https://arxiv.org/abs/1705.09367" id="roth-et-al-2017-5" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Stabilizing Training of Generative Adversarial Networks through Regularization&#39;, Roth et al 2017">penal­ty’s</a></strong> gamma pa­ra­me­ter from 10 to 5</p></li>
<li><p class="cyxy-trs-source"><strong>model size in­crease</strong> (more than dou­bles, to 1GB to­tal):</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source">in­creases <span class="smallcaps cyxy-trs-source">G fea­ture-maps’</span> ini­tial base value from 10 to 32</li>
<li class="cyxy-trs-source cyxy-trs-source">in­creases the <span class="smallcaps cyxy-trs-source">max fea­ture-map</span> value from 512 to 1024</li>
<li class="cyxy-trs-source cyxy-trs-source">in­creased the <span class="smallcaps cyxy-trs-source cyxy-trs-source cyxy-trs-source">la­tent <em>w</em> map­ping/<wbr>em­bed­ding</span> mod­ule from 8×512 FC lay­ers to 4×1024 FC lay­ers<a href="https://www.gwern.net/Faces#sn32" class="footnote-ref spawns-popup" id="fnref32" role="doc-noteref"><sup>32</sup></a></li>
<li class="cyxy-trs-source cyxy-trs-source">in­creases <a href="https://www.gwern.net/docs/www/arxiv.org/937f9f5b5f4f529df3f1d3b18905f1eec9722d10.pdf#page=3" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1710.10196.pdf#page=3" data-attribute-title="&#39;Progressive Growing of GANs for Improved Quality, Stability, and Variation&#39;, Karras et al 2017: &#39;3. Increasing Variation Using Minibatch Standard Deviation&#39; (Original URL: https://arxiv.org/pdf/1710.10196.pdf#page=3 )"><span class="smallcaps cyxy-trs-source">mini­batch stan­dard de­vi­a­tion</span></a> to 32, and num­ber of fea­tures to 4</li>
</ul></li>
</ul>
</section>
<section id="tadne-training" class="level3">
<h3><a href="https://www.gwern.net/Faces#tadne-training" title="Link to section: § &#39;TADNE Training&#39;" class="no-popup cyxy-trs-source">TADNE Training</a></h3>
<p class="cyxy-trs-source">Train­ing-wise:</p>
<ul>
<li class="cyxy-trs-source">the model was run for <strong>5.26m it­er­a­tions</strong> on a pre-emptible <span class="smallcaps-auto">TPU</span>v3-32 pod for a month or two (with in­ter­rup­tions) up to 2020-11-27, pro­vided by Google <a href="https://www.tensorflow.org/tfrc" class="docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">TFRC</span></a>⁠; qual­ity was poor up to 2m, and then in­creased steadily to 4m</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">on <a href="https://www.gwern.net/Danbooru2020#danbooru2019" class="link-local has-content spawns-popup cyxy-trs-source">Dan­booru2019</a> orig­i­nal/<wbr>­ful­l-sized <span class="smallcaps-auto cyxy-trs-source">SFW</span> im­ages (<em>n</em> = 2.82m), com­bined with <a href="https://www.gwern.net/Crops#danbooru2019-portraits" id="gwern-crops-danbooru2019-portraits" class="link-local docMetadata has-annotation spawns-popup cyxy-trs-source">Por­traits</a> (<em>n</em> = 0.3m), <a href="https://www.gwern.net/Crops#figure" class="link-local has-content spawns-popup cyxy-trs-source">Fig­ures</a> (<em>n</em> = 0.85m), &amp; <a href="https://www.gwern.net/Crops#palm" class="link-local has-content spawns-popup"><span class="smallcaps-auto cyxy-trs-source">PALM</span></a> (<em>n</em> = 0.05m) for data aug­men­ta­tion (to­tal <em>n</em> ~ 4m)</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">ran­dom crop­ping was ini­tially used for the orig­i­nal im­ages, but we learned mid­way through train­ing from <a href="https://arxiv.org/abs/2006.10738" id="zhao-et-al-2020-6" class="docMetadata has-annotation spawns-popup cyxy-trs-source" data-attribute-title="&#39;Differentiable Augmentation for Data-Efficient GAN Training&#39;, Zhao et al 2020">Zhao&nbsp;et&nbsp;al&nbsp;2020</a> that ran­dom crop­ping se­verely dam­aged Im­a­geNet per­for­mance of Big<span class="smallcaps-auto cyxy-trs-source">GAN</span> and <strong class="cyxy-trs-source">switched to top-crop­ping</strong> every­where; Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-ext ben­e­fited</li>
<li class="cyxy-trs-source"><strong>de­cayed learn­ing rate</strong> late in train­ing</li>
<li class="cyxy-trs-source"><strong><a href="https://arxiv.org/abs/2002.06224" id="sinha-et-al-2020-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples&#39;, Sinha et al 2020">top-<em>k</em></a> train­ing</strong> en­abled late in train­ing (min­i­mal ben­e­fit, per­haps be­cause en­abled too late)</li>
</ul>
<div class="horizontalRule-nth-0">
<hr>
</div>
<p class="cyxy-trs-source">Ob­ser­va­tions:</p>
<ul>
<li><p class="cyxy-trs-source">Train­ing re­mained much <strong>more sta­ble than Big<span class="smallcaps-auto">GAN</span></strong> runs, with no di­ver­gences</p></li>
<li><p class="cyxy-trs-source">We ob­serve that Style<span class="smallcaps-auto">GAN</span>2-ext vastly <strong>out­per­forms base­line Style<span class="smallcaps-auto">GAN</span>2</strong> runs on Dan­booru2019: the prob­lems with Style<span class="smallcaps-auto">GAN</span>2 ap­pear to be less in­her­ent weak­ness of the AdaIN ar­chi­tec­ture (as com­pared to Big<span class="smallcaps-auto">GAN</span> etc) than the ex­tremely heavy reg­u­lar­iza­tion built into it by de­fault (ca­ter­ing to small <em>n</em>/<wbr>sim­ple do­mains like hu­man faces).</p>
<p class="cyxy-trs-source">The key is <span class="smallcaps">dis­abling path length reg­u­lar­iza­tion &amp; style mix­ing</span>, at which point Style<span class="smallcaps-auto">GAN</span>2 is able to scale &amp; make good use of model size in­creases (rather than un­der­fit­ting &amp; churn­ing end­less­ly).</p>
<p class="cyxy-trs-source"><span class="smallcaps">Changes that did not help</span> (much): adding self­-at­ten­tion to Style<span class="smallcaps-auto">GAN</span><a href="https://www.gwern.net/Faces#sn33" class="footnote-ref spawns-popup" id="fnref33" role="doc-noteref"><sup>33</sup></a>⁠, re­mov­ing the “4×4 const” in­put, re­mov­ing the <em>w</em> deep FC stack (adding it to Big<span class="smallcaps-auto">GAN</span> also proved desta­bi­liz­ing), re­mov­ing the Lapla­cian pyra­mid/<wbr>ad­di­tive im­age ar­chi­tec­ture (did­n’t help Big<span class="smallcaps-auto">GAN</span> much when added), large batch train­ing.</p></li>
<li><p class="cyxy-trs-source">Also of note is the sur­pris­ingly <strong>high qual­ity writ­ing</strong>—while dis­abling mir­ror­ing makes it pos­si­ble to learn non-gib­ber­ish let­ters, we were still sur­prised how well Style<span class="smallcaps-auto">GAN</span>2-ext can learn to write.</p></li>
<li><p class="cyxy-trs-source">Hands have long been a weak point, but <strong>its hands are bet­ter</strong> than usu­al, and the ad­di­tion of <span class="smallcaps-auto">PALM</span> to train­ing caused an im­me­di­ate qual­ity im­prove­ment, demon­strat­ing the ben­e­fits of care­ful­ly-tar­geted data aug­men­ta­tion.</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source"><span class="smallcaps cyxy-trs-source"><span class="smallcaps-auto cyxy-trs-source">PALM</span>’s draw­back: affect­ing G</span>. The draw­back of us­ing <span class="smallcaps-auto cyxy-trs-source">PALM</span> is that the fi­nal model gen­er­ates closeup hands oc­ca­sion­al­ly. Our orig­i­nal plan was to re­move the data aug­men­ta­tion datasets at the end of train­ing and fine­tune for a short pe­ri­od, to teach it to not gen­er­ate the <span class="smallcaps-auto cyxy-trs-source">PALM</span>-style hand­s—but when Ay­dao tried that, the hands in the reg­u­lar im­ages be­gan to de­grade as Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-ext ap­par­ently be­gan to for­get how to do hands! So it would be bet­ter to leave the data aug­men­ta­tion datasets in, and in­stead, screen out gen­er­ated hands us­ing the orig­i­nal <span class="smallcaps-auto cyxy-trs-source">PALM</span> <span class="smallcaps-auto cyxy-trs-source">YOLO</span>v3 hand-de­tec­tion mod­el.</li>
</ul></li>
<li><p class="cyxy-trs-source">But de­spite the qual­i­ty, <strong>the usual ar­ti­facts re­main</strong>: ar­ti­facts that I’ve ob­served in ear­lier Style<span class="smallcaps-auto">GAN</span>2 runs (like This Fur­sona Does Not Ex­ist/<wbr>This Pony Does Not Ex­ist), con­tinue to ap­pear, such as strangely smooth tran­si­tions of dis­crete ob­jects such as neck­laces or sleeves, and ‘cel­lu­lar di­vi­sion’ of 1 🡺 2 head­s/<wbr>char­ac­ter­s/<wbr>­bod­ies.</p>
<p class="cyxy-trs-source">I spec­u­late these ar­ti­facts, which re­main present de­spite great in­creases in qual­i­ty, are caused by the smooth <em>z</em>/<wbr><em>w</em> la­tent space which is forced to trans­form Gauss­ian vari­ables into bi­nary in­di­ca­tors, and does so poorly (eg in <span class="smallcaps-auto">TPDNE</span>, many sam­ples of ponies have stubby pseudo-horns, even though ponies in the orig­i­nal art­work ei­ther have a horn or do not, and in <span class="smallcaps-auto">TWDNE</span>, there are faces with am­bigu­ous half-glasses smeared on them); the <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=27" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=27" data-attribute-title="Appendix E: Choosing Latent Spaces (Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=27 )">Big<span class="smallcaps-auto">GAN</span> pa­per</a> notes that there were gains from us­ing non-nor­mal vari­ables like bi­nary vari­ables, and I would pre­dict that salt­ing in bi­nary vari­ables or <a href="https://en.wikipedia.org/wiki/Rectified_Gaussian_distribution" class="docMetadata has-annotation spawns-popup">cen­sored (rec­ti­fied) nor­mals</a> (a change I call <a href="https://github.com/tensorfork/tensorfork/issues/34" class="no-popup">‘mix­ture la­tents’</a>) to the la­tent space would fix this.</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">In­ter­est­ing­ly, the <span class="smallcaps cyxy-trs-source">la­tent space is qual­i­ta­tively differ­ent</span> and ‘larger’ than our ear­lier Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>s: whereas the face/<wbr>­por­trait Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>s fa­vor a sweet spot of ψ ~ 0.7, and sam­ples se­ri­ously de­te­ri­o­rate by ψ ~ 1.2, the Dan­booru2019 Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-ext is best at ψ ~ 1.1 and only be­gins no­tice­ably de­grad­ing by ψ ~ 1.4, with good sam­ples find­able all the way up to 2.0 (!).</li>
</ul></li>
<li><p class="cyxy-trs-source"><strong>Fine de­tail­s/<wbr>­tex­tures are poor</strong>; this sug­gests <a href="https://github.com/l4rz/scaling-up-stylegan2" title="Scaling up StyleGAN2" class="no-popup">l4rz’s scal­ing strat­egy</a> of re­al­lo­cat­ing pa­ra­me­ters from the glob­al/<wbr>low-res­o­lu­tion lay­ers to the high­er-res­o­lu­tion lay­ers would be use­ful</p></li>
<li><p class="cyxy-trs-source">Over­all, <strong>we can do even bet­ter!</strong>—we ex­pect that a fresh train­ing run in­cor­po­rat­ing the fi­nal hy­per­pa­ra­me­ters, com­bined with ad­di­tional im­ages from Dan­booru2020 &amp; pos­si­bly a new ar­m/<wbr>­tor­so-fo­cused data aug­men­ta­tion, would be no­tice­ably bet­ter. (How­ev­er, as sur­pris­ingly well as Style<span class="smallcaps-auto">GAN</span>2-ext scaled, I per­son­ally still think Big<span class="smallcaps-auto">GAN</span> will scale bet­ter.)</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">We <span class="smallcaps cyxy-trs-source">have not tried trans­fer learn­ing</span> (a­side from <a href="https://nitter.cc/arfafax/status/1353246916797075457" title="Some heavily cherrypicked samples from transfer learning using @AydaoAI&#39;s enhanced StyleGAN2 anime model after 2 days." class="no-popup cyxy-trs-source cyxy-trs-source">Ar­fa’s pre­lim­i­nary furry Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>2-ext</a>), but given how well trans­fer learn­ing has worked for <em>n</em> as small as <em>n</em> ~ 100 for the Face/<wbr>­Por­trait mod­els, we ex­pect it to work well for this model too—and po­ten­tially give even bet­ter re­sults on faces, as it was trained on a su­per­set of the Face/<wbr>­Por­trait data &amp; knows more about bod­ies/<wbr>back­ground­s/<wbr>­head­wear/<wbr>etc.</li>
</ul></li>
</ul>
</section>
<section id="tadne-download" class="level3">
<h3><a href="https://www.gwern.net/Faces#tadne-download" title="Link to section: § &#39;TADNE Download&#39;" class="no-popup cyxy-trs-source">TADNE Download</a></h3>
<p class="cyxy-trs-source">Down­load (as with all our mod­els, the mod­el/<wbr>sam­ples is li­censed un­der CC-0 or pub­lic do­main):</p>
<ul>
<li><p class="cyxy-trs-source"><strong>Model</strong>:</p>
<ul>
<li class="cyxy-trs-source"><a href="https://mega.nz/file/nUkWFZgS#EHHrqILumjpTppSXG-QlCOdWaUIVLTDnqPxsXPrI3UQ" title="aydao-anime-danbooru2019s-512-5268480.pkl" class="no-popup cyxy-trs-source">Mega</a> (1GB)</li>
<li><a href="https://drive.google.com/file/d/1qNhyusI0hwBLI-HOavkNP5I0J0-kcN4C/view" class="no-popup cyxy-trs-source">Google Drive</a> (<a href="https://drive.google.com/file/d/1A-E_E32WAtTHRlOzjhhYhyyBDXLJN9_H/view" class="no-popup cyxy-trs-source">backup</a>)</li>
<li class="cyxy-trs-source">Rsync mir­ror: <code>rsync -v rsync://78.46.86.149:873/biggan/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl ./</code></li>
</ul></li>
<li><p class="cyxy-trs-source"><strong>Im­ages</strong>: <a href="https://thisanimedoesnotexist.ai/downloads.html" class="no-popup">tar­balls of all im­ages, or­dered by ψ</a> (&gt;1.8m im­ages; &gt;618GB)</p></li>
</ul>
</section>
</section>
</section>
<section id="transfer-learning" class="level1">
<h1><a href="https://www.gwern.net/Faces#transfer-learning" title="Link to section: § &#39;Transfer Learning&#39;" class="no-popup cyxy-trs-source">Transfer Learning</a></h1>
<div class="epigraph">
<blockquote>
<p class="cyxy-trs-source">In the days when <a href="https://en.wikipedia.org/wiki/Joseph_M._Sussman" class="docMetadata has-annotation spawns-popup">Suss­man</a> was a novice, <a href="https://en.wikipedia.org/wiki/Marvin_Minsky" class="docMetadata has-annotation spawns-popup">Min­sky</a> once came to him as he sat hack­ing at the <a href="https://en.wikipedia.org/wiki/PDP-6" class="docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">PDP-6</span></a>⁠.</p>
<p class="cyxy-trs-source">“What are you do­ing?”, asked Min­sky. “I am train­ing a ran­domly wired neural net to play Tic-Tac-Toe” Suss­man replied. “Why is the net wired ran­dom­ly?”, asked Min­sky. “I do not want it to have any pre­con­cep­tions of how to play”, Suss­man said.</p>
<p class="cyxy-trs-source">Min­sky then shut his eyes. “Why do you close your eyes?”, Suss­man asked his teacher. “So that the room will be emp­ty.”</p>
<p class="cyxy-trs-source">At that mo­ment, Suss­man was en­light­ened.</p>
<p class="cyxy-trs-source">“Suss­man at­tains en­light­en­ment”, <a href="http://catb.org/jargon/html/koans.html" class="no-popup">“AI Koans”</a>⁠, <a href="https://en.wikipedia.org/wiki/Jargon_File" class="docMetadata has-annotation spawns-popup"><em>Jar­gon File</em></a></p>
</blockquote>
</div>
<p class="cyxy-trs-source">One of the most use­ful things to do with a trained model on a broad data cor­pus is to use it as a launch­ing pad to train a bet­ter model quicker on lesser data, called “trans­fer learn­ing”. For ex­am­ple, one might trans­fer learn from Nvidi­a’s <span class="smallcaps-auto">FFHQ</span> face Style<span class="smallcaps-auto">GAN</span> model to a differ­ent celebrity dataset, or from <a href="https://www.gwern.net/docs/www/www.chrisplaysgames.com/4d471aaaf56bc67e0bcada9667255ebe77e9341b.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.chrisplaysgames.com/gadgets/2019/02/25/how-i-learned-to-stop-worrying-and-love-transfer-learning/" data-attribute-title="How I Learned to Stop Worrying and Love Transfer Learning (Original URL: https://www.chrisplaysgames.com/gadgets/2019/02/25/how-i-learned-to-stop-worrying-and-love-transfer-learning/ )">bed­rooms → kitchens</a>⁠. Or with the anime face mod­el, one might re­train it on a sub­set of faces—all char­ac­ters with red hair, or all male char­ac­ters, or just a sin­gle spe­cific char­ac­ter. Even if a dataset seems differ­ent, start­ing from a pre­trained model can save time; after all, while male and fe­male faces may look differ­ent and it may seem like a mis­take to start from a most­ly-fe­male anime face mod­el, the al­ter­na­tive of start­ing from scratch means start­ing with a model gen­er­at­ing ran­dom rain­bow-col­ored sta­t­ic, and surely male faces look far more like fe­male faces than they do ran­dom sta­t­ic?<a href="https://www.gwern.net/Faces#sn34" class="footnote-ref spawns-popup" id="fnref34" role="doc-noteref"><sup>34</sup></a> In­deed, you can quickly train a pho­to­graphic face model start­ing from the anime face mod­el.</p>
<p class="cyxy-trs-source">This ex­tends the reach of good Style<span class="smallcaps-auto">GAN</span> mod­els from those blessed with both big data &amp; big com­pute to those with lit­tle of ei­ther. Trans­fer learn­ing works par­tic­u­larly well for <em>spe­cial­iz­ing</em> the anime face model to a spe­cific char­ac­ter: the im­ages of that char­ac­ter would be too lit­tle to train a good Style<span class="smallcaps-auto">GAN</span> on, too data-im­pov­er­ished for the sam­ple-in­effi­cient Style<span class="smallcaps-auto">GAN</span>1–2<a href="https://www.gwern.net/Faces#sn35" class="footnote-ref spawns-popup" id="fnref35" role="doc-noteref"><sup>35</sup></a>⁠, but hav­ing been trained on all anime faces, the Style<span class="smallcaps-auto">GAN</span> has learned well the full space of anime faces and can eas­ily spe­cial­ize down with­out over­fit­ting. Try­ing to do, say, faces ↔︎ land­scapes is prob­a­bly a bridge too far.</p>
<p class="cyxy-trs-source">Data-wise, for do­ing face spe­cial­iza­tion, the more the bet­ter but <em>n</em> = 500–5000 is an ad­e­quate range, but even as low as <em>n</em> = 50 works sur­pris­ingly well. I don’t know to what ex­tent data aug­men­ta­tion can sub­sti­tute for orig­i­nal dat­a­points but it’s prob­a­bly worth a try es­pe­cially if you have <em>n</em> &lt; 5000.</p>
<p class="cyxy-trs-source">Com­pute-wise, spe­cial­iza­tion is rapid. Adap­ta­tion can hap­pen within a few ticks, pos­si­bly even 1. This is sur­pris­ingly fast given that Style<span class="smallcaps-auto">GAN</span> is not de­signed for few-shot/<wbr>­trans­fer learn­ing. I spec­u­late that this may be be­cause the Style<span class="smallcaps-auto">GAN</span> la­tent space is ex­pres­sive enough that even new faces (such as new hu­man faces for a <span class="smallcaps-auto">FFHQ</span> mod­el, or a new anime char­ac­ter for an ani­me-face mod­el) are still al­ready present in the la­tent space. Ex­am­ples of the ex­pres­siv­ity are pro­vided by <a href="https://www.gwern.net/docs/ai/2019-abdal.pdf" id="abdal-et-al-2019" class="docMetadata has-annotation spawns-popup" data-attribute-title="Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?">Ab­dal&nbsp;et&nbsp;al&nbsp;2019</a>⁠, who find that “al­though the Style<span class="smallcaps-auto">GAN</span> gen­er­a­tor is trained on a hu­man face dataset [<span class="smallcaps-auto">FFHQ</span>], the em­bed­ding al­go­rithm is ca­pa­ble of go­ing far be­yond hu­man faces. As Fig­ure 1 shows, al­though slightly worse than those of hu­man faces, we can ob­tain rea­son­able and rel­a­tively high­-qual­ity em­bed­dings of cats, dogs and even paint­ings and cars.” If even im­ages as differ­ent as cars can be en­coded suc­cess­fully into a face Style<span class="smallcaps-auto">GAN</span>, then clearly the la­tent space can eas­ily model new faces and so any new face train­ing data is in some sense <em>al­ready</em> learned; so the train­ing process is per­haps not so much about learn­ing ‘new’ faces as about mak­ing the new faces more ‘im­por­tant’ by ex­pand­ing the la­tent space around them &amp; con­tract­ing it around every­thing else, which seems like a far eas­ier task.</p>
<p class="cyxy-trs-source">How does one ac­tu­ally do trans­fer learn­ing? Since Style<span class="smallcaps-auto">GAN</span> is (cur­rent­ly) un­con­di­tional with no dataset-spe­cific cat­e­gor­i­cal or text or meta­data en­cod­ing, just a flat set of im­ages, all that has to be done is to en­code the new dataset and sim­ply start train­ing with an ex­ist­ing mod­el. One cre­ates the new dataset as usu­al, and then ed­its <code>training.py</code> with a new <code>-desc</code> line for the new dataset, and if <code>resume_kimg</code> is set cor­rectly (see next para­graph) and <code>resume_run_id = "latest"</code> en­abled as ad­vised, you can then run <code>python train.py</code> and presto, trans­fer learn­ing.</p>
<p class="cyxy-trs-source">The main prob­lem seems to be that train­ing can­not be done from scratch/<wbr>0 it­er­a­tions, as one might naively as­sume—when I tried this, it did not work well and Style<span class="smallcaps-auto">GAN</span> ap­peared to be ig­nor­ing the pre­trained mod­el. My hy­poth­e­sis is that as part of the pro­gres­sive grow­ing/<wbr>­fad­ing in of ad­di­tional res­o­lu­tion/<wbr>lay­ers, Style<span class="smallcaps-auto">GAN</span> sim­ply ran­dom­izes or wipes out each new layer and over­writes them—­mak­ing it point­less. This is easy to avoid: sim­ply jump the train­ing sched­ule all the way to the de­sired res­o­lu­tion. For ex­am­ple, to start at one’s max­i­mum size (here 512px) one might set <code>resume_kimg=7000</code> in <code>training_loop.py</code>. This forces Style<span class="smallcaps-auto">GAN</span> to skip all the pro­gres­sive grow­ing and load the full model as-is. To make sure you did it right, check the <em>first</em> sam­ple (<code>fakes07000.png</code> or what­ev­er), from be­fore any trans­fer learn­ing train­ing has been done, and it should look like the orig­i­nal model did at the end of its train­ing. Then sub­se­quent train­ing sam­ples should show the orig­i­nal quickly mor­ph­ing to the new dataset. (Any­thing like <code>fakes00000.png</code> should not show up be­cause that in­di­cates be­gin­ning from scratch.)</p>
<section id="anime-faces-character-faces" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces-character-faces" title="Link to section: § &#39;Anime Faces → Character Faces&#39;" class="no-popup cyxy-trs-source">Anime Faces → Character Faces</a></h2>
<section id="holo" class="level3">
<h3><a href="https://www.gwern.net/Faces#holo" title="Link to section: § &#39;Holo&#39;" class="no-popup cyxy-trs-source">Holo</a></h3>
<p class="cyxy-trs-source">The first trans­fer learn­ing was done with Holo of <em><a href="https://en.wikipedia.org/wiki/Spice_%26_Wolf" class="docMetadata has-annotation spawns-popup">Spice &amp; Wolf</a></em>. It used a 512px Holo face dataset cre­ated with Na­gadomi’s crop­per from all of Dan­booru2017, up­scaled with <code>waifu2x</code>, cleaned by hand, and then data-aug­mented from <em>n</em> = 3900 to <em>n</em> = 12600; mir­ror­ing was en­abled since Holo is sym­met­ri­cal. I then used the anime face model as of 2019-02-09—it was not fully con­verged, in­deed, would­n’t con­verge with weeks more train­ing, but the qual­ity was so good I was too cu­ri­ous as to how well re­train­ing would work so I switched gears.</p>
<p class="cyxy-trs-source">It’s worth men­tion­ing that this dataset was used pre­vi­ously with Pro<span class="smallcaps-auto">GAN</span>, where after weeks of train­ing, <a href="https://www.gwern.net/Faces-graveyard#progan" class="link-local has-content spawns-popup">Pro<span class="smallcaps-auto">GAN</span> over­fit badly as demon­strated by the sam­ples &amp; in­ter­po­la­tion videos</a>⁠.</p>
<p class="cyxy-trs-source">Train­ing hap­pened re­mark­ably quick­ly, with all the faces con­verted to rec­og­niz­ably Holo faces within a few hun­dred it­er­a­tions:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-10-stylegan-holotransfer-trainingmontage.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-10-stylegan-holotransfer-trainingmontage.mp4" class="has-content spawns-popup cyxy-trs-source">Train­ing mon­tage</a> of a Holo face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> (blink &amp; you’ll miss it)
</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-20-stylegan-holo-interpolation.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-03-20-stylegan-holo-interpolation.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> of the Holo face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>
</figcaption></span></span></figure>
<p class="cyxy-trs-source">The best sam­ples were con­vinc­ing with­out ex­hibit­ing the fail­ures of the Pro<span class="smallcaps-auto">GAN</span>:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="64 hand-s­e­lected Holo face sam­ples" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-10-stylegan-holotransfer-handselected-64samples.jpg" srcset="/images/gan/stylegan/2019-02-10-stylegan-holotransfer-handselected-64samples.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-10-stylegan-holotransfer-handselected-64samples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">64 hand-s­e­lected Holo face sam­ples</figcaption></span></span></figure>
<p class="cyxy-trs-source">The Style<span class="smallcaps-auto">GAN</span> was much more suc­cess­ful, de­spite a few fail­ure la­tent points car­ried over from the anime faces. In­deed, after a few hun­dred it­er­a­tions, it was start­ing to over­fit with the ‘crack’ ar­ti­facts &amp; smear­ing in the in­ter­po­la­tions. The lat­est I was will­ing to use was it­er­a­tion #11370, and I think it is still some­what over­fit any­way. I thought that with its to­tal <em>n</em> (after data aug­men­ta­tion), Holo would be able to train longer (be­ing 1⁄7<sup>th</sup> the size of <span class="smallcaps-auto">FFHQ</span>), but ap­par­ently not. Per­haps the data aug­men­ta­tion is con­sid­er­ably less valu­able than 1-for-1, ei­ther be­cause the in­vari­ants en­coded in aren’t that use­ful (sug­gest­ing that Geirhos&nbsp;et&nbsp;al&nbsp;2018-like style trans­fer data aug­men­ta­tion is what’s nec­es­sary) or that they would be but the anime face Style<span class="smallcaps-auto">GAN</span> has <em>al­ready</em> learned them all as part of the pre­vi­ous train­ing &amp; needs more real data to bet­ter un­der­stand Holo-like faces. It’s also pos­si­ble that the re­sults could be im­proved by us­ing one of the later anime face Style<span class="smallcaps-auto">GAN</span>s since they did im­prove when I trained them fur­ther after my 2 Holo/<wbr>A­suka trans­fer ex­per­i­ments.</p>
<p class="cyxy-trs-source">Nev­er­the­less, im­pressed, I could­n’t help but won­der if they had reached hu­man-levels of verisimil­i­tude: would an un­wary viewer as­sume they were hand­made?</p>
<p class="cyxy-trs-source">So I se­lected <a href="https://www.gwern.net/docs/ai/anime/2019-02-10-stylegan-holo-handselectedsamples.zip" class="no-popup">~100 of the best sam­ples</a> (24MB; <a href="https://imgur.com/a/xXuKwYV" class="no-popup">Imgur mir­ror</a>) from a dump of 2000, cropped about 5% from the left­/<wbr>right sides to hide the back­ground ar­ti­facts a lit­tle bit, and sub­mit­ted them on 2019-02-11 to <a href="https://www.gwern.net/docs/www/old.reddit.com/85dc17fcdbc1de2baabe4d81b8bac3b2e83c01b6.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/SpiceandWolf/comments/apazs0/my_holo_face_collection/" data-attribute-title="(Original URL: https://old.reddit.com/r/SpiceandWolf/comments/apazs0/my_holo_face_collection/ )">/<wbr>r/<wbr>Spice­and­Wolf</a> un­der an alt ac­count. I made the mis­take of sort­ing by file­size &amp; thus lead­ing with a face that was par­tic­u­larly sus­pi­cious (streaky hair) so one Red­di­tor voiced the sus­pi­cion they were from <span class="smallcaps-auto">MGM</span> (ab­surd yet not en­tirely wrong) but all the other com­menters took the faces in stride or prais­ing them, and the sub­mis­sion re­ceived +248 votes (99% pos­i­tive) by March. A Red­di­tor then <a href="https://www.gwern.net/docs/www/old.reddit.com/ee2e597b657b61b7d7ecba68f77833672627b400.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/SpiceandWolf/comments/apbz6r/all_those_cropped_holo_faces_uprimarypizza_posted/" data-attribute-title="(Original URL: https://old.reddit.com/r/SpiceandWolf/comments/apbz6r/all_those_cropped_holo_faces_uprimarypizza_posted/ )">turned them all into a <span class="smallcaps-auto">GIF</span> video</a> which earned +192 (100%) and many pos­i­tive com­ments with no fur­ther sus­pi­cions un­til I ex­plained. Not bad in­deed.</p>
<p class="cyxy-trs-source">The #11370 Holo Style<span class="smallcaps-auto">GAN</span> model is <a href="https://mega.nz/#!afIjAAoJ!ATuVaw-9k5I5cL_URTuK2zI9mybdgFGYMJKUUHUfbk8" title="2019-02-10-stylegan-holofaces-networksnapshot-00015-011370.pkl" class="no-popup">avail­able for down­load</a>⁠.</p>
</section>
<section id="asuka" class="level3">
<h3><a href="https://www.gwern.net/Faces#asuka" title="Link to section: § &#39;Asuka&#39;" class="no-popup cyxy-trs-source">Asuka</a></h3>
<p class="cyxy-trs-source">After the Holo train­ing &amp; link sub­mis­sion went so well, I knew I had to try my other char­ac­ter dataset, Asuka, us­ing <em>n</em> = 5300 data-aug­mented to <em>n</em> = 58,000.<a href="https://www.gwern.net/Faces#sn36" class="footnote-ref spawns-popup" id="fnref36" role="doc-noteref"><sup>36</sup></a> Keep­ing in mind how data seemed to limit the Holo qual­i­ty, I left mir­ror­ing en­abled for Asuka, even though she is not sym­met­ri­cal due to her <a href="https://en.wikipedia.org/wiki/Evangelion%3A_3.0_You_Can_%28Not%29_Redo" class="docMetadata has-annotation spawns-popup"><em>3.0</em></a> eye­patch over her left eye (as purists will no doubt note).</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-10-stylegan-holotransfer-trainingmontage.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-03-20-stylegan-asukatransfer.mp4" class="has-content spawns-popup cyxy-trs-source">Train­ing mon­tage</a> of an Asuka face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>
</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-21-stylegan-asukatransfer-interpolation.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-21-stylegan-asukatransfer-interpolation.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> of the Asuka face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>
</figcaption></span></span></figure>
<p class="cyxy-trs-source">In­ter­est­ing­ly, while Holo trained within 4 <span class="smallcaps-auto">GPU</span>-hours, Asuka proved much more diffi­cult and did not seem to be fin­ished train­ing or show­ing the cracks de­spite train­ing twice as long. Is this due to hav­ing ~35% more real data, hav­ing 10× rather than 3× data aug­men­ta­tion, or some in­her­ent differ­ence like Asuka be­ing more com­plex (eg be­cause of more vari­a­tions in her ap­pear­ance like the eye­patches or plug­suit­s)?</p>
<p class="cyxy-trs-source">I gen­er­ated <a href="https://mega.nz/#!gEFVwAoK!qYrejFI0w1g1BzeuI-st5ajaQLoVFOZsj5j_OTREp1c" title="2019-02-13-stylegan-asuka-psi1.2.tar" class="no-popup">1000 ran­dom sam­ples with Ψ = 1.2</a> be­cause they were par­tic­u­larly in­ter­est­ing to look at. As with Holo, I picked out the <a href="https://www.gwern.net/docs/ai/anime/2019-02-11-stylegan-asuka-handselectedsamples.zip" class="no-popup">best 100</a> (13MB; <a href="https://imgur.com/a/7R8IRVh" class="no-popup">Imgur mir­ror</a>) from ~2000:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="64 hand-s­e­lected Asuka face sam­ples" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-10-stylegan-asukatransfer-handselected-64samples.jpg" srcset="/images/gan/stylegan/2019-02-10-stylegan-asukatransfer-handselected-64samples.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-10-stylegan-asukatransfer-handselected-64samples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">64 hand-s­e­lected Asuka face sam­ples</figcaption></span></span></figure>
<p class="cyxy-trs-source">And I sub­mit­ted to the <a href="https://www.gwern.net/docs/www/old.reddit.com/8383755a3d9b902b46c5abbff5d9d0429ffbac8b.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/evangelion/comments/apmkjm/brighten_your_monday_with_some_asukas_album_of_130/" data-attribute-title="(Original URL: https://old.reddit.com/r/evangelion/comments/apmkjm/brighten_your_monday_with_some_asukas_album_of_130/ )">/<wbr>r/<wbr>E­van­ge­lion</a> sub­red­dit, where it also did well (+109, 98%); there were no spec­u­la­tions about the faces be­ing NN-gen­er­ated be­fore I re­vealed it, merely re­quests for more. Be­tween the two, it ap­pears that with ad­e­quate data (<em>n</em> &gt; 3000) and mod­er­ate cu­ra­tion, a sim­ple kind of art Tur­ing test can be passed.</p>
<p class="cyxy-trs-source">The #7903 Asuka Style<span class="smallcaps-auto">GAN</span> model is <a href="https://mega.nz/#!0JVxHQCD!C7ijBpRWNpcL_gubWFR-GTBDJTW1jXI6ThzSxwaw2aE" title="2019-02-10-stylegan-asuka-networksnapshot-00025-007903.pkl" class="no-popup">avail­able for down­load</a>⁠.</p>
</section>
<section id="zuihou" class="level3">
<h3><a href="https://www.gwern.net/Faces#zuihou" title="Link to section: § &#39;Zuihou&#39;" class="no-popup cyxy-trs-source">Zuihou</a></h3>
<p class="cyxy-trs-source">In early Feb­ru­ary 2019, us­ing the then-re­leased mod­el, Red­di­tor <a href="https://www.gwern.net/docs/www/old.reddit.com/9023192ca316c950c7f2c82747916997ded6e92b.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/apq4xu/p_stylegan_on_anime_faces/egf8pvt/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/apq4xu/p_stylegan_on_anime_faces/egf8pvt/ )">End­ing_­Cred­its</a> tried trans­fer learn­ing to <em>n</em> = 500 faces of the <em><a href="https://en.wikipedia.org/wiki/Kantai_Collection" class="docMetadata has-annotation spawns-popup">Kan­tai Col­lec­tion</a></em> <a href="https://kancolle.fandom.com/wiki/Zuihou" class="no-popup">Zui­hou</a> for ~1 tick (~60k it­er­a­tions).</p>
<p class="cyxy-trs-source">The sam­ples &amp; in­ter­po­la­tions have many ar­ti­facts, but the sam­ple size is tiny and I’d con­sider this good fine­tun­ing from a model never in­tended for few-shot learn­ing:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt=" trans­fer learn­ing from anime face  to Kan­Colle Zui­hou by End­ing_­Cred­its, 8×15 ran­dom sam­ple grid" height="747" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-14-endingcredits-stylegan-zuihoutransfer.jpg" srcset="/images/gan/stylegan/2019-02-14-endingcredits-stylegan-zuihoutransfer.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-14-endingcredits-stylegan-zuihoutransfer.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> trans­fer learn­ing from anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> to <em class="cyxy-trs-source">Kan­Colle</em> Zui­hou by End­ing_­Cred­its, 8×15 ran­dom sam­ple grid</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-14-endingcredits-stylegan-zuihoutransfer-interpolation-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-14-endingcredits-stylegan-zuihoutransfer-interpolation-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Zui­hou face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, trained by End­ing_­Cred­its
</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-14-endingcredits-stylegan-zuihoutransfer-interpolation-1x.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-14-endingcredits-stylegan-zuihoutransfer-interpolation-1x.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (1×1) of the Zui­hou face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, trained by End­ing_­Cred­its
</figcaption></span></span></figure>
<p class="cyxy-trs-source">Prob­a­bly it could be made bet­ter by start­ing from the lat­est anime face Style<span class="smallcaps-auto">GAN</span> mod­el, and us­ing ag­gres­sive data aug­men­ta­tion. An­other op­tion would be to try to find as many char­ac­ters which look <em>sim­i­lar</em> to Zui­hou (match­ing on hair color might work) and train on a joint dataset—un­con­di­tional sam­ples would then need to be fil­tered for just Zui­hou faces, but per­haps that draw­back could be avoided by a third stage of Zui­hou-only train­ing?</p>
</section>
<section id="ganso" class="level3">
<h3><a href="https://www.gwern.net/Faces#ganso" title="Link to section: § &#39;Ganso&#39;" class="no-popup cyxy-trs-source">Ganso</a></h3>
<section id="akizuki" class="level4">
<h4><a href="https://www.gwern.net/Faces#akizuki" title="Link to section: § &#39;Akizuki&#39;" class="no-popup cyxy-trs-source">Akizuki</a></h4>
<p class="cyxy-trs-source">An­other <em>Kan­colle</em> char­ac­ter, <a href="https://kancolle.fandom.com/wiki/Akizuki" class="no-popup">Ak­izuki</a>⁠, was trained in <a href="https://nitter.cc/Gansodeva/status/1122361947410849792" class="no-popup">April 2019</a> by Gan­so.</p>
</section>
<section id="ptilopsis" class="level4">
<h4><a href="https://www.gwern.net/Faces#ptilopsis" title="Link to section: § &#39;Ptilopsis&#39;" class="no-popup cyxy-trs-source">Ptilopsis</a></h4>
<p class="cyxy-trs-source">In Jan­u­ary 2020, Ganso trained a <a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-down has-content spawns-popup">Style<span class="smallcaps-auto">GAN</span> 2</a> model from the S2 por­trait model on a tiny cor­pus of Ptilop­sis im­ages, a char­ac­ter from <a href="https://www.gwern.net/docs/www/www.arknights.global/78ce5cdf870491231e7b7312174d5009dac60b0f.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.arknights.global/" data-attribute-title="(Original URL: https://www.arknights.global/ )"><em>Arknights</em></a>⁠, a 2017 Chi­nese <a href="https://en.wikipedia.org/wiki/Tower_defense" class="docMetadata has-annotation spawns-popup">tower de­fense</a> <span class="smallcaps-auto">RPG</span> mo­bile game.</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Train­ing sam­ples of Ptilop­sis, Arknights  2 por­traits trans­fer, by Gan­so)" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2020-01-19-ganso-stylegan2-arknights-ptilopsis-samples.jpg" srcset="/images/gan/stylegan/2020-01-19-ganso-stylegan2-arknights-ptilopsis-samples.jpg-768px.jpg 768w, /images/gan/stylegan/2020-01-19-ganso-stylegan2-arknights-ptilopsis-samples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Train­ing sam­ples of Ptilop­sis, <em class="cyxy-trs-source">Arknights</em> (Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> 2 por­traits trans­fer, by Gan­so)</figcaption></span></span></figure>
<p class="cyxy-trs-source"><a href="https://en.wikipedia.org/wiki/Ptilopsis" class="docMetadata has-annotation spawns-popup"><em>Ptilop­sis</em></a> are owls, and her char­ac­ter de­sign shows promi­nent ears; de­spite the few im­ages to work with (just 21 on Dan­booru as of 2020-01-19), the in­ter­po­la­tion shows smooth ad­just­ments of the ears in all po­si­tions &amp; align­ments, demon­strat­ing the power of trans­fer learn­ing:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2020-01-19-ganso-stylegan2-arknights-ptilopsis-interpolation.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2020-01-19-ganso-stylegan2-arknights-ptilopsis-interpolation.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Ptilop­sis face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> 2, trained by Ganso
</figcaption></span></span></figure>
</section>
</section>
<section id="fate" class="level3">
<h3><a href="https://www.gwern.net/Faces#fate" title="Link to section: § &#39;Fate&#39;" class="no-popup"><em class="cyxy-trs-source">Fate</em></a></h3>
<section id="saber" class="level4">
<h4><a href="https://www.gwern.net/Faces#saber" title="Link to section: § &#39;Saber&#39;" class="no-popup cyxy-trs-source">Saber</a></h4>
<p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/old.reddit.com/ca779acd0915a9f1cc35f35f2d884fb0d8f953b3.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/apq4xu/p_stylegan_on_anime_faces/egmyf60/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/apq4xu/p_stylegan_on_anime_faces/egmyf60/ )">End­ing_­Cred­its</a> like­wise did trans­fer to <a href="https://en.wikipedia.org/wiki/Saber_%28Fate%2Fstay_night%29" class="docMetadata has-annotation spawns-popup">Saber</a> (<em><a href="https://en.wikipedia.org/wiki/Fate%2Fstay_night" class="docMetadata has-annotation spawns-popup">Fate/<wbr>s­tay night</a></em>), <em>n</em> = 4000. The re­sults look about as ex­pected given the sam­ple sizes and pre­vi­ous trans­fer re­sults:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-14-endingcredits-stylegan-sabertransfer-interpolation-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-14-endingcredits-stylegan-sabertransfer-interpolation-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Saber face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, trained by End­ing_­Cred­its
</figcaption></span></span></figure>
</section>
<section id="fategrand-order" class="level4">
<h4><a href="https://www.gwern.net/Faces#fategrand-order" title="Link to section: § &#39;Fate/Grand Order&#39;" class="no-popup"><em class="cyxy-trs-source">Fate/Grand Order</em></a></h4>
<p class="cyxy-trs-source"><a href="https://towardsdatascience.com/fgo-stylegan-this-heroic-spirit-doesnt-exist-23d62fbb680e" title="FGO StyleGAN: This Heroic Spirit Doesn&#39;t Exist" class="no-popup">Michael Sug­imura</a> in May 2019 ex­per­i­mented with trans­fer learn­ing from the 512px anime por­trait <span class="smallcaps-auto">GAN</span> to faces cropped from ~6k <a href="https://en.wikipedia.org/wiki/Fate%2FGrand_Order" class="docMetadata has-annotation spawns-popup"><em>Fate/<wbr>­Grand Or­der</em></a> wall­pa­pers he down­loaded via Google search queries. His re­sults for Saber &amp; re­lated char­ac­ters look rea­son­able but more broad­ly, some­what low-qual­i­ty, which Sug­imura sus­pects is due to in­ad­e­quate data clean­ing (“there are a num­ber of lower qual­ity im­ages and also im­ages of back­grounds, ar­mor, non-char­ac­ter im­ages left in the dataset which causes weird ar­ti­facts in gen­er­ated im­ages or just lower qual­ity gen­er­ated im­ages.”).</p>
</section>
</section>
<section id="louise" class="level3">
<h3><a href="https://www.gwern.net/Faces#louise" title="Link to section: § &#39;Louise&#39;" class="no-popup cyxy-trs-source">Louise</a></h3>
<p class="cyxy-trs-source">Fi­nal­ly, End­ing_­Cred­its did trans­fer to <a href="https://en.wikipedia.org/wiki/List_of_The_Familiar_of_Zero_characters#Louise" class="docMetadata has-annotation spawns-popup">Louise</a> (<em><a href="https://en.wikipedia.org/wiki/Zero_no_Tsukaima" class="docMetadata has-annotation spawns-popup">Zero no Tsukaima</a></em>), <em>n</em> = 350:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-14-endingcredits-stylegan-louisetransfer-interpolation-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-14-endingcredits-stylegan-louisetransfer-interpolation-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Louise face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, trained by End­ing_­Cred­its
</figcaption></span></span></figure>
<p class="cyxy-trs-source">Not as good as Saber due to the much smaller sam­ple size.</p>
</section>
<section id="lelouch" class="level3">
<h3><a href="https://www.gwern.net/Faces#lelouch" title="Link to section: § &#39;Lelouch&#39;" class="no-popup cyxy-trs-source">Lelouch</a></h3>
<p class="cyxy-trs-source">road­run­ner01 ex­per­i­mented with a num­ber of trans­fers, <a href="https://nitter.cc/roadrunning01/status/1097513035474845696" class="no-popup">in­clud­ing a trans­fer</a> of the male char­ac­ter <a href="https://en.wikipedia.org/wiki/Lelouch_Lamperouge" class="docMetadata has-annotation spawns-popup">Lelouch Lam­per­ouge</a> (<a href="https://en.wikipedia.org/wiki/Code_Geass" class="docMetadata has-annotation spawns-popup"><em>Code Ge­ass</em></a>) with <em>n</em> = 50 (!), which is not nearly as garbage as it should be.</p>
</section>
<section id="asashio" class="level3">
<h3><a href="https://www.gwern.net/Faces#asashio" title="Link to section: § &#39;Asashio&#39;" class="no-popup cyxy-trs-source">Asashio</a></h3>
<p class="cyxy-trs-source"><a href="https://nitter.cc/FlatIsNice/status/1112671357706424322" class="no-popup">Flatis­Dogchi</a> ex­per­i­mented with trans­fer to <em>n</em> = 988 (aug­mented to <em>n</em> = 18772) <a href="https://kancolle.fandom.com/wiki/Asashio" class="no-popup">Asashio</a> (Kan­Colle) faces, cre­at­ing <a href="https://flatisjustice.moe/TADNE" class="no-popup">“This Asashio Does Not Ex­ist”</a>⁠.</p>
</section>
<section id="marisa-kirisame-the-komeijis" class="level3">
<h3><a href="https://www.gwern.net/Faces#marisa-kirisame-the-komeijis" title="Link to section: § &#39;Marisa Kirisame &amp; the Komeijis&#39;" class="no-popup cyxy-trs-source">Marisa Kirisame &amp; the Komeijis</a></h3>
<p class="cyxy-trs-source">A Japan­ese user mei_miya posted <a href="https://nitter.cc/__meimiya__/status/1102679068242173952" class="no-popup">an in­ter­po­la­tion video</a> of the Touhou char­ac­ter Marisa Kirisame by <a href="https://nitter.cc/__meimiya__/status/1134441616477806592" class="no-popup">trans­fer learn­ing on 5000 faces</a>⁠. They also did the Touhou char­ac­ters Satori/<wbr>Koishi Komeiji <a href="https://nitter.cc/__meimiya__/status/1134751068758265856" class="no-popup">with <em>n</em> = 6000</a>⁠.</p>
<p class="cyxy-trs-source">The Red­dit user <a href="https://www.gwern.net/docs/www/old.reddit.com/43e45a1b61d4356630a440ff1b0ae798cb8052d3.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/touhou/comments/gl180j/here_have_a_few_marisa_portraits/" data-attribute-title="(Original URL: https://old.reddit.com/r/touhou/comments/gl180j/here_have_a_few_marisa_portraits/ )">Jepa­cor</a> also has <a href="https://imgur.com/a/NZzcMbC" title="A few Marisa portraits" class="no-popup">done Marisa</a>⁠, us­ing Dan­booru sam­ples.</p>
</section>
<section id="lexington" class="level3">
<h3><a href="https://www.gwern.net/Faces#lexington" title="Link to section: § &#39;Lexington&#39;" class="no-popup cyxy-trs-source">Lexington</a></h3>
<p class="cyxy-trs-source">A Chi­nese user <a href="https://nitter.cc/3D_DLW/status/1227313334237745155" class="no-popup">3D_<span class="smallcaps-auto">DLW</span></a> (S2 write­up/<wbr>­tu­to­ri­al: <a href="https://blog.csdn.net/DLW__/article/details/104222546" class="no-popup">1</a>⁠/ <a href="https://www.gwern.net/docs/www/blog.csdn.net/63e52b97e151c7175b36eccd6af13b7890d86f5a.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://blog.csdn.net/DLW__/article/details/104243506" data-attribute-title="(Original URL: https://blog.csdn.net/DLW__/article/details/104243506 )">2</a>) in Feb­ru­ary 2020 did trans­fer­-learn­ing from the S2 por­trait model to Pixiv art­work of the char­ac­ter Lex­ing­ton from <a href="https://tvtropes.org/pmwiki/pmwiki.php/VideoGame/WarshipGirls" class="no-popup"><em>War­ship Girls</em></a>⁠. He used a sim­i­lar work­flow: crop­ping faces with <code>lbpcascade_animeface</code>, up­scal­ing with wai­fu2x, and clean­ing with <code>ranker.py</code> (us­ing the orig­i­nal S2 mod­el’s Dis­crim­i­na­tor &amp; pro­duc­ing datasets of vary­ing clean­li­ness at <em>n</em> = 302–1659). Sam­ples:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Ran­dom sam­ples for anime por­trait S2 → War­ship Girls char­ac­ter Lex­ing­ton." height="741" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2020-02-08-3d_dlw-stylegan2-warshipgirls-lexington.webp" srcset="/images/gan/stylegan/2020-02-08-3d_dlw-stylegan2-warshipgirls-lexington.png-768px.png 768w, /images/gan/stylegan/2020-02-08-3d_dlw-stylegan2-warshipgirls-lexington.png 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Ran­dom sam­ples for anime por­trait S2 → <em class="cyxy-trs-source">War­ship Girls</em> char­ac­ter Lex­ing­ton.</figcaption></span></span></figure>
</section>
<section id="hayasaka-ai" class="level3">
<h3><a href="https://www.gwern.net/Faces#hayasaka-ai" title="Link to section: § &#39;Hayasaka Ai&#39;" class="no-popup cyxy-trs-source">Hayasaka Ai</a></h3>
<p class="cyxy-trs-source"><a href="https://nitter.cc/TazikShahjahan/status/1315441277236899842" title="Played around with @gwern&#39;s TWDNEv2 model to generate images of Hayasaka Ai! This is after ~9 hours of training (n = 300+). Stopped working on it after a bit, so a bunch of potential improvements. More thoughts here: https://github.com/ZKTKZ/thdne/bl" class="no-popup">Tazik Shah­ja­han</a> fine­tuned S2 on <a href="https://en.wikipedia.org/wiki/Kaguya-sama%3A_Love_Is_War" class="docMetadata has-annotation spawns-popup"><em>Kaguya-sama: Love Is War</em></a>’s Hayasaka Ai, pro­vid­ing a <a href="https://github.com/ZKTKZ/thdne/blob/master/StyleGAN2_Tazik_25GB_RAM.ipynb" class="no-popup">Co­lab note­book</a> demon­strat­ing how he scraped Pixiv and fil­tered out in­valid im­ages to cre­ate the train­ing cor­pus</p>
</section>
</section>
<section id="ahegao" class="level2 collapse">
<h2 class=""><a href="https://www.gwern.net/Faces#ahegao" title="Link to section: § &#39;Ahegao&#39;" class="no-popup cyxy-trs-source">Ahegao</a></h2><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into.">
<p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/www.deviantart.com/ef842508133a0b980110dff68c7fc829a46e9fa7.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.deviantart.com/caji9i/art/stylegan-neural-ahegao-842847987" data-attribute-title="(Original URL: https://www.deviantart.com/caji9i/art/stylegan-neural-ahegao-842847987 )">Ca­JI9I</a> cre­ated an “<a href="https://en.wikipedia.org/wiki/Ahegao" class="docMetadata has-annotation spawns-popup">ahe­gao</a>” Style<span class="smallcaps-auto">GAN</span>; un­spec­i­fied cor­pus or method:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="6×6 sam­ple of ahe­gao  faces" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 3072px" src="./一个StyleGAN动漫脸编辑详细教程_files/2020-05-22-caji9-deviantart-stylegan-ahegao.png" srcset="/images/gan/2020-05-22-caji9-deviantart-stylegan-ahegao.png-768px.png 768w, /images/gan/2020-05-22-caji9-deviantart-stylegan-ahegao.png 3072w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">6×6 sam­ple of ahe­gao Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> faces</figcaption></span></span></figure>
</section>
<section id="emilia-rezero" class="level2">
<h2><a href="https://www.gwern.net/Faces#emilia-rezero" title="Link to section: § &#39;Emilia (Re:Zero)&#39;" class="no-popup cyxy-trs-source">Emilia (<em class="cyxy-trs-source">Re:Zero</em>)</a></h2>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2020-12-30-shipblazer420-stylegan-rezeroemilia-interpolations.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2020-12-30-shipblazer420-stylegan-rezeroemilia-interpolations.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Emilia <a href="https://en.wikipedia.org/wiki/Re%3AZero_%E2%88%92_Starting_Life_in_Another_World" class="docMetadata has-annotation spawns-popup"><em class="cyxy-trs-source">Re:Zero</em></a> face model ini­tial­ized from the Por­trait Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, trained by <a href="https://old.reddit.com/r/Re_Zero/comments/kn26lc/mediaaigenerated_emilia_heads/ghhw1ej/" class="no-popup cyxy-trs-source">ship­blaz­er420</a>
</figcaption></span></span></figure>
</section>
<section id="anime-faces-anime-headshots" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces-anime-headshots" title="Link to section: § &#39;Anime Faces → Anime Headshots&#39;" class="no-popup cyxy-trs-source">Anime Faces → Anime Headshots</a></h2>
<p class="cyxy-trs-source">Twit­ter user <a href="https://nitter.cc/sunkworld/status/1100954144905543680" class="no-popup">Sunk</a> did trans­fer learn­ing to <a href="https://www.element3ds.com/thread-171119-1-1.html" class="no-popup">an im­age cor­pus</a> of a spe­cific artist, <a href="https://nitter.cc/misaki_cradle" class="no-popup">Kure­hito Mis­aki (深崎暮人)</a>⁠, <em>n</em>≅1000. His im­ages work well and the in­ter­po­la­tion looks nice:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-27-sunk-stylegan-misakikurehitotransfer-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-27-sunk-stylegan-misakikurehitotransfer-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Louise face model ini­tial­ized from the Kure­hito Mis­aki Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, trained by sunk
</figcaption></span></span></figure>
</section>
<section id="anime-faces-portrait" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces-portrait" title="Link to section: § &#39;Anime Faces → Portrait&#39;" class="no-popup cyxy-trs-source">Anime Faces → Portrait</a></h2>
<p class="cyxy-trs-source"><span class="smallcaps-auto">TWDNE</span> was a huge suc­cess and pop­u­lar­ized the anime face Style<span class="smallcaps-auto">GAN</span>. It was not per­fect, though, and flaws were not­ed.</p>
<section id="portrait-improvements" class="level3 cyxy-trs-source">
<h3><a href="https://www.gwern.net/Faces#portrait-improvements" title="Link to section: § &#39;Portrait Improvements&#39;" class="no-popup cyxy-trs-source">Portrait Improvements</a></h3>
<p class="cyxy-trs-source">The por­traits could be im­proved by more care­fully se­lect­ing <span class="smallcaps-auto">SFW</span> im­ages to avoid over­ly-sug­ges­tive faces, ex­pand­ing the crops to avoid cut­ting off edges of heads like hair­styles,</p>
**­For de­tails and
<div id="portrait-dataset" class="cyxy-trs-source">
down­loads
</div>
<p class="cyxy-trs-source">, please see <a href="https://www.gwern.net/Crops#danbooru2019-portraits" id="gwern-crops-danbooru2019-portraits" class="link-local docMetadata has-annotation spawns-popup">Dan­booru2019 Por­traits</a>⁠.**</p>
</section>
<section id="portrait-results" class="level3">
<h3><a href="https://www.gwern.net/Faces#portrait-results" title="Link to section: § &#39;Portrait Results&#39;" class="no-popup cyxy-trs-source">Portrait Results</a></h3>
<p class="cyxy-trs-source">After re­train­ing the fi­nal face Style<span class="smallcaps-auto">GAN</span> 2019-03-08–2019-04-30 on the new im­proved por­traits dataset, the re­sults im­proved:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Train­ing sam­ple for Por­trait   #66,083" height="800" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-04-30-stylegan-portrait-trainingsamples-66083.jpg" srcset="/images/gan/stylegan/2019-04-30-stylegan-portrait-trainingsamples-66083.jpg-768px.jpg 768w, /images/gan/stylegan/2019-04-30-stylegan-portrait-trainingsamples-66083.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Train­ing sam­ple for Por­trait Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>: 2019-04-30/<wbr>it­er­a­tion #66,083</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-04-30-stylegan-portraits-interpolation-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-04-30-stylegan-portraits-interpolation-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Dan­booru2018 por­trait model ini­tial­ized from the Dan­booru2017 face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>
</figcaption></span></span></figure>
<div class="admonition note">
<p class="cyxy-trs-source">This S1 anime por­trait model is ob­so­leted by the <a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-down has-content spawns-popup">Style<span class="smallcaps-auto">GAN</span> 2 por­trait model</a>⁠.</p>
</div>
<p class="cyxy-trs-source">The fi­nal model from 2019-04-30 is <a href="https://mega.nz/#!CRtiDI7S!xo4zm3n7pkq1Lsfmuio1O8QPpUwHrtFTHjNJ8_XxSJs" title="2019-04-30-stylegan-danbooru2018-portraits-02095-066083.pkl" class="no-popup">avail­able for down­load</a>⁠.</p>
<p class="cyxy-trs-source">I used this model at 𝛙 = 0.5 to gen­er­ate 100,000 new por­traits for <span class="smallcaps-auto">TWDNE</span> (#100,000–199,999), bal­anc­ing the pre­vi­ous faces.</p>
<p class="cyxy-trs-source">I was sur­prised how diffi­cult up­grad­ing to por­traits seemed to be; I spent al­most two months train­ing it be­fore giv­ing up on fur­ther im­prove­ments, while I had been ex­pect­ing more like a week or two. The por­trait re­sults are in­deed bet­ter than the faces (I was right that not crop­ping off the top of the head adds verisimil­i­tude), but the up­grade did­n’t im­press me as much as the orig­i­nal faces did com­pared to ear­lier <span class="smallcaps-auto">GAN</span>s. And our other ex­per­i­men­tal runs on whole-Dan­booru2018 im­ages never pro­gressed be­yond sug­ges­tive blobs dur­ing this pe­ri­od.</p>
<p class="cyxy-trs-source">I sus­pect that Style<span class="smallcaps-auto">GAN</span>—at least, on its de­fault ar­chi­tec­ture &amp; hy­per­pa­ra­me­ters, with­out a great deal more com­pute—is reach­ing its lim­its here, and that changes may be nec­es­sary to scale to richer im­ages. (Self-at­ten­tion is prob­a­bly the eas­i­est to add since it should be easy to plug in ad­di­tional lay­ers to the con­vo­lu­tion code.)</p>
</section>
</section>
<section id="anime-faces-male-faces" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces-male-faces" title="Link to section: § &#39;Anime Faces → Male Faces&#39;" class="no-popup cyxy-trs-source">Anime Faces → Male Faces</a></h2>
<p class="cyxy-trs-source">A few peo­ple have ob­served that it would be nice to have an anime face <span class="smallcaps-auto">GAN</span> for <em>male</em> char­ac­ters in­stead of al­ways gen­er­at­ing fe­male ones. The anime face Style<span class="smallcaps-auto">GAN</span> does in fact have male faces in its dataset as I did no fil­ter­ing—it’s merely that fe­male faces are over­whelm­ingly fre­quent (and it may also be that male anime faces are rel­a­tively an­drog­y­nous/<wbr>fem­i­nized any­way so it’s hard to tell any differ­ence be­tween a fe­male with short hair &amp; a guy<a href="https://www.gwern.net/Faces#sn37" class="footnote-ref spawns-popup" id="fnref37" role="doc-noteref"><sup>37</sup></a>).</p>
<p class="cyxy-trs-source">Train­ing a male-only anime face Style<span class="smallcaps-auto">GAN</span> would be an­other good ap­pli­ca­tion of trans­fer learn­ing.</p>
<p class="cyxy-trs-source">The faces can be eas­ily ex­tracted out of Dan­booru2018 by query­ing for <code>"male_focus"</code>, which will pick up ~150k im­ages. More nar­row­ly, one could search <code>"1boy"</code> &amp; <code>"solo"</code>, to en­sure that the only face in the im­age is a male face (as op­posed to, say, <code>1boy 1girl</code>, where a fe­male face might be cropped out as well). This pro­vides <em>n</em> = 99k raw hits. It would be good to also fil­ter out ‘trap’ or over­ly-fe­male-look­ing faces (else what’s the point?), by fil­ter­ing on tags like cat ears or par­tic­u­larly pop­u­lar ‘trap’ char­ac­ters like <em>Fate/<wbr>­Grand Or­der</em>’s As­tol­fo. A more com­pli­cated query to pick up scenes with mul­ti­ple males could be to search for both <code>"1boy"</code> &amp; <code>"multiple_boys"</code> and then fil­ter out <code>"1girl"</code> &amp; <code>"multiple_girls"</code>, in or­der to se­lect all im­ages with 1 or more males and then re­move all im­ages with 1 or more fe­males; this dou­bles the raw hits to <em>n</em> = 198k. (A down­side is that the face-crop­ping will often un­avoid­ably yield crops with two faces, a pri­mary face and an over­lap­ping face, which is bad and in­tro­duces ar­ti­fact­ing when I tried this with all faces.)</p>
<p class="cyxy-trs-source">Com­bined with trans­fer learn­ing from the gen­eral anime face Style<span class="smallcaps-auto">GAN</span>, the re­sults should be as good as the gen­eral (fe­male) faces.</p>
<p class="cyxy-trs-source">I set­tled for <code>"1boy"</code> &amp; <code>"solo"</code>, and did con­sid­er­able clean­ing by hand. The raw count of im­ages turned out to be highly mis­lead­ing, and many faces are un­us­able for a male anime face Style<span class="smallcaps-auto">GAN</span>: many are so highly styl­ized (such as ac­tion sce­nes) as to be dam­ag­ing to a <span class="smallcaps-auto">GAN</span>, or they are al­most in­dis­tin­guish­able from fe­male faces (be­cause they are bis­honen or trap or just an­drog­y­nous), which would be point­less to in­clude (the reg­u­lar por­trait Style<span class="smallcaps-auto">GAN</span> cov­ers those al­ready). After hand clean­ing &amp; <a href="https://www.gwern.net/Faces#discriminator-ranking" class="link-self identifier-link-up has-content spawns-popup">use of <code>ranker.py</code></a>⁠, I was left with <em>n</em>~3k, so I used heavy data aug­men­ta­tion to bring it up to <em>n</em>~57k, and I ini­tial­ized from the fi­nal por­trait Style<span class="smallcaps-auto">GAN</span> for the high­est qual­i­ty.</p>
<p class="cyxy-trs-source">It did not over­fit after ~4 days of train­ing, but the re­sults were not no­tice­ably im­prov­ing, so I stopped (in or­der to start train­ing the <span class="smallcaps-auto">GPT-2-345M</span>, which Ope­nAI had just re­leased, <a href="https://www.gwern.net/GPT-2" id="gwern-gpt-2" class="link-local docMetadata has-annotation spawns-popup">on po­etry</a>). There are hints in the in­ter­po­la­tion videos, I think, that it is in­deed slightly over­fit­ting, in the form of ‘glitches’ where the im­age abruptly jumps slight­ly, pre­sum­ably to an­other mod­e/<wbr>­face/<wbr>char­ac­ter of the orig­i­nal data; nev­er­the­less, the male face Style<span class="smallcaps-auto">GAN</span> mostly works.</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Train­ing sam­ples for the male por­trait  (2019-05-03); com­pare with the same la­ten­t-space points in the orig­i­nal por­trait " height="800" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-05-03-stylegan-malefaces-trainingsamples.webp" srcset="/images/gan/stylegan/2019-05-03-stylegan-malefaces-trainingsamples.png-768px.png 768w, /images/gan/stylegan/2019-05-03-stylegan-malefaces-trainingsamples.png 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Train­ing sam­ples for the male por­trait Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> (2019-05-03); com­pare with the same la­ten­t-space points in the orig­i­nal por­trait Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>.</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-05-06-stylegan-malefaces-interpolation-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-05-06-stylegan-malefaces-interpolation-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the Dan­booru2018 male faces model ini­tial­ized from the Dan­booru2018 por­trait Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>
</figcaption></span></span></figure>
<p class="cyxy-trs-source">The male face Style<span class="smallcaps-auto">GAN</span> model is <a href="https://mega.nz/#!fMNDkYwS!X-7_nBtsC6P_09CINIJAoVqR3V8Ffbv5On74rVoUbik" title="2019-05-03-stylegan-malefaces-02107-069770.pkl" class="no-popup">avail­able for down­load</a>⁠, as is <a href="https://www.gwern.net/docs/anime/2019-05-06-stylegan-malefaces-1ksamples.tar" class="no-popup">1000 ran­dom faces with 𝛙 = 0.7</a> (<a href="https://mega.nz/#!OEFjWKAS!QIqbb38fR5PnIZbdr7kx5K-koEMtOQ_XQXRqppAyv-k" class="no-popup">mir­ror</a>⁠; <a href="https://imgur.com/a/L7BSnkm" class="no-popup">par­tial Imgur al­bum</a>).</p>
</section>
<section id="anime-faces-ukiyo-e-faces" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces-ukiyo-e-faces" title="Link to section: § &#39;Anime Faces → Ukiyo-e Faces&#39;" class="no-popup cyxy-trs-source cyxy-trs-source">Anime Faces → <em class="cyxy-trs-source">Ukiyo-e</em> Faces</a></h2>
<p class="cyxy-trs-source">In Jan­u­ary 2020, <a href="https://nitter.cc/Buntworthy/status/1213402237269159936" class="no-popup">Justin (@Bunt­wor­thy)</a> used 5000 <a href="https://en.wikipedia.org/wiki/Ukiyo-e" class="docMetadata has-annotation spawns-popup"><em>ukiy­o-e</em></a> faces cropped with <a href="https://en.wikipedia.org/wiki/Amazon_Rekognition" class="docMetadata has-annotation spawns-popup">Ama­zon Rekog­ni­tion</a> from <a href="https://www.gwern.net/docs/www/ukiyo-e.org/967b9677f986b05fd3ed594230c91f247b822099.html" id="resig-2013" class="docMetadata localArchive has-annotation spawns-popup" rel="archived alternate nofollow" data-url-original="https://ukiyo-e.org/" data-attribute-title="Japanese Woodblock Print Search: Ukiyo-e Search provides an incredible resource: The ability to both search for Japanese woodblock prints by simply taking a picture of an existing print AND the ability to see similar prints across multiple collections of prints. Below is an example print, click to see it in action. (Original URL: https://ukiyo-e.org/ )">Ukiy­o-e Search</a> to do trans­fer learn­ing. After ~24h train­ing:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Justin’s ukiy­o-e  sam­ples, 2020-01-04." height="799" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2020-01-04-justin-stylegan-transferlearning-ukiyoe.jpg" srcset="/images/gan/stylegan/2020-01-04-justin-stylegan-transferlearning-ukiyoe.jpg-768px.jpg 768w, /images/gan/stylegan/2020-01-04-justin-stylegan-transferlearning-ukiyoe.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Justin’s <em class="cyxy-trs-source">ukiy­o-e</em> Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> sam­ples, 2020-01-04.</figcaption></span></span></figure>
</section>
<section id="anime-faces-western-portrait-faces" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces-western-portrait-faces" title="Link to section: § &#39;Anime Faces → Western Portrait Faces&#39;" class="no-popup cyxy-trs-source">Anime Faces → Western Portrait Faces</a></h2>
<p class="cyxy-trs-source">In 2019, <a href="https://nitter.cc/AydaoGMan/status/1217276442230378497" class="no-popup">ay­dao ex­per­i­mented with</a> trans­fer learn­ing to Eu­ro­pean por­trait faces drawn from <a href="https://github.com/cs-chan/ArtGAN/tree/master/WikiArt%20Dataset" class="no-popup">WikiArt</a>⁠; the trans­fer learn­ing was done via <a href="https://www.gwern.net/docs/www/www.nathanshipley.com/3faf4a89ab1795dc5307ad689ae1816951aa61e5.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://www.nathanshipley.com/gan" data-attribute-title="GAN Explorations 011: StyleGAN2 + Stochastic Weight Averaging (Original URL: http://www.nathanshipley.com/gan )">Nathan Ship­ley’s</a> abuse of <a href="https://arxiv.org/abs/1803.05407" id="izmailov-et-al-2018-3" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Averaging Weights Leads to Wider Optima and Better Generalization&#39;, Izmailov et al 2018"><span class="smallcaps-auto">SWA</span></a> where two mod­els are sim­ply av­er­aged to­geth­er, pa­ra­me­ter by pa­ra­me­ter and layer by lay­er, to yield a new mod­el. (Sur­pris­ing­ly, this work­s—as long as the mod­els aren’t <em>too</em> differ­ent; if they are, the av­er­aged model will gen­er­ate only col­or­ful blob­s.) The re­sults were amus­ing. From early in train­ing:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="ay­dao 2019, anime faces → west­ern por­trait train­ing sam­ples (ear­ly)" height="1032" loading="lazy" sizes="(max-width: 768px) 100vw, 1032px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-aydao-stylegan-transferlearning-westernportraitfaces-1.webp" srcset="/images/gan/stylegan/2019-aydao-stylegan-transferlearning-westernportraitfaces-1.png-768px.png 768w, /images/gan/stylegan/2019-aydao-stylegan-transferlearning-westernportraitfaces-1.png 1032w" width="1032" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">ay­dao 2019, anime faces → west­ern por­trait train­ing sam­ples (ear­ly)</figcaption></span></span></figure>
<p class="cyxy-trs-source">Lat­er:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="ay­dao 2019, anime faces → west­ern por­trait train­ing sam­ples (later)" height="1032" loading="lazy" sizes="(max-width: 768px) 100vw, 1032px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-aydao-stylegan-transferlearning-westernportraitfaces-2.webp" srcset="/images/gan/stylegan/2019-aydao-stylegan-transferlearning-westernportraitfaces-2.png-768px.png 768w, /images/gan/stylegan/2019-aydao-stylegan-transferlearning-westernportraitfaces-2.png 1032w" width="1032" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source">ay­dao 2019, anime faces → west­ern por­trait train­ing sam­ples (later)</figcaption></span></span></figure>
</section>
<section id="anime-faces-danbooru2018" class="level2">
<h2><a href="https://www.gwern.net/Faces#anime-faces-danbooru2018" title="Link to section: § &#39;Anime Faces → Danbooru2018&#39;" class="no-popup cyxy-trs-source">Anime Faces → Danbooru2018</a></h2>
<p class="cyxy-trs-source">nshep­perd be­gan <a href="https://www.gwern.net/docs/www/zlkj.in/90364ee19f19da6b67727d7fec827fbfee056df4.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://zlkj.in/tmp/stylegan/00051-sgan-danbooru-512px-1gpu-progan/" data-attribute-title="(Original URL: https://zlkj.in/tmp/stylegan/00051-sgan-danbooru-512px-1gpu-progan/ )">a train­ing run</a> us­ing an early anime face Style<span class="smallcaps-auto">GAN</span> model on the 512px <span class="smallcaps-auto">SFW</span> Dan­booru2018 sub­set; after ~3–5 weeks (with many in­ter­rup­tions) on 1 <span class="smallcaps-auto">GPU</span>, as of 2019-03-22, the train­ing sam­ples look like this:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt=" train­ing sam­ples on Dan­booru2018  512px; it­er­a­tion #14204 (n­shep­perd)" height="1024" loading="lazy" sizes="(max-width: 768px) 100vw, 1024px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-22-stylegan-danbooru2018-nshepperd-trainingsample-014204.jpg" srcset="/images/gan/stylegan/2019-03-22-stylegan-danbooru2018-nshepperd-trainingsample-014204.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-22-stylegan-danbooru2018-nshepperd-trainingsample-014204.jpg 1024w" width="1024" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> train­ing sam­ples on Dan­booru2018 <span class="smallcaps-auto cyxy-trs-source">SFW</span> 512px; it­er­a­tion #14204 (n­shep­perd)</figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Real 512px  Dan­booru2018 train­ing dat­a­points, for com­par­i­son" height="800" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-23-danbooru2018-sfw-512px-trainingsamples.jpg" srcset="/images/gan/2019-03-23-danbooru2018-sfw-512px-trainingsamples.jpg-768px.jpg 768w, /images/gan/2019-03-23-danbooru2018-sfw-512px-trainingsamples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Real 512px <span class="smallcaps-auto cyxy-trs-source">SFW</span> Dan­booru2018 train­ing dat­a­points, for com­par­i­son</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-22-stylegan-danbooru2018-nshepperd-trainingmontage.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-03-22-stylegan-danbooru2018-nshepperd-trainingmontage.mp4" class="has-content spawns-popup cyxy-trs-source">Train­ing mon­tage video</a> of the Dan­booru2018 model (up to #14204, 2019-03-22), trained by nshep­perd
</figcaption></span></span></figure>
<p class="cyxy-trs-source">The Style<span class="smallcaps-auto">GAN</span> is able to pick up global struc­ture and there are rec­og­niz­ably anime fig­ures, de­spite the sheer di­ver­sity of im­ages, which is promis­ing. The fine de­tails are se­ri­ously lack­ing, and train­ing, to my eye, is wan­der­ing around with­out any steady im­prove­ment or sharp de­tails (ex­cept per­haps the faces which are in­her­ited from the pre­vi­ous mod­el). I sus­pect that the learn­ing rate is still too high and, es­pe­cially with only 1 <span class="smallcaps-auto">GPU</span>/<em>n</em> = 4, such small mini­batches don’t cover enough modes to en­able steady im­prove­ment. If so, the LR will need to be set much lower (or gra­di­ent ac­cu­mu­la­tion used in or­der to fake hav­ing large mini­batches where large LRs are sta­ble) &amp; train­ing time ex­tended to mul­ti­ple months. An­other pos­si­bil­ity would be to restart with added self­-at­ten­tion lay­ers, which I have no­ticed seem to par­tic­u­larly help with com­pli­cated de­tails &amp; sharp­ness; the style noise ap­proach may be ad­e­quate for the job but just a few vanilla con­vo­lu­tion lay­ers may be too few (pace the Big<span class="smallcaps-auto">GAN</span> re­sults on the ben­e­fits of in­creas­ing depth while de­creas­ing pa­ra­me­ter coun­t).</p>
</section>
<section id="ffhq-variations" class="level2">
<h2><a href="https://www.gwern.net/Faces#ffhq-variations" title="Link to section: § &#39;FFHQ Variations&#39;" class="no-popup cyxy-trs-source">FFHQ Variations</a></h2>
<section id="anime-faces-ffhq-faces" class="level3">
<h3><a href="https://www.gwern.net/Faces#anime-faces-ffhq-faces" title="Link to section: § &#39;Anime Faces → FFHQ Faces&#39;" class="no-popup cyxy-trs-source">Anime Faces → FFHQ Faces</a></h3>
<p class="cyxy-trs-source">If Style<span class="smallcaps-auto">GAN</span> can smoothly warp anime faces among each other and ex­press global trans­forms like hair length­+­color with Ψ, could Ψ be a quick way to gain con­trol over a sin­gle large-s­cale vari­able? For ex­am­ple, male vs fe­male faces, or… anime ↔︎ real faces? (Given a par­tic­u­lar im­age/<wbr>la­tent vec­tor, one would sim­ply flip the sign to con­vert it to the op­po­site; this would give the op­po­site ver­sion of each ran­dom face, and if one had an en­coder, one could do au­to­mat­i­cally ani­me-fy or re­al-fy an ar­bi­trary face by en­cod­ing it into the la­tent vec­tor which cre­ates it, and then flip­ping.<a href="https://www.gwern.net/Faces#sn38" class="footnote-ref spawns-popup" id="fnref38" role="doc-noteref"><sup>38</sup></a>)</p>
<p class="cyxy-trs-source">Since Kar­ras&nbsp;et&nbsp;al&nbsp;2801 pro­vide a nice <span class="smallcaps-auto">FFHQ</span> down­load script (al­beit slower than I’d like once Google Drive rate-lim­its you a wall­clock hour into the full down­load) for the ful­l-res­o­lu­tion <span class="smallcaps-auto">PNG</span>s, it would be easy to down­scale to 512px and cre­ate a 512px <span class="smallcaps-auto">FFHQ</span> dataset to train on, or even cre­ate a <em>com­bined</em> anime+<span class="smallcaps-auto">FFHQ</span> dataset.</p>
<p class="cyxy-trs-source">The first and fastest thing was to do trans­fer learn­ing from the anime faces to <span class="smallcaps-auto">FFHQ</span> real faces. It was un­likely that the model would re­tain much anime knowl­edge &amp; be able to do mor­ph­ing, but it was worth a try.</p>
<p class="cyxy-trs-source">The ini­tial re­sults early in train­ing are <em>hi­lar­i­ous</em> and look like zom­bies:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Ran­dom train­ing sam­ples of anime face →   trans­fer learn­ing, show­ing bizarrely-arte­fac­tual in­ter­me­di­ate faces" height="800" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-15-stylegan-animefaces-ffhqtransfer.jpg" srcset="/images/gan/stylegan/2019-02-15-stylegan-animefaces-ffhqtransfer.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-15-stylegan-animefaces-ffhqtransfer.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Ran­dom train­ing sam­ples of anime face → <span class="smallcaps-auto cyxy-trs-source">FFHQ</span>-only Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> trans­fer learn­ing, show­ing bizarrely-arte­fac­tual in­ter­me­di­ate faces</figcaption></span></span></figure>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/2019-02-15-ffhqonly-interpolation-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/2019-02-15-ffhqonly-interpolation-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the <span class="smallcaps-auto cyxy-trs-source">FFHQ</span> face model ini­tial­ized from the anime face Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, a few ticks into train­ing, show­ing bizarre ar­ti­facts
</figcaption></span></span></figure>
<p class="cyxy-trs-source">After 97 ticks, the model has con­verged to a bor­ingly nor­mal ap­pear­ance, with the only hint of its ori­gins be­ing per­haps some ex­ces­sive­ly-fab­u­lous hair in the train­ing sam­ples:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Anime faces →   train­ing sam­ples after much con­ver­gence, show­ing ani­me-ness largely washed out" height="800" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-15-stylegan-animefaces-ffhqtransfer-converged.jpg" srcset="/images/gan/stylegan/2019-02-15-stylegan-animefaces-ffhqtransfer-converged.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-15-stylegan-animefaces-ffhqtransfer-converged.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Anime faces → <span class="smallcaps-auto cyxy-trs-source">FFHQ</span>-only Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> train­ing sam­ples after much con­ver­gence, show­ing ani­me-ness largely washed out</figcaption></span></span></figure>
</section>
<section id="anime-faces-anime-faces-ffhq-faces" class="level3">
<h3><a href="https://www.gwern.net/Faces#anime-faces-anime-faces-ffhq-faces" title="Link to section: § &#39;Anime Faces → Anime Faces + FFHQ Faces&#39;" class="no-popup cyxy-trs-source">Anime Faces → Anime Faces + FFHQ Faces</a></h3>
<p class="cyxy-trs-source">So, that was a bust. The next step is to try train­ing on anime &amp; <span class="smallcaps-auto">FFHQ</span> faces si­mul­ta­ne­ous­ly; given the stark differ­ence be­tween the datasets, would pos­i­tive vs neg­a­tive Ψ wind up split­ting into real vs anime and pro­vide a cheap &amp; easy way of con­vert­ing ar­bi­trary faces?</p>
<p class="cyxy-trs-source">This sim­ply merged the 512px <span class="smallcaps-auto">FFHQ</span> faces with the 512px anime faces and re­sumed train­ing from the pre­vi­ous <span class="smallcaps-auto">FFHQ</span> model (I rea­soned that some of the ani­me-ness should still be in the mod­el, so it would be slightly faster than restart­ing from the orig­i­nal anime face mod­el). I trained it for 812 it­er­a­tions, #11,359–12,171 (some­what over 2 <span class="smallcaps-auto">GPU</span>-days), at which point it was mostly done.</p>
<p class="cyxy-trs-source">It did man­age to learn both kinds of faces quite well, sep­a­rat­ing them clearly in ran­dom sam­ples:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Ran­dom train­ing sam­ples,  " height="800" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-02-16-stylegan-ffhqdanbooru-trainingsample.jpg" srcset="/images/gan/stylegan/2019-02-16-stylegan-ffhqdanbooru-trainingsample.jpg-768px.jpg 768w, /images/gan/stylegan/2019-02-16-stylegan-ffhqdanbooru-trainingsample.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Ran­dom train­ing sam­ples, anime+<span class="smallcaps-auto cyxy-trs-source">FFHQ</span> Style<span class="smallcaps-auto cyxy-trs-source">GAN</span></figcaption></span></span></figure>
<p class="cyxy-trs-source">How­ev­er, the style trans­fer &amp; Ψ sam­ples were dis­ap­point­ments. The style mix­ing shows lim­ited abil­ity to mod­ify faces cross-do­main or con­vert them, and the trun­ca­tion trick chart shows no clear dis­en­tan­gle­ment of the de­sired fac­tor (in­deed, the var­i­ous halves of Ψ cor­re­spond to noth­ing clear):</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Style mix­ing re­sults for the  " height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1200px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-22-stylegan-ffhqdanbooru-figure03-stylemixing.jpg" srcset="/images/gan/stylegan/2019-03-22-stylegan-ffhqdanbooru-figure03-stylemixing.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-22-stylegan-ffhqdanbooru-figure03-stylemixing.jpg 1200w" width="1200" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Style mix­ing re­sults for the anime+<span class="smallcaps-auto cyxy-trs-source">FFHQ</span> Style<span class="smallcaps-auto cyxy-trs-source">GAN</span></figcaption></span></span></figure>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Trun­ca­tion trick re­sults for the  " height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1120px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-22-stylegan-ffhqdanbooru-figure08-truncationtrick.jpg" srcset="/images/gan/stylegan/2019-03-22-stylegan-ffhqdanbooru-figure08-truncationtrick.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-22-stylegan-ffhqdanbooru-figure08-truncationtrick.jpg 1120w" width="1120" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">Trun­ca­tion trick re­sults for the anime+<span class="smallcaps-auto cyxy-trs-source">FFHQ</span> Style<span class="smallcaps-auto cyxy-trs-source">GAN</span></figcaption></span></span></figure>
<p class="cyxy-trs-source">The in­ter­po­la­tion video <em>does</em> show that it learned to in­ter­po­late slightly be­tween real &amp; anime faces, giv­ing half-anime/<wbr>half-real faces, but it looks like it only hap­pens some­times—­mostly with young fe­male faces<a href="https://www.gwern.net/Faces#sn39" class="footnote-ref spawns-popup" id="fnref39" role="doc-noteref"><sup>39</sup></a>:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-02-22-stylegan-ffhqdanbooru-interpolation-4x4.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-02-22-stylegan-ffhqdanbooru-interpolation-4x4.mp4" class="has-content spawns-popup cyxy-trs-source">In­ter­po­la­tion video</a> (4×4) of the <span class="smallcaps-auto cyxy-trs-source">FFHQ</span>+anime face mod­el, after con­ver­gence.
</figcaption></span></span></figure>
<p class="cyxy-trs-source">They’re hard to spot in the in­ter­po­la­tion video be­cause the tran­si­tion hap­pens abrupt­ly, so I gen­er­ated sam­ples &amp; se­lected some of the more in­ter­est­ing ani­me-ish faces:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="Se­lected sam­ples from the   show­ing cu­ri­ous ‘in­ter­me­di­ate’ faces (4×4 grid)" height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2019-03-22-stylegan-ffhqdanbooru-intermediatesamples-4x4.jpg" srcset="/images/gan/stylegan/2019-03-22-stylegan-ffhqdanbooru-intermediatesamples-4x4.jpg-768px.jpg 768w, /images/gan/stylegan/2019-03-22-stylegan-ffhqdanbooru-intermediatesamples-4x4.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">Se­lected sam­ples from the anime+<span class="smallcaps-auto cyxy-trs-source">FFHQ</span> Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, show­ing cu­ri­ous ‘in­ter­me­di­ate’ faces (4×4 grid)</figcaption></span></span></figure>
<p class="cyxy-trs-source">Sim­i­lar­ly, <a href="https://www.youtube.com/watch?v=XDWua850n54" class="has-content spawns-popup" data-attribute-title="http://www.areben.com: styleGAN model trained on mixed real and drawn faces">Alexan­der Reben</a> trained a Style<span class="smallcaps-auto">GAN</span> on <span class="smallcaps-auto">FFHQ</span>+Western por­trait il­lus­tra­tions, and the in­ter­po­la­tion video is much smoother &amp; more mixed, sug­gest­ing that more re­al­is­tic &amp; more var­ied il­lus­tra­tions are eas­ier for Style<span class="smallcaps-auto">GAN</span> to in­ter­po­late be­tween.</p>
</section>
<section id="anime-faces-ffhq-danbooru2018" class="level3">
<h3><a href="https://www.gwern.net/Faces#anime-faces-ffhq-danbooru2018" title="Link to section: § &#39;Anime Faces + FFHQ → Danbooru2018&#39;" class="no-popup cyxy-trs-source">Anime Faces + FFHQ → Danbooru2018</a></h3>
<p class="cyxy-trs-source">While I did­n’t have the com­pute to prop­erly train a Dan­booru2018 Style<span class="smallcaps-auto">GAN</span>, after <a href="https://www.gwern.net/Faces#anime-faces-danbooru2018" class="link-self identifier-link-up has-content spawns-popup">nshep­perd’s re­sults</a>⁠, I was cu­ri­ous and spent some time (817 it­er­a­tions, so ~2 <span class="smallcaps-auto">GPU</span>-days?) re­train­ing the anime face+<span class="smallcaps-auto">FFHQ</span> model on Dan­booru2018 <span class="smallcaps-auto">SFW</span> 512px im­ages.</p>
<p class="cyxy-trs-source">The train­ing mon­tage is in­ter­est­ing for show­ing how faces get re­pur­posed into fig­ures:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-23-stylegan-danbooru2018transfer.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">
<a href="https://www.gwern.net/images/gan/stylegan/2019-03-23-stylegan-danbooru2018transfer.mp4" class="has-content spawns-popup cyxy-trs-source">Train­ing mon­tage video</a> of a Dan­booru2018 Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> ini­tial­ized on an anime faces+<span class="smallcaps-auto cyxy-trs-source">FFHQ</span> Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>.
</figcaption></span></span></figure>
<p class="cyxy-trs-source">One might think that it is a bridge too far for trans­fer learn­ing, but it seems not.</p>
</section>
</section>
</section>
<section id="reversing-stylegan-to-control-modify-images" class="level1">
<h1><a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images" title="Link to section: § &#39;Reversing StyleGAN To Control &amp; Modify Images&#39;" class="no-popup cyxy-trs-source">Reversing StyleGAN To Control &amp; Modify Images</a></h1>
<p class="cyxy-trs-source">Mod­i­fy­ing im­ages is harder than gen­er­at­ing them. If we had a con­di­tional anime face <span class="smallcaps-auto">GAN</span> like <a href="https://www.gwern.net/Faces#conditional-anime-faces-arfafax" class="link-self identifier-link-up has-content spawns-popup">Ar­fafax’s</a>⁠, then we are fine, but if we have an un­con­di­tional ar­chi­tec­ture of some sort, then what? An un­con­di­tional <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture is, by de­fault, ‘one-way’: the la­tent vec­tor <em>z</em> gets gen­er­ated from a bunch of 𝒩(0,1) vari­ables, fed through the <span class="smallcaps-auto">GAN</span>, and out pops an im­age. There is no way to run the un­con­di­tional <span class="smallcaps-auto">GAN</span> ‘back­wards’ to feed in an im­age and pop out the <em>z</em> in­stead.<a href="https://www.gwern.net/Faces#sn40" class="footnote-ref spawns-popup" id="fnref40" role="doc-noteref"><sup>40</sup></a></p>
<p class="cyxy-trs-source">If one could, one could take an ar­bi­trary im­age and en­code it into the <em>z</em> and by jit­ter­ing <em>z</em>, gen­er­ate many new ver­sions of it; or one could feed it back into Style<span class="smallcaps-auto">GAN</span> and play with the style noises at var­i­ous lev­els in or­der to trans­form the im­age; or do things like ‘av­er­age’ two im­ages or cre­ate in­ter­po­la­tions be­tween two ar­bi­trary faces’; or one could (as­sum­ing one knew what each vari­able in <em>z</em> ‘means’) edit the im­age to changes things like <a href="https://nitter.cc/pbaylies/status/1136307166695108609" class="no-popup">which di­rec­tion their head tilts</a> or whether they are smil­ing.</p>
<p class="cyxy-trs-source">There are some at­tempts at learn­ing con­trol in an un­su­per­vised fash­ion (eg <a href="https://arxiv.org/abs/2002.03754" id="voynov-babenko-2020-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="Unsupervised Discovery of Interpretable Directions in the GAN Latent Space">Voynov &amp; Babenko 2020</a>⁠, <a href="https://github.com/harskish/ganspace" class="no-popup"><span class="smallcaps-auto">GANS</span>pace</a>) but while ex­cel­lent start­ing points, they have lim­its and may not find a spe­cific con­trol that one wants.</p>
<p class="cyxy-trs-source">The most straight­for­ward way would be to switch to a <em>con­di­tional</em> <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture based on a text or tag em­bed­ding. Then to gen­er­ate a spe­cific char­ac­ter wear­ing glass­es, one sim­ply says as much as the con­di­tional in­put: <code>"character glasses"</code>. Or if they should be smil­ing, add <code>"smile"</code>. And so on. This would cre­ate im­ages of said char­ac­ter with the de­sired mod­i­fi­ca­tions. This op­tion is not avail­able at the mo­ment as cre­at­ing a tag em­bed­ding &amp; train­ing Style<span class="smallcaps-auto">GAN</span> re­quires quite a bit of mod­i­fi­ca­tion. It also is not a com­plete so­lu­tion as it would­n’t work for the cases of edit­ing an ex­ist­ing im­age.</p>
<p class="cyxy-trs-source">For an un­con­di­tional <span class="smallcaps-auto">GAN</span>, there are two com­ple­men­tary ap­proaches to in­vert­ing the G:</p>
<ol type="1">
<li><p class="cyxy-trs-source">what one NN can learn to de­code, an­other can learn to <strong>en­code</strong> (eg <a href="https://arxiv.org/abs/1605.09782" id="donahue-et-al-2016-5" class="docMetadata has-annotation spawns-popup" data-attribute-title="Adversarial Feature Learning">Don­ahue&nbsp;et&nbsp;al&nbsp;2016</a>⁠, <a href="https://arxiv.org/abs/1907.02544" id="donahue-simonyan-2019-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="Large Scale Adversarial Representation Learning">Don­ahue &amp; Si­monyan 2019</a>):</p>
<p class="cyxy-trs-source">If Style<span class="smallcaps-auto">GAN</span> has learned <em>z</em>→im­age, then train a sec­ond en­coder NN on the su­per­vised learn­ing prob­lem of im­age→<em>z</em>! The sam­ple size is in­fi­nite (just keep run­ning G) and the map­ping is fixed (given a fixed G), so it’s ugly but not that hard.</p></li>
<li><p class="cyxy-trs-source"><strong>back­prop­a­gate</strong> a pixel or fea­ture-level loss to ‘op­ti­mize’ a la­tent code (eg <a href="https://arxiv.org/abs/1802.05701" id="creswell-bharath-2018-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="Inverting The Generator Of A Generative Adversarial Network (II)">Creswell &amp; Bharath 2018</a>):</p>
<p class="cyxy-trs-source">While Style<span class="smallcaps-auto">GAN</span> is not in­her­ently re­versible, it’s not a black­box as, be­ing a NN trained by <a href="https://en.wikipedia.org/wiki/Backpropagation" class="docMetadata has-annotation spawns-popup">back­prop­a­ga­tion</a>⁠, it must ad­mit of gra­di­ents. In train­ing neural net­works, there are 3 com­po­nents: in­puts, model pa­ra­me­ters, and out­put­s/<wbr>loss­es, and thus there are <em>3</em> ways to use back­prop­a­ga­tion, even if we usu­ally only use 1. One can hold the in­puts fixed, and vary the model pa­ra­me­ters in or­der to change (usu­ally re­duce) the fixed out­puts in or­der to re­duce a loss, which is train­ing a NN; one can hold the in­puts fixed and vary the out­puts in or­der to change (often in­crease) in­ter­nal pa­ra­me­ters such as lay­ers, which cor­re­sponds to neural net­work vi­su­al­iza­tions &amp; ex­plo­ration; and fi­nal­ly, one can hold the pa­ra­me­ters &amp; out­puts fixed, and use the gra­di­ents to it­er­a­tively find a set of <em>in­puts</em> which cre­ates a spe­cific out­put with a low loss (eg <a href="https://www.gwern.net/docs/www/blog.benwiener.com/34fd4a754dcdaa50d2abfc8053835c88e72a7278.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://blog.benwiener.com/programming/2019/04/29/reinventing-the-wheel.html" data-attribute-title="&#39;Reinventing the Wheel: Discovering the Optimal Rolling Shape with PyTorch&#39;, Ben Wiener 2019 (Original URL: https://blog.benwiener.com/programming/2019/04/29/reinventing-the-wheel.html )">op­ti­mize a wheel-shape in­put for rolling-effi­ciency out­put</a>).<a href="https://www.gwern.net/Faces#sn41" class="footnote-ref spawns-popup" id="fnref41" role="doc-noteref"><sup>41</sup></a></p>
<p class="cyxy-trs-source">This can be used to cre­ate im­ages which are ‘op­ti­mized’ in some sense. For ex­am­ple, <a href="https://arxiv.org/abs/1605.09304" id="nguyen-et-al-2016-5" class="docMetadata has-annotation spawns-popup">Nguyen&nbsp;et&nbsp;al&nbsp;2016</a> uses ac­ti­va­tion max­i­miza­tion, demon­strat­ing how im­ages of Im­a­geNet classes can be pulled out of a stan­dard <span class="smallcaps-auto">CNN</span> clas­si­fier by back­prop over the clas­si­fier to max­i­mize a par­tic­u­lar out­put class; or re­design a <a href="https://arxiv.org/abs/2012.12235" id="salman-et-al-2020-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Unadversarial Examples: Designing Objects for Robust Vision&#39;, Salman et al 2020">fighter jet’s cam­ou­flage</a> for eas­ier clas­si­fi­ca­tion by a mod­el; more amus­ing­ly, in <a href="https://www.gwern.net/docs/ai/2016-goh-opennsfw.html" id="goh-2016" class="docMetadata has-annotation spawns-popup">“Im­age Syn­the­sis from Ya­hoo’s <code>open_nsfw</code>”</a>⁠, the gra­di­ent as­cent<a href="https://www.gwern.net/Faces#sn42" class="footnote-ref spawns-popup" id="fnref42" role="doc-noteref"><sup>42</sup></a> on the in­di­vid­ual pix­els of an im­age is done to min­i­mize/<wbr>­max­i­mize a <span class="smallcaps-auto">NSFW</span> clas­si­fier’s pre­dic­tion. This can also be done on a higher level by try­ing to max­i­mize sim­i­lar­ity to a NN em­bed­ding of an im­age to make it as ‘sim­i­lar’ as pos­si­ble, as was done orig­i­nally in Gatys&nbsp;et&nbsp;al&nbsp;2014 for style trans­fer, or for more com­pli­cated kinds of style trans­fer like in <a href="https://distill.pub/2018/differentiable-parameterizations/" class="no-popup">“Differ­en­tiable Im­age Pa­ra­me­ter­i­za­tions: A pow­er­ful, un­der­-ex­plored tool for neural net­work vi­su­al­iza­tions and art”</a>⁠.</p>
<p class="cyxy-trs-source">In this case, given an ar­bi­trary de­sired im­age’s <em>z</em>, one can ini­tial­ize a ran­dom <em>z</em>, run it for­ward through the <span class="smallcaps-auto">GAN</span> to get an im­age, com­pare it at the pixel level with the de­sired (fixed) im­age, and the to­tal differ­ence is the ‘loss’; hold­ing the <span class="smallcaps-auto">GAN</span> fixed, the back­prop­a­ga­tion goes back through the model and ad­justs the in­puts (the un­fixed <em>z</em>) to make it slightly more like the de­sired im­age. Done many times, the fi­nal <em>z</em> will now yield some­thing like the de­sired im­age, and that can be treated as its true <em>z</em>. Com­par­ing at the pix­el-level can be im­proved by in­stead look­ing at the higher lay­ers in a NN trained to do clas­si­fi­ca­tion (often an Im­a­geNet <span class="smallcaps-auto">VGG</span>), which will fo­cus more on the se­man­tic sim­i­lar­ity (more of a “per­cep­tual loss”) rather than mis­lead­ing de­tails of sta­tic &amp; in­di­vid­ual pix­els. The la­tent code can be the orig­i­nal <em>z</em>, or <em>z</em> after it’s passed through the stack of 8 FC lay­ers and has been trans­formed, or it can even be the var­i­ous per-layer style noises in­side the <span class="smallcaps-auto">CNN</span> part of Style<span class="smallcaps-auto">GAN</span>; the last is what <a href="https://github.com/avivga/style-image-prior" class="no-popup"><code>style-image-prior</code></a> uses &amp; <a href="https://arxiv.org/abs/1906.11880" id="gabbay-hoshen-2019-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Style Generator Inversion for Image Enhancement and Animation">Gab­bay &amp; Hoshen 2019</a><a href="https://www.gwern.net/Faces#sn43" class="footnote-ref spawns-popup" id="fnref43" role="doc-noteref"><sup>43</sup></a> ar­gue that it works bet­ter to tar­get the lay­er-wise en­cod­ings than the orig­i­nal <em>z</em>.</p>
<p class="cyxy-trs-source">This may not work too well as the lo­cal op­tima might be bad or the <span class="smallcaps-auto">GAN</span> may have trou­ble gen­er­at­ing pre­cisely the de­sired im­age no mat­ter how care­fully it is op­ti­mized, the pix­el-level loss may not be a good loss to use, and the whole process may be quite slow, es­pe­cially if one runs it many times with many differ­ent ini­tial ran­dom <em>z</em> to try to avoid bad lo­cal op­ti­ma. But it does mostly work.</p></li>
<li><p class="cyxy-trs-source"><strong>En­code+Back­prop­a­gate</strong> is a use­ful hy­brid strat­e­gy: the en­coder makes its best guess at the <em>z</em>, which will usu­ally be close to the true <em>z</em>, and then back­prop­a­ga­tion is done for a few it­er­a­tions to fine­tune the <em>z</em>. This can be much faster (one for­ward pass vs many for­ward+back­ward pass­es) and much less prone to get­ting stuck in bad lo­cal op­tima (s­ince it starts at a good ini­tial <em>z</em> thanks to the en­coder).</p>
<p class="cyxy-trs-source"><span class="marginnote sidenote"> Com­par­i­son with edit­ing in flow-based mod­els</span> On a tan­gent, edit­ing/<wbr>re­vers­ing is one of the great ad­van­tages<a href="https://www.gwern.net/Faces#sn44" class="footnote-ref spawns-popup" id="fnref44" role="doc-noteref"><sup>44</sup></a> of ‘flow’-based NN mod­els such as Glow, which is one of the fam­i­lies of NN mod­els com­pet­i­tive with <span class="smallcaps-auto">GAN</span>s for high­-qual­ity im­age gen­er­a­tion (a­long with au­tore­gres­sive pixel pre­dic­tion mod­els like Pixel<span class="smallcaps-auto">RNN</span>, and <span class="smallcaps-auto">VAE</span>s). Flow mod­els have the same shape as <span class="smallcaps-auto">GAN</span>s in push­ing a ran­dom la­tent vec­tor <em>z</em> through a se­ries of up­scal­ing con­vo­lu­tion or other lay­ers to pro­duce fi­nal pixel val­ues, but flow mod­els use a care­ful­ly-lim­ited set of prim­i­tives which make the model runnable both for­wards <em>and</em> back­wards ex­act­ly. This means every set of pix­els cor­re­sponds to a unique <em>z</em> and vice-ver­sa, and so an ar­bi­trary set of pix­els can put in and the model run back­wards to yield the ex­act cor­re­spond­ing <em>z</em>. There is no need to fight with the model to cre­ate an en­coder to re­verse it or use back­prop­a­ga­tion op­ti­miza­tion to try to find some­thing al­most right, as the flow model can al­ready do this. This makes edit­ing easy: plug the im­age in, get out the ex­act <em>z</em> with the equiv­a­lent of a sin­gle for­ward pass, fig­ure out which part of <em>z</em> con­trols a de­sired at­tribute like ‘glasses’, change that, and run it for­ward. The down­side of flow mod­els, which is why I do not (yet) use them, is that the re­stric­tion to re­versible lay­ers means that they are typ­i­cally <em>much</em> larger and slower to train than a more-or-less per­cep­tu­ally equiv­a­lent <span class="smallcaps-auto">GAN</span> mod­el, by eas­ily an or­der of mag­ni­tude (for Glow). When <a href="https://www.gwern.net/Faces-graveyard#glow" class="link-local has-content spawns-popup">I tried Glow</a>⁠, I could barely run an in­ter­est­ing model de­spite ag­gres­sive mem­o­ry-sav­ing tech­niques, and I did­n’t get any­where in­ter­est­ing with the sev­eral <span class="smallcaps-auto">GPU</span>-days I spent (which was un­sur­pris­ing when I re­al­ized how many <span class="smallcaps-auto">GPU</span>-months OA had spen­t). Since high­-qual­ity pho­to­re­al­is­tic <span class="smallcaps-auto">GAN</span>s are at the limit of 2019 train­abil­ity for most re­searchers or hob­by­ists, flow mod­els are clearly out of the ques­tion de­spite their many prac­ti­cal &amp; the­o­ret­i­cal ad­van­tages—they’re just too ex­pen­sive! How­ev­er, there is no known rea­son flow mod­els could­n’t be com­pet­i­tive with <span class="smallcaps-auto">GAN</span>s (they will prob­a­bly al­ways be larg­er, but be­cause they are more cor­rect &amp; do more), and fu­ture im­prove­ments or hard­ware scal­ing may make them more vi­able, so flow-based mod­els are an ap­proach to keep an eye on.</p></li>
</ol>
<p class="cyxy-trs-source">One of those 3 ap­proaches will en­code an im­age into a la­tent <em>z</em>. So far so good, that en­ables things like gen­er­at­ing ran­dom­ly-d­iffer­ent ver­sions of a spe­cific im­age or in­ter­po­lat­ing be­tween 2 im­ages, but how does one con­trol the <em>z</em> in a more in­tel­li­gent fash­ion to make spe­cific ed­its?</p>
<p class="cyxy-trs-source">If one knew what each vari­able in the <em>z</em> meant, one could sim­ply slide them in the −1/<wbr>+1 range, change the <em>z</em>, and gen­er­ate the cor­re­spond­ing edited im­age. But there are 512 vari­ables in <em>z</em> (for Style<span class="smallcaps-auto">GAN</span>), which is a lot to ex­am­ine man­u­al­ly, and their mean­ing is opaque as Style<span class="smallcaps-auto">GAN</span> does­n’t nec­es­sar­ily map each vari­able onto a hu­man-rec­og­niz­able fac­tor like ‘smil­ing’. A rec­og­niz­able fac­tor like ‘eye­glasses’ might even be gov­erned by mul­ti­ple vari­ables si­mul­ta­ne­ously which are non­lin­early in­ter­act­ing.</p>
<p class="cyxy-trs-source">As al­ways, the so­lu­tion to one mod­el’s prob­lems is yet more mod­els; to con­trol the <em>z</em>, like with the en­coder, we can sim­ply train yet an­other model (per­haps just a lin­ear clas­si­fier or ran­dom forests this time) to take the <em>z</em> of many im­ages which are all la­beled ‘smil­ing’ or ‘not smil­ing’, and learn what parts of <em>z</em> cause ‘smil­ing’ (eg <a href="https://arxiv.org/abs/1907.10786" id="shen-et-al-2019-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="Interpreting the Latent Space of GANs for Semantic Face Editing">Shen&nbsp;et&nbsp;al&nbsp;2019</a>). These ad­di­tional mod­els can then be used to con­trol a <em>z</em>. The nec­es­sary la­bels (a few hun­dred to a few thou­sand will be ad­e­quate since the <em>z</em> is only 512 vari­ables) can be ob­tained by hand or by us­ing a pre-ex­ist­ing clas­si­fi­er.</p>
<p class="cyxy-trs-source">So, the pieces of the puz­zle &amp; putting it all to­geth­er:</p>
<ul>
<li><p class="cyxy-trs-source">For anime faces as of March 2019, KichangKim’s <a href="https://www.gwern.net/docs/www/old.reddit.com/584c6c362de1ea72bc8962de25d364f366b0a301.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/ )">Deep­Dan­booru</a> is avail­able as <a href="http://kanotype.iptime.org:8003/deepdanbooru/" class="no-popup">a ser­vice</a> and as a down­load­able Keras mod­el, which pro­vides tags for many traits.</p>
<p class="cyxy-trs-source">Note that ex­plicit clas­si­fi­ca­tion/<wbr>­tag­ging may be overkill; if there is a me­chan­i­cal way of con­trol­ling an at­trib­ute, di­rect con­trol of the la­tents can be skipped. For ex­am­ple, an in­ter­po­la­tion of mir­ror­ing a face can be done by tak­ing a face+la­tent, mir­ror­ing it (us­ing an en­coder to get the mir­ror’s la­tents), and then sim­ply lin­early in­ter­po­lat­ing be­tween the two sets of la­tents; since they differ only in ori­en­ta­tion, their la­tents must also differ only in ori­en­ta­tion, and in­ter­po­la­tion=­con­trol.</p></li>
<li><p class="cyxy-trs-source">An ex­am­ple of in­ter­ac­tively con­trol­ling a CelebA face <span class="smallcaps-auto">GAN</span> in a con­ve­nient <span class="smallcaps-auto">GUI</span> is Sum­mitK­wan’s <a href="https://github.com/SummitKwan/transparent_latent_gan" class="no-popup"><span class="smallcaps-auto">TL-GAN</span></a> (<a href="https://www.kaggle.com/summitkwan/tl-gan-demo" class="no-popup">Kag­gle in­ter­ac­tive demo</a>⁠, <a href="https://blog.insightdatascience.com/generating-custom-photo-realistic-faces-using-ai-d170b1b59255" title="Generating custom photo-realistic faces using AI: Controlled image synthesis and editing using a novel (Transparent Latent-space GAN) TL-GAN model" class="no-popup">dis­cus­sion</a>).</p></li>
<li><p class="cyxy-trs-source">Dmitry Nikitko has writ­ten a <a href="https://github.com/Puzer/stylegan" class="no-popup">Style<span class="smallcaps-auto">GAN</span> en­coder</a> (<a href="https://www.gwern.net/docs/www/old.reddit.com/d159f9df903ba3064d6412dbc39fa1b09699fef2.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/aq6jxf/p_stylegan_encoder_from_real_images_to_latent/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/aq6jxf/p_stylegan_encoder_from_real_images_to_latent/ )">dis­cus­sion</a>⁠; <a href="https://github.com/pbaylies/stylegan-encoder" class="no-popup">al­ter­na­tive en­coder with ad­di­tional loss­es, us­ing resnets</a>) us­ing the back­prop­a­ga­tion ap­proach on Im­a­geNet <span class="smallcaps-auto">VGG</span> fea­tures (but not a di­rect en­coder)</p>
<p class="cyxy-trs-source">He has trained 3 clas­si­fi­ca­tion mod­els for age/<wbr>­gen­der/<wbr>s­mil­ing, and so can do things like <a href="https://github.com/Puzer/stylegan-encoder/blob/master/Play_with_latent_directions.ipynb" class="no-popup">edit Don­ald Trump or Hillary Clin­ton pho­tos to smile</a>⁠.</p></li>
<li><p class="cyxy-trs-source"><a href="https://nitter.cc/halcy" class="no-popup">snowy halcy</a> has reused the en­coder with the <span class="smallcaps-auto">VGG</span> loss+Dis­crim­i­na­tor loss, lin­ear mod­els trained on Deep­Dan­booru tags from <em>n</em> = 6.5k, al­low­ing <a href="https://github.com/halcy/stylegan" class="no-popup">con­trol of gen­er­ated anime faces</a>⁠. (A later in­de­pen­dent im­ple­men­ta­tion of back­prop­a­ga­tion-only was done by <a href="https://www.gwern.net/docs/ai/2019-abdal.pdf" id="abdal-et-al-2019" class="docMetadata has-annotation spawns-popup" data-attribute-title="Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?">Ab­dal&nbsp;et&nbsp;al&nbsp;2019</a>⁠.) Some of the en­cod­ings <a href="https://imgur.com/d8EYyel" class="no-popup">work well on solo faces</a> &amp; <a href="https://imgur.com/BLWbiXT" class="no-popup">oth­ers don’t (on mul­ti­ple faces)</a>⁠, so stick­ing close to anime face Style<span class="smallcaps-auto">GAN</span> sam­ples is ad­vised. Links:</p>
<ul>
<li><a href="https://github.com/halcy/stylegan/blob/master/Stylegan-Generate-Encode.ipynb" class="no-popup cyxy-trs-source">in­ter­ac­tive note­book</a></li>
<li><a href="https://colab.research.google.com/drive/1LiWxqJJMR5dg4BxwUgighaWp2U_enaFd#offline=true&amp;sandboxMode=true" class="no-popup cyxy-trs-source">Google Co­lab ver­sion</a></li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">videos: <a href="https://www.gwern.net/docs/www/icosahedron.website/863b32de406aac1effc96edf32da05495456ba4a.html" class="localArchive has-content spawns-popup cyxy-trs-source cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://icosahedron.website/@halcy/101654023952557254" data-attribute-title="(Original URL: https://icosahedron.website/@halcy/101654023952557254 )">trans­form­ing into red-eyed/<wbr>black­-haired ver­sions</a> (<a href="https://www.gwern.net/docs/www/old.reddit.com/db2863c0b37e3fb1c5d7c93893ccafad849c5fbf.html" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/AnimeResearch/comments/aul582/modification_of_anime_face_stylegan_disentangled/" data-attribute-title="(Original URL: https://old.reddit.com/r/AnimeResearch/comments/aul582/modification_of_anime_face_stylegan_disentangled/ )">im­age</a>⁠; <a href="https://www.gwern.net/images/gan/stylegan/2020-snowyhalcy-stylegan-animefaceediting-brightness.png" class="invertible-not has-content spawns-popup cyxy-trs-source" data-image-height="338" data-image-width="1103">bright­ness edit­ing</a>), <a href="https://www.youtube.com/watch?v=Z1-3JKDh0nI" class="has-content spawns-popup cyxy-trs-source"><span class="smallcaps-auto cyxy-trs-source">TL-GAN</span> <span class="smallcaps-auto cyxy-trs-source">GUI</span> demon­stra­tion of gen­eral edit­ing</a></li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">mouth flap con­trol: Kyle McLean’s <a href="https://www.gwern.net/docs/www/everyoneishappy.com/17c80492252323f59ac82996be2c5d617c545a58.html" class="localArchive has-content spawns-popup cyxy-trs-source" rel="archived alternate nofollow" data-url-original="http://everyoneishappy.com/portfolio/waifu-synthesis-real-time-generative-anime/" data-attribute-title="(Original URL: http://everyoneishappy.com/portfolio/waifu-synthesis-real-time-generative-anime/ )">“Waifu Syn­the­sis” video</a> &amp; <a href="https://github.com/Aiterasu/stylegan" class="no-popup cyxy-trs-source">Ait­erasu repo</a> (demos: <a href="https://www.gwern.net/docs/www/old.reddit.com/b2324dffcfdbdae094f0b15b2d7b25d28ff30edd.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MediaSynthesis/comments/c6axmr/close_the_world_txen_eht_nepo/" data-attribute-title="(Original URL: https://old.reddit.com/r/MediaSynthesis/comments/c6axmr/close_the_world_txen_eht_nepo/ )">1</a>⁠, <a href="https://www.youtube.com/watch?v=vXUFchjWR58" class="has-content spawns-popup" data-attribute-title="This Anime Does Not Exist">2</a>), which use the Halcy tag­ger to open/<wbr>­close mouths, al­low­ing for lip-sync­inggw ’ar</li>
<li class="cyxy-trs-source cyxy-trs-source"><a href="https://artbreeder.com/" id="simon-2019" class="docMetadata has-annotation spawns-popup cyxy-trs-source">Art­breeder</a> also im­ple­ments face edit­ing for my anime Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> (<a href="https://nitter.cc/Artbreeder/status/1182293849181495296" class="no-popup cyxy-trs-source">demo video</a>) along with a num­ber of other mod­els such as West­ern art por­traits</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">the <a href="https://www.thisfursonadoesnotexist.com/" id="arfafax-2020" class="docMetadata has-annotation spawns-popup cyxy-trs-source">This Fur­sona Does Not Ex­ist</a> model can be used <a href="https://nitter.cc/arfafax/status/1263638042889224193" class="no-popup cyxy-trs-source">to edit furry faces</a> <a href="https://fursona.app/" class="no-popup cyxy-trs-source">through</a> <a href="https://colab.research.google.com/drive/1O5XbpMRU9i83mLAiTrMufCqmImgTRI7A" title="This Fursona Does Not Exist - Fursona Editor (Tensorflow Version)" class="no-popup cyxy-trs-source"><span class="smallcaps-auto cyxy-trs-source">GAN</span>space</a> (as pre­sum­ably can <a href="https://thisponydoesnotexist.net/" id="arfafax-2020" class="docMetadata has-annotation spawns-popup cyxy-trs-source">“This Pony Does Not Ex­ist”</a> for pony faces)</li>
</ul></li>
<li><p class="cyxy-trs-source"><a href="https://github.com/harskish/ganspace" class="no-popup"><span class="smallcaps-auto">GANS</span>pace</a> (<a href="https://arxiv.org/abs/2004.02546" id="härkönen-et-al-2020-4" class="docMetadata has-annotation spawns-popup" data-attribute-title="GANSpace: Discovering Interpretable GAN Controls">Härkö­nen&nbsp;et&nbsp;al&nbsp;2020</a>) is a semi­-au­to­mated ap­proach to dis­cov­er­ing use­ful la­tent vec­tor con­trols: it tries to find ‘large’ changes in im­ages, un­der the as­sump­tion those cor­re­spond to in­ter­est­ing dis­en­tan­gled fac­tors. A hu­man tweak­ing the lay­ers it uses and which ones are se­lected can find in­ter­est­ing (eg a <a href="https://nitter.cc/realmeatyhuman/status/1255570195319590913" class="no-popup">“stoned” face vec­tor</a> in <span class="smallcaps-auto">FFHQ</span> Style<span class="smallcaps-auto">GAN</span>), and it can be used in <a href="https://colab.research.google.com/drive/1g-ShMzkRWDMHPyjom_p-5kqkn2f-GwBi" class="no-popup">Co­lab</a>⁠.</p></li>
</ul>
<p class="cyxy-trs-source">The fi­nal re­sult is in­ter­ac­tive edit­ing of anime faces along many differ­ent fac­tors:</p>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2019-03-02-snowyhalcy-stylegan-animefaces-tlgan-interactivewaifumodification.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">
<a href="https://www.youtube.com/watch?v=Z1-3JKDh0nI" class="has-content spawns-popup cyxy-trs-source" data-attribute-title="Interactive Waifu Modification">snowy halcy</a> (<a href="https://www.gwern.net/images/gan/stylegan/2019-03-02-snowyhalcy-stylegan-animefaces-tlgan-interactivewaifumodification.mp4" class="has-content spawns-popup cyxy-trs-source">MP4</a>) demon­strat­ing in­ter­ac­tive edit­ing of Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> anime faces us­ing anime-face-Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>+DeepDanbooru+Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>-encoder+<span class="smallcaps-auto cyxy-trs-source">TL-GAN</span>
</figcaption></span></span></figure>
<section id="editing-rare-attributes" class="level2">
<h2><a href="https://www.gwern.net/Faces#editing-rare-attributes" title="Link to section: § &#39;Editing Rare Attributes&#39;" class="no-popup cyxy-trs-source">Editing Rare Attributes</a></h2>
<p class="cyxy-trs-source">A strat­egy of hand-edit­ing or us­ing a tag­ger to clas­sify at­trib­utes works for com­mon ones which will be well-rep­re­sented in a sam­ple of a few thou­sand since the clas­si­fier needs a few hun­dred cases to work with, but what about rarer at­trib­utes which might ap­pear only on one in a thou­sand ran­dom sam­ples, or at­trib­utes too rare in the dataset for Style<span class="smallcaps-auto">GAN</span> to have learned, or at­trib­utes which may not be in the dataset at all? Edit­ing “red eyes” should be easy, but what about some­thing like “bunny ears”? It would be amus­ing to be able to edit por­traits to add bunny ears, but there aren’t <em>that</em> many bunny ear sam­ples (although cat ears might be much more com­mon); is one doomed to gen­er­ate &amp; clas­sify hun­dreds of thou­sands of sam­ples to en­able bunny ear edit­ing? That would be in­fea­si­ble for hand la­bel­ing, and diffi­cult even with a tag­ger.</p>
<p class="cyxy-trs-source">One sug­ges­tion I have for this use-case would be to briefly train <em>an­other</em> Style<span class="smallcaps-auto">GAN</span> model on an en­riched or boosted dataset, like a dataset of 50:50 bunny ear im­ages &amp; nor­mal im­ages. If one can ob­tain a few thou­sand bunny ear im­ages, then this is ad­e­quate for trans­fer learn­ing (com­bined with a few thou­sand ran­dom nor­mal im­ages from the orig­i­nal dataset), and one can re­train the Style<span class="smallcaps-auto">GAN</span> on an equal bal­ance of im­ages. The high pres­ence of bunny ears will en­sure that the Style<span class="smallcaps-auto">GAN</span> quickly learns all about those, while the nor­mal im­ages pre­vent it from over­fit­ting or cat­a­strophic for­get­ting of the full range of im­ages.</p>
<p class="cyxy-trs-source">This new bun­ny-ear Style<span class="smallcaps-auto">GAN</span> will then pro­duce bun­ny-ear sam­ples half the time, cir­cum­vent­ing the rare base rate is­sue (or fail­ure to learn, or nonex­is­tence in dataset), and en­abling effi­cient train­ing of a clas­si­fi­er. And since nor­mal faces were used to pre­serve its gen­eral face knowl­edge de­spite the trans­fer learn­ing po­ten­tially de­grad­ing it, it will re­main able to en­code &amp; op­ti­mize nor­mal faces. (The orig­i­nal clas­si­fiers may even be reusable on this, de­pend­ing on how ex­treme the new at­tribute is, as the la­tent space <em>z</em> might not be too affected by the new at­tribute and the var­i­ous other at­trib­utes ap­prox­i­mately main­tain the orig­i­nal re­la­tion­ship with <em>z</em> as be­fore the re­train­ing.)</p>
</section>
</section>
<section id="stylegan-2" class="level1">
<h1><a href="https://www.gwern.net/Faces#stylegan-2" title="Link to section: § &#39;StyleGAN 2&#39;" class="no-popup cyxy-trs-source">StyleGAN 2</a></h1>
<p class="cyxy-trs-source"><a href="https://arxiv.org/abs/1912.04958#nvidia" id="karras-et-al-2019-2" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Analyzing and Improving the Image Quality of StyleGAN&#39;, Karras et al 2019">Style<span class="smallcaps-auto">GAN</span> 2</a> (<a href="https://github.com/NVlabs/stylegan2" class="no-popup">source</a> (<a href="https://github.com/NVlabs/stylegan2-ada-pytorch" class="no-popup">Py­Torch</a>), <a href="https://www.youtube.com/watch?v=c-NJtV9Jvp0" class="has-content spawns-popup">video</a>), elim­i­nates blob ar­ti­facts, adds a na­tive en­cod­ing ‘pro­jec­tion’ fea­ture for <a href="https://www.gwern.net/Faces#reversing-stylegan-to-control-modify-images" class="link-self identifier-link-up has-content spawns-popup">edit­ing</a>⁠, sim­pli­fies the run­time by scrap­ping pro­gres­sive grow­ing in fa­vor of <a href="https://arxiv.org/abs/1903.06048" id="karnewar-wang-2019-3" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;MSG-GAN: Multi-Scale Gradient GAN for Stable Image Synthesis&#39;, Karnewar &amp; Wang 2019"><span class="smallcaps-auto">MSG-GAN</span></a>-like mul­ti­-s­cale ar­chi­tec­ture, &amp; has higher over­all qual­i­ty—but sim­i­lar to­tal train­ing time/<wbr>re­quire­ments<a href="https://www.gwern.net/Faces#sn45" class="footnote-ref spawns-popup" id="fnref45" role="doc-noteref"><sup>45</sup></a></p>
<p class="cyxy-trs-source">I used a 512px anime por­trait S2 model trained by Aaron Gokaslan to cre­ate <a href="https://www.gwern.net/TWDNE#twdnev3" id="gwern-twdne-twdnev3" class="link-local docMetadata has-annotation spawns-popup">ThisWai­fu­Does­No­tEx­ist v3</a>:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="100 ran­dom sam­ple im­ages from the  2 anime por­trait faces in  arranged in a 10×10 grid." height="1400" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/thiswaifudoesnotexist-v3-100samples.jpg" srcset="/images/gan/stylegan/thiswaifudoesnotexist-v3-100samples.jpg-768px.jpg 768w, /images/gan/stylegan/thiswaifudoesnotexist-v3-100samples.jpg 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">100 ran­dom sam­ple im­ages from the Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> 2 anime por­trait faces in <span class="smallcaps-auto cyxy-trs-source">TWDNE</span>v3, arranged in a 10×10 grid.</figcaption></span></span></figure>
<p class="cyxy-trs-source">Train­ing sam­ples:</p>
<figure>

<span class="figure-inner-wrapper"><span class="image-wrapper focusable"><img alt="It­er­a­tion #24,303 of Gokaslan’s train­ing of an anime por­trait  2 model (train­ing sam­ples)" height="746" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="./一个StyleGAN动漫脸编辑详细教程_files/2020-01-11-stylegan2-skylion-animeportraits-24303.webp" srcset="/images/gan/stylegan/2020-01-11-stylegan2-skylion-animeportraits-24303.png-768px.png 768w, /images/gan/stylegan/2020-01-11-stylegan2-skylion-animeportraits-24303.png 1400w" width="1400" class="focusable"></span><span class="caption-wrapper"><figcaption aria-hidden="true" class="cyxy-trs-source cyxy-trs-source">It­er­a­tion #24,303 of Gokaslan’s train­ing of an anime por­trait Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> 2 model (train­ing sam­ples)</figcaption></span></span></figure>
<p class="cyxy-trs-source">The model was trained to it­er­a­tion #24,664 for &gt;2 weeks on 4 Nvidia 2080ti <span class="smallcaps-auto">GPU</span>s at 35–70s per 1k im­ages. The Ten­sor­flow S2 model is <a href="https://mega.nz/#!PeIi2ayb!xoRtjTXyXuvgDxSsSMn-cOh-Zux9493zqdxwVMaAzp4" title="2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl.xz" class="no-popup">avail­able for down­load</a> (320M­B).<a href="https://www.gwern.net/Faces#sn46" class="footnote-ref spawns-popup" id="fnref46" role="doc-noteref"><sup>46</sup></a> (<a href="https://hivemind-repo.s3-us-west-2.amazonaws.com/twdne3/twdne3.pt" class="no-popup">Py­Torch</a> &amp; <a href="https://hivemind-repo.s3-us-west-2.amazonaws.com/twdne3/twdne3.onnx" class="no-popup">Onnx</a> ver­sions have been made by <a href="https://github.com/antonpaquin/stylegan2-pytorch" class="no-popup">An­ton us­ing a cus­tom repo</a> Note that both my face &amp; por­trait mod­els can be run via the Gen­Force Py­Torch repo as well.) This model can be used in Google Co­lab (<a href="https://colab.research.google.com/drive/1Pv8OIFlonha4KeYyY2oEFaK4mG-alaWF" class="no-popup">demon­stra­tion note­book</a>⁠, al­though it seems it may pull in an older S2 mod­el) &amp; the model can also be used with the S2 code­base for <a href="https://nitter.cc/layolu/status/1218177246495535104" class="no-popup">en­cod­ing anime faces</a>⁠.</p>
<section id="running-s2" class="level2">
<h2><a href="https://www.gwern.net/Faces#running-s2" title="Link to section: § &#39;Running S2&#39;" class="no-popup cyxy-trs-source">Running S2</a></h2>
<p class="cyxy-trs-source">Be­cause of the op­ti­miza­tions, which re­quires cus­tom lo­cal com­pi­la­tion of <span class="smallcaps-auto">CUDA</span> code for max­i­mum effi­cien­cy, get­ting S2 run­ning can be more chal­leng­ing than get­ting S1 run­ning.</p>
<ul>
<li><p class="cyxy-trs-source">No Ten­sor­Flow 2 com­pat­i­bil­i­ty: the TF ver­sion must be 1.14/<wbr>1.15. Try­ing to run with TF 2 will give er­rors like: <code>TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'</code>.</p>
<p class="cyxy-trs-source">I ran into cu<span class="smallcaps-auto">DNN</span> com­pat­i­bil­ity prob­lems with TF 1.15 (which re­quires cu<span class="smallcaps-auto">DNN</span> &gt;7.6.0, 2019-05-20, for <span class="smallcaps-auto">CUDA</span> 10.0), which gave er­rors like this:</p>
<pre><code>...[2020-01-11 23:10:35.234784: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library:
   7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher
   minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.
   If building from sources, make sure the library loaded at runtime is compatible with the version specified
   during compile configuration...</code></pre>
<p class="cyxy-trs-source">But then with 1.14, the <code>tpu-estimator</code> li­brary was not found! (I ul­ti­mately took the risk of up­grad­ing my in­stal­la­tion with <code>libcudnn7_7.6.0.64-1+cuda10.0_amd64.deb</code>, and thank­ful­ly, that worked and did not seem to break any­thing else.)</p></li>
<li><p class="cyxy-trs-source">Get­ting the en­tire pipeline to com­pile the cus­tom ops in a Conda en­vi­ron­ment was an­noy­ing so Gokaslan tweaked it to use 1.14 on Lin­ux, used <code>cudatoolkit-dev</code> from Conda Forge, and changed the build script to use <code>gcc-7</code> (s­ince <code>gcc-8</code> was un­sup­port­ed)</p></li>
<li><p class="cyxy-trs-source">one is­sue with Ten­sor­Flow 1.14 is you need to force <code>allow_growth</code> or it will er­ror out on Nvidia 2080tis</p></li>
<li><p class="cyxy-trs-source">con­fig name change: <code>train.py</code> has been re­named (a­gain) to <code>run_training.py</code></p></li>
<li><p class="cyxy-trs-source">buggy learn­ing rates: S2 (but not S1) <a href="https://nitter.cc/theshawwn/status/1230022825538248704" class="no-popup">ac­ci­den­tally uses the same LR for both G &amp; D</a>⁠; ei­ther fix this or keep it in mind when do­ing LR tun­ing—changes to <code>D_lrate</code> do noth­ing!</p></li>
<li><p class="cyxy-trs-source"><em>n</em> = 1 mini­batch prob­lems: S2 is not a large NN so it can be trained on low-end <span class="smallcaps-auto">GPU</span>s; how­ev­er, the S2 code make an un­nec­es­sary as­sump­tion that <em>n</em>≥2; to fix this in <code>training/loss.py</code> (fixed in <a href="https://github.com/shawwn/stylegan2" class="no-popup">Shawn Presser’s <span class="smallcaps-auto">TPU</span>/self-attention ori­ented fork</a>):</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode Diff"><code class="sourceCode diff"><span class="dt">@@ -157,9 +157,8 @@ def G_logistic_ns_pathreg(G, D, opt, training_set, minibatch_size, pl_minibatch_</span>
    with tf.name_scope('PathReg'):

        # Evaluate the regularization term using a smaller minibatch to conserve memory.
        if pl_minibatch_shrink &gt; 1 and minibatch_size &gt; 1:
            assert minibatch_size % pl_minibatch_shrink == 0
            pl_minibatch = minibatch_size // pl_minibatch_shrink
        if pl_minibatch_shrink &gt; 1:
            pl_minibatch = tf.maximum(1, minibatch_size // pl_minibatch_shrink)
            pl_latents = tf.random_normal([pl_minibatch] + G.input_shapes[0][1:])
            pl_labels = training_set.get_random_labels_tf(pl_minibatch)
            fake_images_out, fake_dlatents_out = G.get_output_for(pl_latents, pl_labels, is_training=True, return_dlatents=True)</code></pre></div>
<p class="cyxy-trs-source"><!-- ) --></p></li>
<li><p class="cyxy-trs-source">S2 has some sort of mem­ory leak, pos­si­bly re­lated to the <span class="smallcaps-auto">FID</span> eval­u­a­tions, re­quir­ing reg­u­lar restarts, like putting it into a loop</p></li>
</ul>
<p class="cyxy-trs-source">Once S2 was run­ning, Gokaslan trained the S2 por­trait model with gen­er­ally de­fault hy­per­pa­ra­me­ters.</p>
</section>
</section>
<section id="future-work" class="level1">
<h1><a href="https://www.gwern.net/Faces#future-work" title="Link to section: § &#39;Future Work&#39;" class="no-popup cyxy-trs-source">Future Work</a></h1>
<p class="cyxy-trs-source">Some open ques­tions about Style<span class="smallcaps-auto">GAN</span>’s ar­chi­tec­ture &amp; train­ing dy­nam­ics:</p>
<ul>
<li class="cyxy-trs-source">is <strong>pro­gres­sive grow­ing still nec­es­sary</strong> with Style<span class="smallcaps-auto">GAN</span>? (<a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-up has-content spawns-popup">Style<span class="smallcaps-auto">GAN</span> 2</a> im­plies that it is not, as it uses a <span class="smallcaps-auto">MSG-GAN</span>-like ap­proach)</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">are 8×512 FC lay­ers nec­es­sary? (<a href="https://github.com/tensorfork/tensorfork/issues/26" class="no-popup cyxy-trs-source cyxy-trs-source">Pre­lim­i­nary Big<span class="smallcaps-auto cyxy-trs-source">GAN</span> work</a> sug­gests that they are <em class="cyxy-trs-source">not</em> nec­es­sary for Big<span class="smallcaps-auto cyxy-trs-source">GAN</span>.)</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">what are the wrinkly-line/<wbr>cracks <strong class="cyxy-trs-source">noise ar­ti­facts</strong> which ap­pear at the end of train­ing?</li>
<li class="cyxy-trs-source cyxy-trs-source">how does Style<span class="smallcaps-auto cyxy-trs-source">GAN</span> <strong class="cyxy-trs-source">com­pare to <a href="https://www.gwern.net/Faces#biggan" class="link-self identifier-link-down has-content spawns-popup cyxy-trs-source">Big<span class="smallcaps-auto cyxy-trs-source">GAN</span></a></strong> in fi­nal qual­i­ty?</li>
</ul>
<p class="cyxy-trs-source">Fur­ther pos­si­ble work:</p>
<ul>
<li><p class="cyxy-trs-source">ex­plo­ration of “<strong>cur­ricu­lum learn­ing</strong>”: can train­ing be sped up by train­ing to con­ver­gence on small <em>n</em> and then pe­ri­od­i­cally ex­pand­ing the dataset?</p></li>
<li><p class="cyxy-trs-source"><strong>boot­strap­ping</strong> im­age gen­er­a­tion by start­ing with a seed cor­pus, gen­er­at­ing many ran­dom sam­ples, se­lect­ing the best by hand, and re­train­ing; eg ex­pand a cor­pus of a spe­cific char­ac­ter, or ex­plore ‘hy­brid’ cor­puses which mix A/<wbr>B im­ages &amp; one then se­lects for im­ages which look most A+B-ish</p></li>
<li><p class="cyxy-trs-source"><strong>im­proved trans­fer learn­ing</strong> scripts to edit trained mod­els so 512px pre­trained mod­els can be pro­moted to work with 1024px im­ages and vice versa</p></li>
<li><p class="cyxy-trs-source">bet­ter Dan­booru tag­ger <span class="smallcaps-auto">CNN</span> for pro­vid­ing <strong>clas­si­fi­ca­tion em­bed­dings</strong> for var­i­ous pur­pos­es, par­tic­u­larly <span class="smallcaps-auto">FID</span> loss mon­i­tor­ing, mini­batch dis­crim­i­na­tion/<wbr>aux­il­iary loss, and style trans­fer for cre­at­ing a ‘StyleDan­booru’</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">with a StyleDan­booru, I am cu­ri­ous if that can be used as a par­tic­u­larly <span class="smallcaps cyxy-trs-source">Pow­er­ful Form Of Data Aug­men­ta­tion</span> for small <em>n</em> char­ac­ter datasets, and whether it leads to a re­ver­sal of train­ing dy­nam­ics with edges com­ing <em class="cyxy-trs-source">be­fore</em> col­ors/<wbr>­tex­tures—it’s pos­si­ble that a StyleDan­booru could make many <span class="smallcaps-auto cyxy-trs-source">GAN</span> ar­chi­tec­tures, not just Style<span class="smallcaps-auto cyxy-trs-source">GAN</span>, sta­ble to train on ani­me/<wbr>il­lus­tra­tion datasets</li>
</ul></li>
<li><p class="cyxy-trs-source"><strong>bor­row­ing ar­chi­tec­tural en­hance­ments from Big<span class="smallcaps-auto">GAN</span></strong>: self­-at­ten­tion lay­ers, spec­tral norm reg­u­lar­iza­tion, large-mini­batch train­ing, and a rec­ti­fied Gauss­ian dis­tri­b­u­tion for the la­tent vec­tor <em>z</em></p></li>
<li><p class="cyxy-trs-source"><strong>text → im­age</strong> con­di­tional <span class="smallcaps-auto">GAN</span> ar­chi­tec­ture (à la Stack<span class="smallcaps-auto">GAN</span>):</p>
<p class="cyxy-trs-source">This would take the text tag de­scrip­tions of each im­age com­piled by Dan­booru users and use those as in­puts to Style<span class="smallcaps-auto">GAN</span>, which, should it work, would mean you could cre­ate ar­bi­trary anime im­ages sim­ply by typ­ing in a string like <code>1_boy samurai facing_viewer red_hair clouds sword armor blood</code> etc.</p>
<p class="cyxy-trs-source">This should al­so, by pro­vid­ing rich se­man­tic de­scrip­tions of each im­age, make train­ing faster &amp; sta­bler and con­verge to higher fi­nal qual­i­ty.</p></li>
<li><p class="cyxy-trs-source"><strong>meta-learn­ing</strong> for few-shot face or char­ac­ter or artist im­i­ta­tion (eg <a href="https://github.com/EndingCredits/Set-CGAN" class="no-popup">Set-<span class="smallcaps-auto">CGAN</span></a> or <a href="https://arxiv.org/abs/1901.02199" id="clouâtre-demers-2019" class="docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">FIGR</span></a> or per­haps <a href="https://arxiv.org/abs/1905.01723" id="liu-et-al-2019-5" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Few-Shot Unsupervised Image-to-Image Translation&#39;, Liu et al 2019"><span class="smallcaps-auto">FUNIT</span></a>⁠, or <a href="https://arxiv.org/abs/1904.01774" id="noguchi-harada-2019-4" class="docMetadata has-annotation spawns-popup" data-attribute-title="Image Generation from Small Datasets via Batch Statistics Adaptation">Noguchi &amp; Harada 2019</a>—the last of which achieves few-shot learn­ing with sam­ples of <em>n</em> = 25 <span class="smallcaps-auto">TWDNE</span> Style<span class="smallcaps-auto">GAN</span> anime faces)</p></li>
</ul>
<section id="imagenet-stylegan" class="level2">
<h2><a href="https://www.gwern.net/Faces#imagenet-stylegan" title="Link to section: § &#39;ImageNet StyleGAN&#39;" class="no-popup cyxy-trs-source">ImageNet StyleGAN</a></h2>
<p class="cyxy-trs-source">As part of ex­per­i­ments in scal­ing up Style<span class="smallcaps-auto">GAN</span> 2, us­ing <a href="https://www.tensorflow.org/tfrc" class="docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">TFRC</span> re­search cred­its</a>⁠, we ran Style<span class="smallcaps-auto">GAN</span> on large-s­cale datasets in­clud­ing Dan­booru2019, Im­a­geNet, and sub­sets of the <a href="https://arxiv.org/abs/1503.01817#flickr" id="thomee-et-al-2015-3" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;YFCC100M (Yahoo Flickr Creative Commons 100 Million Dataset): The New Data in Multimedia Research&#39;, Thomee et al 2015">Flickr <span class="smallcaps-auto">YFCC</span>100M dataset</a>⁠. De­spite run­ning for mil­lions of im­ages, no S2 run ever achieved re­motely the re­al­ism of S2 on <span class="smallcaps-auto">FFHQ</span> or Big<span class="smallcaps-auto">GAN</span> on Im­a­geNet: while the tex­tures could be sur­pris­ingly good, the se­man­tic global struc­ture never came to­geth­er, with glar­ing flaws—there would be too many heads, or they would be de­tached from bod­ies, etc.</p>
<p class="cyxy-trs-source">Aaron Gokaslan took the time to com­pute the <span class="smallcaps-auto">FID</span> on Im­a­geNet, es­ti­mat­ing a ter­ri­ble score of <span class="smallcaps-auto">FID</span> ~120. (High­er=­worse; for com­par­ison, Big<span class="smallcaps-auto">GAN</span> with <a href="https://arxiv.org/abs/2004.02967#deepmind" id="liu-et-al-2020-4" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;EvoNorm: Evolving Normalization-Activation Layers&#39;, Liu et al 2020">EvoNorm</a> can be as good as <span class="smallcaps-auto">FID</span> ~7, and reg­u­lar Big<span class="smallcaps-auto">GAN</span> typ­i­cally sur­passes <span class="smallcaps-auto">FID</span> 120 within a few thou­sand it­er­a­tions.) Even ex­per­i­ments in in­creas­ing the S2 model size up to ~1GB (by in­creas­ing the fea­ture map mul­ti­pli­er) im­proved qual­ity rel­a­tively mod­est­ly, and showed no signs of ever ap­proach­ing Big<span class="smallcaps-auto">GAN</span>-level qual­i­ty. We con­cluded that Style<span class="smallcaps-auto">GAN</span> is in fact fun­da­men­tally lim­ited as a <span class="smallcaps-auto">GAN</span>, <a href="https://en.wikipedia.org/wiki/Bias-variance_tradeoff" class="docMetadata has-annotation spawns-popup">trad­ing off sta­bil­ity for power</a>⁠, and switched over to Big<span class="smallcaps-auto">GAN</span> work.</p>
<p class="cyxy-trs-source">For those in­ter­est­ed, we pro­vide our 512px Im­a­geNet S2 (step 1,394,688):</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode Bash"><code class="sourceCode bash"><span class="fu">rsync</span> <span class="at">--verbose</span> rsync://78.46.86.149:873/biggan/2020-04-07-shawwn-stylegan-imagenet-512px-run52-1394688.pkl.xz ./</code></pre></div>
<figure>


<span class="figure-inner-wrapper"><video controls="controls" preload="none" loop="">
<source src="/images/gan/stylegan/2020-03-26-shawnpresser-stylegan2-imagenet-run52-1394688-interpolation-7.mp4" type="video/mp4">
</video><span class="caption-wrapper"><figcaption class="cyxy-trs-source cyxy-trs-source cyxy-trs-source">
<a href="https://nitter.cc/theshawwn/status/1243271022616297472" title="StyleGAN2 trained on ImageNet (1.2 million images, unconditional) at 512×512 resolution. Trained from scratch on a TPUv3-512 in just 45 hours. Training speed: 3k images per second." class="no-popup cyxy-trs-source">Shawn Presser</a>⁠, S2 Im­a­geNet in­ter­po­la­tion video from part­way through train­ing (~45 hours on a <span class="smallcaps-auto cyxy-trs-source">TPU</span>v3-512, 3k im­ages/<wbr>s)
</figcaption></span></span></figure>
</section>
</section>
<section id="biggan" class="level1 collapse">
<h1 class=""><a href="https://www.gwern.net/Faces#biggan" title="Link to section: § &#39;BigGAN&#39;" class="no-popup cyxy-trs-source">BigGAN</a></h1><input type="checkbox" class="disclosure-button" aria-label="Open/close collapsed section" title="This is a collapsed region; mouse click to expand it. Collapsed text can be sections, code, text samples, or long digressions which most users will not read, and interested readers can opt into.">
<div id="biggan-transfer-learning">

</div>
<div id="biggan-danbooru2018-1k-experiments">

</div>
<div id="danbooru2018-1k-dataset">

</div>
<div id="biggan-training">

</div>
<div id="biggan-imagenet-danbooru2018-1k">

</div>
<div id="biggan-256px-danbooru2018-1k">

</div>
<div id="px-danbooru2018-1k-samples">

</div>
<div id="px-biggan-downloads">

</div>
<div id="evaluation">

</div>
<div id="danbooru2019e621-256px-biggan">

</div>
<div class="abstract">
<blockquote>
<p class="cyxy-trs-source">Fol­low­ing my Style<span class="smallcaps-auto">GAN</span> anime face ex­per­i­ments, I ex­plore Big<span class="smallcaps-auto">GAN</span>, an­other re­cent <span class="smallcaps-auto">GAN</span> with <span class="smallcaps-auto">SOTA</span> re­sults on one of the most com­plex im­age do­mains tack­led by <span class="smallcaps-auto">GAN</span>s so far (Im­a­geNet). Big<span class="smallcaps-auto">GAN</span>’s ca­pa­bil­i­ties come at a steep com­pute cost, how­ev­er.</p>
<p class="cyxy-trs-source">Us­ing the un­offi­cial Big<span class="smallcaps-auto">GAN-P</span>yTorch reim­ple­men­ta­tion, I ex­per­i­mented in 2019 with 128px Im­a­geNet trans­fer learn­ing (suc­cess­ful) with ~6 <span class="smallcaps-auto">GPU</span>-days, and from-scratch 256px anime por­traits of 1000 char­ac­ters on a 8×2080ti ma­chine for a month (mixed re­sult­s). My Big<span class="smallcaps-auto">GAN</span> re­sults are good but com­pro­mised by the com­pute ex­pense &amp; prac­ti­cal prob­lems with the re­leased Big<span class="smallcaps-auto">GAN</span> code base. While Big<span class="smallcaps-auto">GAN</span> is not yet su­pe­rior to Style<span class="smallcaps-auto">GAN</span> for many pur­pos­es, Big<span class="smallcaps-auto">GAN</span>-like ap­proaches may be nec­es­sary to scale to whole anime im­ages.</p>
<p class="cyxy-trs-source">For fol­lowup ex­per­i­ments, Shawn Presser, I and oth­ers (col­lec­tive­ly, “Ten­sor­fork”) have used Ten­sor­flow Re­search Cloud <span class="smallcaps-auto">TPU</span> cred­its &amp; the com­pare_­gan Big<span class="smallcaps-auto">GAN</span> reim­ple­men­ta­tion. Run­ning this at scale on the full Dan­booru2019 dataset in May 2020, we have reached the best anime <span class="smallcaps-auto">GAN</span> re­sults to date.</p>
</blockquote>
</div>
<p class="drop-cap-kanzlei cyxy-trs-source">See <a href="https://www.gwern.net/BigGAN" id="gwern-biggan" class="link-local docMetadata has-annotation spawns-popup"><strong>Mak­ing Anime With Big<span class="smallcaps-auto">GAN</span></strong></a>⁠.</p>
</section>
<section id="see-also" class="level1">
<h1><a href="https://www.gwern.net/Faces#see-also" title="Link to section: § &#39;See Also&#39;" class="no-popup cyxy-trs-source">See Also</a></h1>
<ul>
<li><a href="https://www.thiswaifudoesnotexist.net/" id="gwern-www-thiswaifudoesnotexist-net" class="docMetadata has-annotation spawns-popup cyxy-trs-source cyxy-trs-source">“This Waifu Does Not Ex­ist (<span class="smallcaps-auto cyxy-trs-source">TWDNE</span>)”</a> (<a href="https://www.gwern.net/TWDNE" id="gwern-twdne" class="link-local docMetadata has-annotation spawns-popup cyxy-trs-source" data-attribute-title="I describe how I made a website displaying random anime faces generated by StyleGAN neural networks, and how it went viral.">back­ground &amp; im­ple­men­ta­tion</a>)</li>
<li><a href="https://www.gwern.net/GPT-2" id="gwern-gpt-2" class="link-local docMetadata has-annotation spawns-popup cyxy-trs-source cyxy-trs-source" data-attribute-title="Retraining SOTA text-generation NNs on new corpuses, such as Project Gutenberg, for high quality English poetry generation">“Fine­tun­ing the <span class="smallcaps-auto cyxy-trs-source">GPT-2-117M</span> Trans­former for Eng­lish Po­etry Gen­er­a­tion”</a></li>
</ul>
</section>
<section id="external-links" class="level1">
<h1><a href="https://www.gwern.net/Faces#external-links" title="Link to section: § &#39;External Links&#39;" class="no-popup cyxy-trs-source">External Links</a></h1>
<ul>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/old.reddit.com/aae34108a9eb44222d7b21a3fab43dc1c4285bb7.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/e23ezq/p_using_stylegan_to_make_a_music_visualizer/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/e23ezq/p_using_stylegan_to_make_a_music_visualizer/ )">“Us­ing Style<span class="smallcaps-auto">GAN</span> to Make a Mu­sic Vi­su­al­izer”</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://medium.com/pickupp/pretrained-anime-stylegan2-convert-to-pytorch-and-editing-images-by-encoder-289a57ac3cab" class="no-popup">“Pre­trained Anime Style<span class="smallcaps-auto">GAN</span> 2—­con­vert to Py­torch and edit­ing im­ages by en­coder”</a>⁠, Allen Ng</p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/soranews24.com/99ce677cc4456f5fdd92b98b651a0c2c3cd92322.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://soranews24.com/2019/02/14/video-shows-off-hundreds-of-beautiful-ai-created-anime-girls-in-less-than-a-minute%E3%80%90video%E3%80%91/" data-attribute-title="(Original URL: https://soranews24.com/2019/02/14/video-shows-off-hundreds-of-beautiful-ai-created-anime-girls-in-less-than-a-minute%E3%80%90video%E3%80%91/ )">“Video shows off hun­dreds of beau­ti­ful AI-cre­ated anime girls in less than a min­ute【Video】”</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://pkhungurn.github.io/talking-head-anime/" id="khungurn-2019" class="docMetadata has-annotation spawns-popup">“Talk­ing Head Anime from a Sin­gle Im­age”</a>⁠, Pramook Khun­gurn</p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/podgorskiy.com/3a1ac0526f0de4dde84b1983b79638654dcc9885.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="http://podgorskiy.com/static/stylegan/stylegan.html" data-attribute-title="(Original URL: http://podgorskiy.com/static/stylegan/stylegan.html )">“Style<span class="smallcaps-auto">GAN</span> in We­bAssem­bly us­ing ten­sor4”</a>⁠, Stanislav Pid­horskyi (<a href="https://github.com/podgorskiy/StyleGANCpp" class="no-popup">code</a>)</p></li>
<li><p class="cyxy-trs-source"><a href="https://github.com/nikhiltiru/stylegan2" class="no-popup">“Style<span class="smallcaps-auto">GAN</span>2: Align, Pro­ject, An­i­mate, Mix Styles and Train”</a>⁠, Nikhil Tiru­mala (“easy-to-use util­ity func­tions to gen­er­ate an­i­ma­tions and mix styles”)</p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/towardsdatascience.com/2020-03-13-alexmartinelli-styleganv2-notesontrainingandlatentspaceexploration.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://towardsdatascience.com/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3" data-attribute-title="(Original URL: https://towardsdatascience.com/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3 )">“Style<span class="smallcaps-auto">GAN</span> v2: notes on train­ing and la­tent space ex­plo­ration”</a>⁠, Alex Mar­tinelli</p></li>
<li><p class="cyxy-trs-source"><a href="https://github.com/l4rz/practical-aspects-of-stylegan2-training" id="lrz-2020" class="docMetadata has-annotation spawns-popup">“Prac­ti­cal as­pects of Style<span class="smallcaps-auto">GAN</span>2 train­ing”</a>⁠/ <a href="https://github.com/l4rz/scaling-up-stylegan2" class="no-popup">“Scal­ing up Style<span class="smallcaps-auto">GAN</span>2”</a>⁠, l4rz (im­prov­ing top­less im­age gen­er­a­tion by hy­per­pa­ra­me­ter tun­ing, model size in­crease, &amp; face-crop­ping data aug­men­ta­tion)</p></li>
<li><p class="cyxy-trs-source"><a href="https://www.youtube.com/watch?v=40FX9HtoUAE" class="has-content spawns-popup">“Mor­ph­ing Anime Girls Quiz”</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/amitness.com/4439814fe2f09038e21c89f8cb4270fddccf3ac4.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://amitness.com/2020/06/google-colaboratory-tips/" data-attribute-title="(Original URL: https://amitness.com/2020/06/google-colaboratory-tips/ )">“Google Co­lab Tips for Power Users”</a></p></li>
<li><p class="cyxy-trs-source"><a href="https://arxiv.org/abs/2103.04922" id="bondtaylor-et-al-2021-3" class="docMetadata has-annotation spawns-popup">“Deep Gen­er­a­tive Mod­el­ling: A Com­par­a­tive Re­view of <span class="smallcaps-auto">VAE</span>s, <span class="smallcaps-auto">GAN</span>s, Nor­mal­iz­ing Flows, En­er­gy-Based and Au­tore­gres­sive Mod­els”</a>⁠, Bond-Tay­lor&nbsp;et&nbsp;al&nbsp;2021</p></li>
<li><p class="cyxy-trs-source">Dis­cus­sion: <a href="https://www.gwern.net/docs/www/old.reddit.com/8b05942f0f814f7dc5b10856d709c6eff41a22bc.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/apq4xu/p_stylegan_on_anime_faces/" data-attribute-title="[P] StyleGAN on Anime Faces (Original URL: https://old.reddit.com/r/MachineLearning/comments/apq4xu/p_stylegan_on_anime_faces/ )">Red­dit</a></p>
<ul>
<li><a href="https://www.reg.ru/blog/anime-generation-with-stylegan/" title="Генерация аниме с помощью нейросети StyleGAN" class="no-popup cyxy-trs-source cyxy-trs-source">Russ­ian trans­la­tion/<wbr>­sum­mary of this page</a></li>
</ul></li>
</ul>
</section>
<section id="appendix" class="level1">
<h1><a href="https://www.gwern.net/Faces#appendix" title="Link to section: § &#39;Appendix&#39;" class="no-popup cyxy-trs-source">Appendix</a></h1>
<p class="cyxy-trs-source">For failed anime ex­per­i­ments us­ing a va­ri­ety of NN ar­chi­tec­tures, see the <a href="https://www.gwern.net/Faces-graveyard" id="gwern-faces-graveyard" class="link-local docMetadata has-annotation spawns-popup"><strong>Anime Neural Net Grave­yard</strong></a>⁠.</p>
</section>
<section class="footnotes" role="doc-endnotes" id="footnotes">
<hr>
<a href="https://www.gwern.net/Faces#footnotes" title="Link to section: § ‘Footnotes’" class="section-self-link no-popup"></a><ol>
<li id="fn1" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn1" title="Link to footnote 1" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Turns out that when train­ing goes <em>re­ally</em> wrong, you can crash many <span class="smallcaps-auto">GAN</span> im­ple­men­ta­tions with ei­ther a seg­fault, in­te­ger over­flow, or di­vi­sion by zero er­ror.<a href="https://www.gwern.net/Faces#fnref1" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn2" title="Link to footnote 2" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Stack<span class="smallcaps-auto">GAN</span>/Stack<span class="smallcaps-auto">GAN</span>++/Pixel<span class="smallcaps-auto">CNN</span> et al are diffi­cult to run as they re­quire a unique im­age em­bed­ding which could only be com­puted in the un­main­tained Torch frame­work us­ing Reed’s prior work on a joint tex­t+im­age em­bed­ding which how­ever does­n’t run on any­thing but the Birds &amp; Flow­ers datasets, and so no one has ever, as far as I am aware, run those im­ple­men­ta­tions on any­thing else—cer­tainly I never man­aged to de­spite quite a few hours try­ing to re­verse-engi­neer the em­bed­ding &amp; var­i­ous im­ple­men­ta­tions.<a href="https://www.gwern.net/Faces#fnref2" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn3" title="Link to footnote 3" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Be sure to check out <a href="https://artbreeder.com/" id="simon-2019" class="docMetadata has-annotation spawns-popup">Art­breeder</a>⁠.<a href="https://www.gwern.net/Faces#fnref3" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn4" title="Link to footnote 4" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Glow’s re­ported re­sults <a href="https://github.com/openai/glow/issues/37" class="no-popup">re­quired &gt;40 <span class="smallcaps-auto">GPU</span>-weeks</a>⁠; Big<span class="smallcaps-auto">GAN</span>’s to­tal com­pute is un­clear as it was trained on a <span class="smallcaps-auto">TPU</span>v3 Google clus­ter but it would ap­pear that a 128px Big<span class="smallcaps-auto">GAN</span> might be ~4 <span class="smallcaps-auto">GPU</span>-months as­sum­ing hard­ware like an 8-<span class="smallcaps-auto">GPU</span> ma­chine, 256px ~8 <span class="smallcaps-auto">GPU</span>-months, and 512px ≫8 <span class="smallcaps-auto">GPU</span>-months, with <span class="smallcaps-auto">VRAM</span> be­ing the main lim­it­ing fac­tor for larger mod­els (although pro­gres­sive grow­ing might be able to cut those es­ti­mates).<a href="https://www.gwern.net/Faces#fnref4" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn5" title="Link to footnote 5" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/anime/2015-saito.pdf" id="masaki-matsui-2015" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Illustration2Vec: A Semantic Vector Representation of Illustrations&#39;, Saito &amp; Matsui"><code>illustration2vec</code></a> is an old &amp; small <span class="smallcaps-auto">CNN</span> trained to pre­dict a few -booru tags on anime im­ages, and so pro­vides an em­bed­ding—but not a good one. The lack of a good em­bed­ding is the ma­jor lim­i­ta­tion for anime deep learn­ing as of Feb­ru­ary 2019. (<a href="https://www.gwern.net/docs/www/old.reddit.com/584c6c362de1ea72bc8962de25d364f366b0a301.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/ )">Deep­Dan­booru</a>⁠, while per­form­ing well ap­par­ent­ly, has not yet been used for em­bed­dings.) An em­bed­ding is nec­es­sary for text → im­age <span class="smallcaps-auto">GAN</span>s, im­age searches &amp; near­est-neigh­bor checks of over­fit­ting, <span class="smallcaps-auto">FID</span> er­rors for ob­jec­tively com­par­ing <span class="smallcaps-auto">GAN</span>s, mini­batch dis­crim­i­na­tion to help the D/<wbr>pro­vide an aux­il­iary loss to sta­bi­lize learn­ing, anime style trans­fer (both for its own sake &amp; for cre­at­ing a ‘StyleDan­booru2018’ to re­duce tex­ture cheat­ing), en­cod­ing into <span class="smallcaps-auto">GAN</span> la­tent spaces for ma­nip­u­la­tion, data clean­ing (to de­tect anom­alous dat­a­points like failed face crop­s), per­cep­tual losses for en­coders or as an ad­di­tional aux­il­iary loss/<wbr>pre­train­ing (like <a href="https://www.gwern.net/docs/www/www.fast.ai/51f7b97683992e9c1476a63377ea01b9648f9db0.html" id="antic-2019" class="docMetadata localArchive has-annotation spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.fast.ai/2019/05/03/decrappify/" data-attribute-title="Decrappification, DeOldification, and Super Resolution (Original URL: https://www.fast.ai/2019/05/03/decrappify/ )">“No<span class="smallcaps-auto">GAN</span>”</a>⁠, which trains a Gen­er­a­tor on a per­cep­tual loss and does <span class="smallcaps-auto">GAN</span> train­ing only for fine­tun­ing) etc. A good tag­ger is also a good start­ing point for do­ing pix­el-level se­man­tic seg­men­ta­tion (via “weak su­per­vi­sion”), which meta­data is key for train­ing some­thing like Nvidi­a’s <a href="https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/" class="no-popup">Gau<span class="smallcaps-auto">GAN</span></a> suc­ces­sor to pix2pix (<a href="https://arxiv.org/abs/1903.07291" id="park-et-al-2019-3" class="docMetadata has-annotation spawns-popup" data-attribute-title="Semantic Image Synthesis with Spatially-Adaptive Normalization">Park&nbsp;et&nbsp;al&nbsp;2019</a>⁠; <a href="https://github.com/NVlabs/SPADE" class="no-popup">source</a>).<a href="https://www.gwern.net/Faces#fnref5" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn6" title="Link to footnote 6" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Tech­ni­cal note: I typ­i­cally train NNs us­ing my work­sta­tion with 2×1080ti <span class="smallcaps-auto">GPU</span>s. For eas­ier com­par­ison, I con­vert all my times to single-<span class="smallcaps-auto">GPU</span> equiv­a­lent (ie “6 <span class="smallcaps-auto">GPU</span>-weeks” means 3 re­al­time/<wbr>wall­clock weeks on my 2 <span class="smallcaps-auto">GPU</span>s).<a href="https://www.gwern.net/Faces#fnref6" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn7" title="Link to footnote 7" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://arxiv.org/abs/1904.06991" id="kynkäänniemi-et-al-2019-4" class="docMetadata has-annotation spawns-popup" data-attribute-title="Improved Precision and Recall Metric for Assessing Generative Models">Kynkään­niemi&nbsp;et&nbsp;al&nbsp;2019</a> ob­serves (§4 “Us­ing pre­ci­sion and re­call to an­a­lyze and im­prove Style<span class="smallcaps-auto">GAN</span>”) that Style<span class="smallcaps-auto">GAN</span> with pro­gres­sive grow­ing dis­abled <em>does</em> work but at some cost to pre­ci­sion/<wbr>re­call qual­ity met­rics; whether this re­flects in­fe­rior per­for­mance on a given train­ing bud­get or an in­her­ent limit—Big<span class="smallcaps-auto">GAN</span> and other self­-at­ten­tion-us­ing <span class="smallcaps-auto">GAN</span>s do not use pro­gres­sive grow­ing at all, sug­gest­ing it is not <em>truly</em> nec­es­sary—is not in­ves­ti­gat­ed. In De­cem­ber 2019, Style<span class="smallcaps-auto">GAN</span> 2 suc­cess­fully dropped pro­gres­sive grow­ing en­tirely at mod­est per­for­mance cost.<a href="https://www.gwern.net/Faces#fnref7" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn8" title="Link to footnote 8" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This has con­fused some peo­ple, so to clar­ify the se­quence of events: I trained my anime face Style<span class="smallcaps-auto">GAN</span> and posted notes on Twit­ter, re­leas­ing an early mod­el; <a href="https://nitter.cc/ak92501" class="no-popup">road­run­ner01</a> gen­er­ated an in­ter­po­la­tion video us­ing said model (but a differ­ent ran­dom seed, of course); this in­ter­po­la­tion video was retweeted by the Japan­ese Twit­ter user <a href="https://nitter.cc/_Ryobot" class="no-popup">_Ry­obot</a>⁠, upon which it went vi­ral and was ‘liked’ by Elon Musk, fur­ther dri­ving vi­ral­ity (19k re­shares, 65k likes, 1.29m watches as of 2019-03-22).<a href="https://www.gwern.net/Faces#fnref8" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn9" title="Link to footnote 9" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Google Co­lab is a free ser­vice in­cludes free <span class="smallcaps-auto">GPU</span> time (up to 12 hours on a small <span class="smallcaps-auto">GPU</span>). Es­pe­cially for peo­ple who do not have a rea­son­ably ca­pa­ble <span class="smallcaps-auto">GPU</span> on their per­sonal com­put­ers (such as all Ap­ple users) or do not want to en­gage in the ad­mit­ted has­sle of rent­ing a real cloud <span class="smallcaps-auto">GPU</span> in­stance, Co­lab can be a great way to play with a pre­trained mod­el, like gen­er­at­ing <span class="smallcaps-auto">GPT-2-117M</span> text com­ple­tions or Style<span class="smallcaps-auto">GAN</span> in­ter­po­la­tion videos, or pro­to­type on tiny prob­lems.</p>
<p class="cyxy-trs-source">How­ev­er, it is a bad idea to try to train real mod­els, like 512–1024px Style<span class="smallcaps-auto">GAN</span>s, on a Co­lab in­stance as the <span class="smallcaps-auto">GPU</span>s are low <span class="smallcaps-auto">VRAM</span>, far slower (6 hours per Style<span class="smallcaps-auto">GAN</span> tick­!), un­wieldy to work with (as one must save snap­shots con­stantly to restart when the ses­sion runs out), does­n’t have a real com­mand-line, etc. Co­lab is just barely ad­e­quate for per­haps 1 or 2 ticks of trans­fer learn­ing, but not more. If you har­bor greater am­bi­tions but still refuse to spend any money (rather than time), Kag­gle has a sim­i­lar ser­vice with P100 <span class="smallcaps-auto">GPU</span> slices rather than K80s. Oth­er­wise, one needs to get ac­cess to real <span class="smallcaps-auto">GPU</span>s.<a href="https://www.gwern.net/Faces#fnref9" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn10" title="Link to footnote 10" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Cu­ri­ous­ly, the ben­e­fit of many more FC lay­ers than usual may have been stum­bled across be­fore: Illustration<span class="smallcaps-auto">GAN</span> found that adding some FC lay­ers seemed to help their <span class="smallcaps-auto">DCGAN</span> gen­er­ate anime faces, and when I &amp; <a href="https://github.com/FeepingCreature" class="no-popup">Feep­ingCrea­ture</a> ex­per­i­mented with adding 2–4 FC lay­ers to <span class="smallcaps-auto">WGAN-GP</span> along Illustration<span class="smallcaps-auto">GAN</span>’s lines, it <em>did</em> help our lack­lus­ter re­sults, and at the time <a href="https://github.com/martinarjovsky/WassersteinGAN/issues/2#issuecomment-278710552" class="no-popup">I spec­u­lated</a> that “the ful­ly-con­nected lay­ers are trans­form­ing the la­tent-<em>z</em>/<wbr>noise into a sort of global tem­plate which the sub­se­quent con­vo­lu­tion lay­ers can then fill in more lo­cal­ly.” But we never dreamed of go­ing as deep as 8!<a href="https://www.gwern.net/Faces#fnref10" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn11" title="Link to footnote 11" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">The Pro<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span> code­base <a href="https://nitter.cc/davidstap/status/1120667403837423616" class="no-popup">re­port­edly does work</a> with con­di­tion­ing, but none of the pa­pers re­port on this func­tion­al­ity and I have not used it my­self.<a href="https://www.gwern.net/Faces#fnref11" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn12" title="Link to footnote 12" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">The la­tent em­bed­ding <em>z</em> is usu­ally gen­er­ated in about the sim­plest pos­si­ble way: draws from the Nor­mal dis­tri­b­u­tion, 𝒩(0,1). A Uni­for­m(−1,1) is some­times used in­stead. There is no good jus­ti­fi­ca­tion for this and some rea­son to think this can be bad (how does a <span class="smallcaps-auto">GAN</span> eas­ily map a dis­crete or bi­nary la­tent fac­tor, such as the pres­ence or ab­sence of the left ear, onto a Nor­mal vari­able?).</p>
<p class="cyxy-trs-source">The <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=26,pdf" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=26,pdf" data-attribute-title="Appendix E: Choosing Latent Spaces (Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=26,pdf )">Big<span class="smallcaps-auto">GAN</span> pa­per ex­plores al­ter­na­tives</a>⁠, find­ing im­prove­ments in train­ing time and/<wbr>or fi­nal qual­ity from us­ing in­stead (in as­cend­ing or­der): a Nor­mal + bi­nary Bernoulli (<em>p</em> = 0.5; per­sonal com­mu­ni­ca­tion, Brock) vari­able, a bi­nary (Bernoul­li), and a <a href="https://en.wikipedia.org/wiki/Rectified_Gaussian_distribution" class="docMetadata has-annotation spawns-popup">Rec­ti­fied Gauss­ian</a> (some­times called a “cen­sored nor­mal” even though that sounds like a <a href="https://en.wikipedia.org/wiki/Truncated_normal_distribution" class="docMetadata has-annotation spawns-popup">trun­cated nor­mal dis­tri­b­u­tion</a> rather than the rec­ti­fied one). The rec­ti­fied Gauss­ian dis­tri­b­u­tion “out­per­forms 𝒩(0,1)(in terms of IS) by 15–20% and tends to re­quire fewer it­er­a­tions.”</p>
<p class="cyxy-trs-source">The down­side is that the <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=4" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=4" data-attribute-title="Section 3.1: Trading Off Variety And Fidelity With The Truncation Trick (Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=4 )">“trun­ca­tion trick”</a>⁠, which yields even larger av­er­age im­prove­ments in im­age qual­ity (at the ex­pense of di­ver­si­ty) does­n’t quite ap­ply, and the rec­ti­fied Gauss­ian sans trun­ca­tion pro­duced sim­i­lar re­sults as the Nor­mal+trun­ca­tion, so Big<span class="smallcaps-auto">GAN</span> re­verted to the de­fault Nor­mal dis­tri­b­u­tion+trun­ca­tion (per­sonal com­mu­ni­ca­tion).</p>
<p class="cyxy-trs-source">The trun­ca­tion trick ei­ther di­rectly ap­plies to some of the other dis­tri­b­u­tions, par­tic­u­larly the Rec­ti­fied Gaus­sian, or could eas­ily be adapt­ed—­pos­si­bly yield­ing an im­prove­ment over ei­ther ap­proach. The Rec­ti­fied Gauss­ian can be trun­cated just like the de­fault Nor­mals can. And for the Bernoul­li, one could de­crease <em>p</em> dur­ing the gen­er­a­tion, or what is prob­a­bly equiv­a­lent, re-sam­ple when­ever the vari­ance (ie squared sum) of all the Bernoulli la­tent vari­ables ex­ceeds a cer­tain con­stant. (With <em>p</em> = 0.5, a la­tent vec­tor of 512 Bernouil­lis would on av­er­age all sum up to sim­ply 0.5 × 512 = 256, with the 2.5%–97.5% quan­tiles be­ing 234–278, so a ‘trun­ca­tion trick’ here might be throw­ing out every vec­tor with a sum above, say, the 80% quan­tile of 266.)</p>
<p class="cyxy-trs-source">One also won­ders about vec­tors which draw from mul­ti­ple dis­tri­b­u­tions rather than just one. Could the Style<span class="smallcaps-auto">GAN</span> 8-FC-layer learned-la­ten­t-vari­able be re­verse-engi­neered? Per­haps the first layer or two merely con­verts the nor­mal in­put into a more use­ful dis­tri­b­u­tion &amp; pa­ra­me­ter­s/<wbr>­train­ing could be saved or in­sight gained by im­i­tat­ing that.<a href="https://www.gwern.net/Faces#fnref12" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn13" title="Link to footnote 13" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Which raises the ques­tion: if you added any or all of those fea­tures, would Style<span class="smallcaps-auto">GAN</span> be­come that much bet­ter? Un­for­tu­nate­ly, while the­o­rists &amp; prac­ti­tion­ers have had many ideas, so far the­ory has proven more fe­cund than fa­tidi­cal and the large-s­cale <span class="smallcaps-auto">GAN</span> ex­per­i­ments nec­es­sary to truly test the sug­ges­tions are too ex­pen­sive for most. Half of these sug­ges­tions are great ideas—but which half?<a href="https://www.gwern.net/Faces#fnref13" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn14" title="Link to footnote 14" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">For more on the choice of con­vo­lu­tion lay­er­s/<wbr>k­er­nel sizes, see Karpa­thy’s 2015 notes for <a href="http://cs231n.github.io/convolutional-networks/" class="no-popup">“CS231n: Con­vo­lu­tional Neural Net­works for Vi­sual Recog­ni­tion”</a>⁠, or take a look at these <a href="https://github.com/vdumoulin/conv_arithmetic" class="no-popup">Con­vo­lu­tion an­i­ma­tions</a> &amp; Yang’s in­ter­ac­tive <a href="https://ezyang.github.io/convolution-visualizer/index.html" class="no-popup">“Con­vo­lu­tion Vi­su­al­izer”</a>⁠.<a href="https://www.gwern.net/Faces#fnref14" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn15" title="Link to footnote 15" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This ob­ser­va­tions ap­ply only to the Gen­er­a­tor in <span class="smallcaps-auto">GAN</span>s (which is what we pri­mar­ily care about); cu­ri­ous­ly, there’s some rea­son to think that <span class="smallcaps-auto">GAN</span> Dis­crim­i­na­tors are in fact mostly mem­o­riz­ing (<a href="https://www.gwern.net/Faces#discriminator-ranking" class="link-self identifier-link-up has-content spawns-popup">see later</a>).<a href="https://www.gwern.net/Faces#fnref15" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn16" title="Link to footnote 16" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">A pos­si­ble al­ter­na­tive is <a href="https://github.com/xinntao/ESRGAN" class="no-popup"><span class="smallcaps-auto">ESRGAN</span></a> (<a href="https://arxiv.org/abs/1809.00219" id="wang-et-al-2018-9" class="docMetadata has-annotation spawns-popup" data-attribute-title="ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks">Wang&nbsp;et&nbsp;al&nbsp;2018</a>).<a href="https://www.gwern.net/Faces#fnref16" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn17" title="Link to footnote 17" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Based on eye­balling the ‘cat’ bar graph in Fig­ure 3 of <a href="https://arxiv.org/abs/1506.03365" id="yu-et-al-2015-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop">Yu&nbsp;et&nbsp;al&nbsp;2015</a>⁠.<a href="https://www.gwern.net/Faces#fnref17" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn18" title="Link to footnote 18" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><span class="smallcaps-auto">CATS</span> offer an amus­ing in­stance of the dan­gers of data aug­men­ta­tion: Pro<span class="smallcaps-auto">GAN</span> used hor­i­zon­tal flip­ping/<wbr>mir­ror­ing for every­thing, be­cause why not? This led to strange Cyril­lic text cap­tions show­ing up in the gen­er­ated cat im­ages. Why not Latin al­pha­bet cap­tions? Be­cause every cat im­age was be­ing shown <em>mir­rored</em> as well as nor­mal­ly! For Style<span class="smallcaps-auto">GAN</span>, mir­ror­ing was dis­abled, so now the lol­cat cap­tions are rec­og­niz­ably Latin al­pha­bet­i­cal, and even <em>al­most</em> Eng­lish words. This demon­strates that even datasets where left­/<wbr>right does­n’t seem to mat­ter, like cat pho­tos, can sur­prise you.<a href="https://www.gwern.net/Faces#fnref18" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn19" title="Link to footnote 19" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">I es­ti­mated the to­tal cost us­ing <span class="smallcaps-auto">AWS</span> EC2 pre­emptible hourly costs on 2019-03-15 as fol­lows:</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">1 <span class="smallcaps-auto cyxy-trs-source">GPU</span>: <code>p2.xlarge</code> in­stance in <code>us-east-2a</code>, Half of a K80 (12GB <span class="smallcaps-auto cyxy-trs-source">VRAM</span>): $0.3235/<wbr>hour</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">2 <span class="smallcaps-auto cyxy-trs-source">GPU</span>s: NA—there is no <a href="https://aws.amazon.com/ec2/instance-types/p2/" class="no-popup cyxy-trs-source">P2 in­stance</a> with 2 <span class="smallcaps-auto cyxy-trs-source">GPU</span>s, only 1/<wbr>8/<wbr>16</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">8 <span class="smallcaps-auto cyxy-trs-source">GPU</span>s: <code>p2.8xlarge</code> in <code>us-east-2a</code>, 8 halves of K80s (12GB <span class="smallcaps-auto cyxy-trs-source">VRAM</span> each): $2.160/<wbr>hour</li>
</ul>
<p class="cyxy-trs-source">As usu­al, there is sub­lin­ear scal­ing, and larger in­stances cost dis­pro­por­tion­ately more, be­cause one is pay­ing for faster wall­clock train­ing (time is valu­able) and for not hav­ing to cre­ate a dis­trib­uted in­fra­struc­ture which can ex­ploit the cheap single-<span class="smallcaps-auto">GPU</span> in­stances.</p>
<p class="cyxy-trs-source">This cost es­ti­mate does not count ad­di­tional costs like hard drive space. In ad­di­tion to the dataset size (the Style<span class="smallcaps-auto">GAN</span> data en­cod­ing is ~18× larger than the raw data size, so a 10GB folder of im­ages → 200GB of <code>.tfrecords</code>), you would need at least 100GB <span class="smallcaps-auto">HDD</span> (50GB for the OS, and 50GB for check­points/<wbr>im­ages/<wbr>etc to avoid crashes from run­ning out of space).<a href="https://www.gwern.net/Faces#fnref19" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn20" title="Link to footnote 20" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">I re­gard this as a flaw in Style<span class="smallcaps-auto">GAN</span> &amp; TF in gen­er­al. Com­put­ers are more than fast enough to load &amp; process im­ages asyn­chro­nously us­ing a few worker threads, and work­ing with a di­rec­tory of im­ages (rather than a spe­cial bi­nary for­mat 10–20× larg­er) avoids im­pos­ing se­ri­ous bur­dens on the user &amp; hard dri­ve. Py­Torch <span class="smallcaps-auto">GAN</span>s al­most al­ways avoid this mis­take, and are much more pleas­ant to work with as one can freely mod­ify the dataset be­tween (and even dur­ing) runs.<a href="https://www.gwern.net/Faces#fnref20" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn21" title="Link to footnote 21" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">For ex­am­ple, my Dan­booru2018 anime por­trait dataset is 16GB, but the Style<span class="smallcaps-auto">GAN</span> en­coded dataset is 296GB.<a href="https://www.gwern.net/Faces#fnref21" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn22" title="Link to footnote 22" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This may be why some peo­ple re­port that Style<span class="smallcaps-auto">GAN</span> just crashes for them &amp; they can’t fig­ure out why. They should try chang­ing their dataset <span class="smallcaps-auto">JPG</span> ↔︎ <span class="smallcaps-auto">PNG</span>.<a href="https://www.gwern.net/Faces#fnref22" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn23" title="Link to footnote 23" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">That is, in train­ing G, the G’s fake im­ages must be aug­mented be­fore be­ing passed to the D for rat­ing; and in train­ing D, both real &amp; fake im­ages must be aug­mented the same way be­fore be­ing passed to D. Pre­vi­ous­ly, all <span class="smallcaps-auto">GAN</span> re­searchers ap­pear to have as­sumed that one should only aug­ment real im­ages be­fore pass­ing to D dur­ing D train­ing, which con­ve­niently can be done at dataset cre­ation; un­for­tu­nate­ly, this hid­den as­sump­tion turns out to be about the most harm­ful way pos­si­ble!<a href="https://www.gwern.net/Faces#fnref23" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn24" title="Link to footnote 24" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">I would de­scribe the dis­tinc­tions as: Soft­ware 0.0 was im­per­a­tive pro­gram­ming for ham­mer­ing out clock­work mech­a­nism; Soft­ware 1.0 was de­clar­a­tive pro­gram­ming with spec­i­fi­ca­tion of pol­i­cy; and Soft­ware 2.0 is deep learn­ing by gar­den­ing loss func­tions (with every­thing else, from model arch to which dat­a­points to la­bel ide­ally learned end-to-end). Con­tin­u­ing the the­me, we might say that di­a­logue with mod­els, like <a href="https://www.gwern.net/GPT-3#prompts-as-programming" id="gwern-gpt-3-prompts-as-programming" class="link-local docMetadata has-annotation spawns-popup">“prompt pro­gram­ming”</a>⁠, are “Soft­ware 3.0”…<a href="https://www.gwern.net/Faces#fnref24" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn25" title="Link to footnote 25" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">But you may not want to–re­mem­ber the lol­cat cap­tions!<a href="https://www.gwern.net/Faces#fnref25" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn26" title="Link to footnote 26" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Note: If you use a differ­ent com­mand to re­size, check it thor­ough­ly. With Im­ageMag­ick, if you use the <code>^</code> op­er­a­tor like <code>-resize 512x512^</code>, you will not get ex­actly 512×512px im­ages as you need; while if you use the <code>!</code> op­er­a­tor like <code>-resize 512x512!</code>, the im­ages will be ex­actly 512×512px but the as­pect ra­tios will dis­torted to make im­ages fit, and this may con­fuse any­thing you are train­ing by in­tro­duc­ing un­nec­es­sary mean­ing­less dis­tor­tions &amp; will make any gen­er­ated im­ages look bad.<a href="https://www.gwern.net/Faces#fnref26" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn27" title="Link to footnote 27" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">If you are us­ing Python 2, you will get <code>print</code> syn­tax er­ror mes­sages; if you are us­ing Python 3–3.6, you will get ‘type hint’ er­rors.<a href="https://www.gwern.net/Faces#fnref27" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn28" title="Link to footnote 28" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/old.reddit.com/d1f03c07a54ed8ab2baf22b1ae6782640f9964f0.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/ecji6v/removing_blob_artifact_from_stylegan_generations/" data-attribute-title="Removing blob artifact from StyleGAN generations without retraining. Inspired by StyleGAN2 (Original URL: https://old.reddit.com/r/MachineLearning/comments/ecji6v/removing_blob_artifact_from_stylegan_generations/ )">Stas Pod­gorskiy</a> has demon­strated that the <a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-up has-content spawns-popup">Style<span class="smallcaps-auto">GAN</span> 2</a> cor­rec­tion can be re­verse-engi­neered and ap­plied back to Style<span class="smallcaps-auto">GAN</span> 1 gen­er­a­tors if nec­es­sary.<a href="https://www.gwern.net/Faces#fnref28" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn29" title="Link to footnote 29" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This makes it con­form to a trun­cated nor­mal dis­tri­b­u­tion; why trun­cated rather than rec­ti­fied/<wbr>win­sorized at a max like 0.5 or 1.0 in­stead? Be­cause then many, pos­si­bly most, of the la­tent vari­ables would all be at the max, in­stead of smoothly spread out over the per­mit­ted range.<a href="https://www.gwern.net/Faces#fnref29" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn30" title="Link to footnote 30" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">No mini­batches are used, so this is much slower than nec­es­sary.<a href="https://www.gwern.net/Faces#fnref30" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn31" title="Link to footnote 31" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Sim­ply en­cod­ing each pos­si­ble tag as a one-hot cat­e­gor­i­cal vari­able would scale poor­ly: in the worst case, Dan­booru2020 has &gt;434,000 pos­si­ble tags. If that was passed into a ful­ly-con­nected lay­er, which out­put a 1024-long em­bed­ding, then that would use up 434,000 × 1,024 = <em>444 mil­lion</em> pa­ra­me­ters! The em­bed­ding would be larger than the ac­tual Style<span class="smallcaps-auto">GAN</span> mod­el, and ac­cord­ingly ex­pen­sive. <span class="smallcaps-auto">RNN</span>s his­tor­i­cally are com­monly used to con­vert text in­puts to an em­bed­ding for a <span class="smallcaps-auto">CNN</span> to process, but they are finicky and hard to work with. <a href="https://en.wikipedia.org/wiki/Word2vec" class="docMetadata has-annotation spawns-popup">Word2vec</a> dos­n’t work be­cause, as the name sug­gests, it only con­verts a sin­gle tag/<wbr>­word at a time into an em­bed­ding; doc2vec is its equiv­a­lent for se­quences of text. If we were do­ing it in 2021, we would prob­a­bly just throw a Trans­former at it (at­ten­tion is all you need!) with a win­dow of 512 to­kens or some­thing. (You don’t need that many tags, and it’s un­likely that a fea­si­ble model would make good use of ~100 tags any­way.)<a href="https://www.gwern.net/Faces#fnref31" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn32" title="Link to footnote 32" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Keep this change in mind if you run into er­rors like <code>ValueError: Cannot feed value of shape (1024,) for Tensor 'G/dlatent_avg/new_value:0', which has shape '(512,)'</code> try­ing to reuse the mod­el. Try in­crease the size of the <code>latent_size</code> and <code>dlatent_size</code> from 512 to 1024 in the <code>networks_stylegan2.py</code> con­fig.<a href="https://www.gwern.net/Faces#fnref32" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn33" title="Link to footnote 33" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Mov­ing self­-at­ten­tion around in Big<span class="smallcaps-auto">GAN</span> also makes sur­pris­ingly lit­tle differ­ence. We dis­cussed it with Big<span class="smallcaps-auto">GAN</span>’s Brock, and he noted that self­-at­ten­tion was ex­pen­sive &amp; never seemed to be as im­por­tant to Big<span class="smallcaps-auto">GAN</span> as one would as­sume (com­pared to other im­prove­ments like the or­thog­o­nal reg­u­lar­iza­tion, large mod­els, and large mini­batch­es). Given ex­am­ples like <a href="https://openai.com/blog/jukebox/" id="dhariwal-et-al-2020" class="docMetadata has-annotation spawns-popup" data-attribute-title="We&#39;re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We&#39;re releasing the model weights and code, along with a tool to explore the generated samples.">Juke­box</a>⁠, <a href="https://compvis.github.io/taming-transformers/" id="esser-et-al-2020" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;VQGAN: Taming Transformers for High-Resolution Image Synthesis&#39;, Esser et al 2020"><span class="smallcaps-auto">VQGAN</span></a>⁠, <a href="https://arxiv.org/abs/2009.04433" id="han-et-al-2020-9" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;not-so-BigGAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution&#39;, Han et al 2020">not-so-BIg<span class="smallcaps-auto">GAN</span></a>⁠, or <a href="https://openai.com/blog/dall-e/" id="ramesh-et-al-2021" class="docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">DALL·E</span></a>⁠, I sus­pect that the ben­e­fits of self­-at­ten­tion may be rel­a­tively min­i­mal at the raw pixel lev­el, and bet­ter fo­cused on the ‘se­man­tic level’ in some sense, such as in pro­cess­ing the la­tent vec­tor or <span class="smallcaps-auto">VQ-VAE</span> to­kens.<a href="https://www.gwern.net/Faces#fnref33" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn34" title="Link to footnote 34" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">The ques­tion is not whether one is to start with an ini­tial­iza­tion at all, but whether to start with one which does every­thing poor­ly, or one which does a few sim­i­lar things well. Sim­i­lar­ly, from a Bayesian sta­tis­tics per­spec­tive, the ques­tion of what <a href="https://en.wikipedia.org/wiki/Prior_probability" class="docMetadata has-annotation spawns-popup">prior</a> to use is one that every­one faces; how­ev­er, many ap­proaches sweep it un­der the rug and effec­tively as­sume a de­fault flat prior that is con­sis­tently bad and op­ti­mal for no mean­ing­ful prob­lem ever.<a href="https://www.gwern.net/Faces#fnref34" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn35" title="Link to footnote 35" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><span class="smallcaps-auto">ADA</span>/Style<span class="smallcaps-auto">GAN</span>3 is re­port­edly much more sam­ple-effi­cient and re­duces the need for trans­fer learn­ing: <a href="https://arxiv.org/abs/2006.06676#nvidia" id="karras-et-al-2020-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="ADA/StyleGAN3: Training Generative Adversarial Networks with Limited Data">Kar­ras&nbsp;et&nbsp;al&nbsp;2020</a>⁠. But if a rel­e­vant model is avail­able, it should still be used. Back­port­ing the <span class="smallcaps-auto">ADA</span> data aug­men­ta­tion trick to Style<span class="smallcaps-auto">GAN</span>1–2 will be a ma­jor up­grade.<a href="https://www.gwern.net/Faces#fnref35" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn36" title="Link to footnote 36" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">There are more real Asuka im­ages than Holo to be­gin with, but there is no par­tic­u­lar rea­son for the 10× data aug­men­ta­tion com­pared to the Holo’s 3×—the data aug­men­ta­tions were just done at differ­ent times and hap­pened to have less or more aug­men­ta­tions en­abled.<a href="https://www.gwern.net/Faces#fnref36" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn37" title="Link to footnote 37" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">A fa­mous ex­am­ple is char­ac­ter de­signer <a href="https://www.gwern.net/images/eva/1996-sadamoto-howtodrawshinjinadia.jpg" class="invertible-auto invertible has-content spawns-popup" data-image-height="488" data-image-width="492">Yoshiyuki Sadamoto demon­strat­ing how to</a> turn <a href="https://en.wikipedia.org/wiki/List_of_Nadia%3A_The_Secret_of_Blue_Water_characters#Nadia" class="docMetadata has-annotation spawns-popup">Na­dia</a> (<a href="https://en.wikipedia.org/wiki/Nadia%3A_The_Secret_of_Blue_Water" class="docMetadata has-annotation spawns-popup"><em>Na­dia: The Se­cret of Blue Wa­ter</em></a>) into <a href="https://en.wikipedia.org/wiki/Shinji_Ikari" class="docMetadata has-annotation spawns-popup">Shinji Ikari</a> (<em>Evan­ge­lion</em>).<a href="https://www.gwern.net/Faces#fnref37" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn38" title="Link to footnote 38" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">It turns out that this la­tent vec­tor trick <em>does</em> work. In­trigu­ing­ly, it works even bet­ter to do ‘model av­er­ag­ing’ or ‘model blend­ing’ (<a href="https://www.gwern.net/docs/www/www.justinpinkney.com/e28f418fd89a348f1e659c67cec5bb17d92d3894.html" id="pinkney-2020" class="docMetadata localArchive has-annotation spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.justinpinkney.com/stylegan-network-blending/" data-attribute-title="(Original URL: https://www.justinpinkney.com/stylegan-network-blending/ )">“Style<span class="smallcaps-auto">GAN</span> net­work blend­ing”</a>⁠/ <a href="https://arxiv.org/abs/2010.05334" id="pinkney-adler-2020-0" class="docMetadata has-annotation spawns-popup">“Res­o­lu­tion De­pen­dent <span class="smallcaps-auto">GAN</span> In­ter­po­la­tion for Con­trol­lable Im­age Syn­the­sis Be­tween Do­mains”</a>⁠, Pinkney &amp; Adler 2020): re­train model A on dataset B, and then take a weighted av­er­age of the 2 mod­els (you av­er­age them, pa­ra­me­ter by pa­ra­me­ter, and re­mark­ably, that Just Works, or you can swap out lay­ers be­tween mod­el­s), and then you can cre­ate faces which are ar­bi­trar­ily in be­tween A and B. So for ex­am­ple, you can <a href="https://www.youtube.com/watch?v=W-VRdE7zagw" class="has-content spawns-popup">blend <span class="smallcaps-auto">FFHQ</span>/Western-animation faces</a> (<a href="https://colab.research.google.com/drive/1tputbmA9EaXs9HL9iO21g7xN7jz_Xrko" title="&#39;Network blending in StyleGAN: Swapping layers between two models in StyleGAN gives some interesting results. You need a base model and a second model which has been fine-tuned from the base.&#39;, Buntworthy" class="no-popup">Co­lab note­book</a>), <a href="https://nitter.cc/Buntworthy/status/1296208260421361665" title="I just tried my StyleGAN layer swapping method the other way round to what I&#39;d been doing before. So making the ukiyo-e model human (rather than the other way around) and I love the results!" class="no-popup"><em>ukiy­o-e</em>/<span class="smallcaps-auto">FFHQ</span> faces</a>⁠, <a href="https://nitter.cc/arfafax/status/1297681537337446400" title="Combining my cross-model interpolation with @Buntworthy&#39;s layer swapping idea. Here the different resolution layers are being interpolated at different rates between furry, FFHQ, and @KitsuneKey&#39;s foxes. p0 is 4x4 and 8x8, p1 is 16x16 to 128x128, and p2 is 256x256 to 512x512." class="no-popup">furries/foxes/<span class="smallcaps-auto">FFHQ</span> faces</a>⁠, or even <a href="https://nitter.cc/arfafax/status/1296084902928986113" title="Cross-model interpolations are one of those neat hidden features that arise from transfer learning. Here I&#39;m interpolating between 5 StyleGAN2 models: furry, FFHQ, anime, ponies, and @KitsuneKey&#39;s fox model. All were trained off the same base model, which makes blending possible." class="no-popup">furries/foxes/<span class="smallcaps-auto">FFHQ</span>/anime/ponies</a>⁠.<a href="https://www.gwern.net/Faces#fnref38" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn39" title="Link to footnote 39" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">In ret­ro­spect, this should­n’t’ve sur­prised me.<a href="https://www.gwern.net/Faces#fnref39" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn40" title="Link to footnote 40" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">There is for other ar­chi­tec­tures like flow-based ones such as Glow, and this is one of their ben­e­fit­s–while the re­quire­ment to be made out of build­ing blocks which <em>can</em> be run back­wards &amp; for­wards equally well, to be ‘in­vert­ible’, is cur­rently ex­tremely ex­pen­sive and the re­sults not com­pet­i­tive ei­ther in fi­nal im­age qual­ity or com­pute re­quire­ments, the in­vert­ibil­ity means that en­cod­ing an ar­bi­trary real im­age to get its in­ferred la­tents Just Works™ and one can eas­ily morph be­tween 2 ar­bi­trary im­ages, or en­code an ar­bi­trary im­age &amp; edit it in the la­tent space to do things like ad­d/<wbr>re­move glasses from a face or cre­ate an op­po­sites-ex ver­sion.<a href="https://www.gwern.net/Faces#fnref40" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn41" title="Link to footnote 41" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This fi­nal ap­proach is, in­ter­est­ing­ly, the his­tor­i­cal rea­son back­prop­a­ga­tion was in­vent­ed: it cor­re­sponds to <em>plan­ning in a model</em>. For ex­am­ple, in plan­ning the flight path of an air­plane (<a href="https://www.gwern.net/docs/statistics/decision/1960-kelley.pdf" id="kelley-1960" class="docMetadata has-annotation spawns-popup" data-attribute-title="Gradient Theory of Optimal Flight Paths">Kel­ley 1960</a>⁠/ <a href="https://www.gwern.net/docs/ai/1962-bryson.pdf" id="bryson-denham-1962" class="docMetadata has-annotation spawns-popup" data-attribute-title="A Steepest-Ascent Method for Solving Optimum Programming Problems">Bryson &amp; Den­ham 1962</a>): the des­ti­na­tion or ‘out­put’ is fixed, the aero­dy­nam­ic­s+­geog­ra­phy or ‘model pa­ra­me­ters’ are also fixed, and the ques­tion is what ac­tions de­ter­min­ing a flight path will re­duce the loss func­tion of time or fuel spent. One starts with a ran­dom set of ac­tions pick­ing a ran­dom flight path, runs it for­ward through the en­vi­ron­ment mod­el, gets a fi­nal time/<wbr>­fuel spent, and then back­prop­a­gates through the model to get the gra­di­ents for the flight path, ad­just­ing the flight path to­wards a new set of ac­tions which will slightly re­duce the time/<wbr>­fuel spent; the new ac­tions are used to plan out the flight to get a new loss, and so on, un­til a lo­cal min­i­mum of the ac­tions has been found. This works with non-s­to­chas­tic prob­lems; for sto­chas­tic ones where the path can’t be guar­an­teed to be ex­e­cut­ed, “mod­el-pre­dic­tive con­trol” can be used to re­plan at every step and ex­e­cute ad­just­ments as nec­es­sary. An­other in­ter­est­ing use of back­prop­a­ga­tion for out­puts is <a href="https://arxiv.org/abs/1906.06565" id="zhang-et-al-2019-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Deep Set Prediction Networks">Zhang&nbsp;et&nbsp;al&nbsp;2019</a> which tack­les the long-s­tand­ing prob­lem of how to get NNs to out­put <em>sets</em> rather than list out­puts by gen­er­at­ing a pos­si­ble set out­put &amp; re­fin­ing it via back­prop­a­ga­tion.<a href="https://www.gwern.net/Faces#fnref41" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn42" title="Link to footnote 42" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><span class="smallcaps-auto">SGD</span> is com­mon, but a sec­ond-order al­go­rithm like <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" class="docMetadata has-annotation spawns-popup">Lim­it­ed-mem­ory <span class="smallcaps-auto">BFGS</span></a> is often used in these ap­pli­ca­tions in or­der to run as few it­er­a­tions as pos­si­ble.<a href="https://www.gwern.net/Faces#fnref42" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn43" title="Link to footnote 43" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://arxiv.org/abs/1907.07171" id="jahanian-et-al-2019-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="On the &#39;steerability&#39; of generative adversarial networks">Ja­han­ian&nbsp;et&nbsp;al&nbsp;2019</a> shows that Big<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span> la­tent em­bed­dings can also go be­yond what one might ex­pect, to in­clude zooms, trans­la­tions, and other trans­forms.<a href="https://www.gwern.net/Faces#fnref43" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn44" title="Link to footnote 44" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Flow mod­els have other ad­van­tages, mostly stem­ming from the max­i­mum like­li­hood train­ing ob­jec­tive. Since the im­age can be prop­a­gated back­wards and for­wards loss­less­ly, in­stead of be­ing lim­ited to gen­er­at­ing ran­dom sam­ples like a <span class="smallcaps-auto">GAN</span>, it’s pos­si­ble to cal­cu­late the ex­act prob­a­bil­ity of an im­age, en­abling max­i­mum like­li­hood as a loss to op­ti­mize, and drop­ping the Dis­crim­i­na­tor en­tire­ly. With no <span class="smallcaps-auto">GAN</span> dy­nam­ics, there’s no worry about weird train­ing dy­nam­ics, and the like­li­hood loss also for­bids ‘mode drop­ping’: the flow model can’t sim­ply con­spire with a Dis­crim­i­na­tor to for­get pos­si­ble im­ages.<a href="https://www.gwern.net/Faces#fnref44" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn45" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn45" title="Link to footnote 45" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> 2 is more com­pu­ta­tion­ally ex­pen­sive but Kar­ras et al op­ti­mized the code­base to make up for it, keep­ing to­tal com­pute con­stant.<a href="https://www.gwern.net/Faces#fnref45" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><a href="https://www.gwern.net/Faces#fn46" title="Link to footnote 46" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Back­up-backup mir­ror: <code>rsync rsync://78.46.86.149:873/biggan/2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl.xz ./</code><a href="https://www.gwern.net/Faces#fnref46" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><div id="sidenote-column-left" class="footnotes" style=""><div class="sidenote" id="sn2" style="top: 1957px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn2" title="Link to footnote 2" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Stack<span class="smallcaps-auto">GAN</span>/Stack<span class="smallcaps-auto">GAN</span>++/Pixel<span class="smallcaps-auto">CNN</span> et al are diffi­cult to run as they re­quire a unique im­age em­bed­ding which could only be com­puted in the un­main­tained Torch frame­work us­ing Reed’s prior work on a joint tex­t+im­age em­bed­ding which how­ever does­n’t run on any­thing but the Birds &amp; Flow­ers datasets, and so no one has ever, as far as I am aware, run those im­ple­men­ta­tions on any­thing else—cer­tainly I never man­aged to de­spite quite a few hours try­ing to re­verse-engi­neer the em­bed­ding &amp; var­i­ous im­ple­men­ta­tions.<a href="https://www.gwern.net/Faces#fnref2" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn2">2</a></div><div class="sidenote" id="sn4" style="top: 2240px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn4" title="Link to footnote 4" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Glow’s re­ported re­sults <a href="https://github.com/openai/glow/issues/37" class="no-popup">re­quired &gt;40 <span class="smallcaps-auto">GPU</span>-weeks</a>⁠; Big<span class="smallcaps-auto">GAN</span>’s to­tal com­pute is un­clear as it was trained on a <span class="smallcaps-auto">TPU</span>v3 Google clus­ter but it would ap­pear that a 128px Big<span class="smallcaps-auto">GAN</span> might be ~4 <span class="smallcaps-auto">GPU</span>-months as­sum­ing hard­ware like an 8-<span class="smallcaps-auto">GPU</span> ma­chine, 256px ~8 <span class="smallcaps-auto">GPU</span>-months, and 512px ≫8 <span class="smallcaps-auto">GPU</span>-months, with <span class="smallcaps-auto">VRAM</span> be­ing the main lim­it­ing fac­tor for larger mod­els (although pro­gres­sive grow­ing might be able to cut those es­ti­mates).<a href="https://www.gwern.net/Faces#fnref4" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn4">4</a></div><div class="sidenote" id="sn6" style="top: 3577px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn6" title="Link to footnote 6" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Tech­ni­cal note: I typ­i­cally train NNs us­ing my work­sta­tion with 2×1080ti <span class="smallcaps-auto">GPU</span>s. For eas­ier com­par­ison, I con­vert all my times to single-<span class="smallcaps-auto">GPU</span> equiv­a­lent (ie “6 <span class="smallcaps-auto">GPU</span>-weeks” means 3 re­al­time/<wbr>wall­clock weeks on my 2 <span class="smallcaps-auto">GPU</span>s).<a href="https://www.gwern.net/Faces#fnref6" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn6">6</a></div><div class="sidenote" id="sn8" style="top: 5167px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn8" title="Link to footnote 8" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This has con­fused some peo­ple, so to clar­ify the se­quence of events: I trained my anime face Style<span class="smallcaps-auto">GAN</span> and posted notes on Twit­ter, re­leas­ing an early mod­el; <a href="https://nitter.cc/ak92501" class="no-popup">road­run­ner01</a> gen­er­ated an in­ter­po­la­tion video us­ing said model (but a differ­ent ran­dom seed, of course); this in­ter­po­la­tion video was retweeted by the Japan­ese Twit­ter user <a href="https://nitter.cc/_Ryobot" class="no-popup">_Ry­obot</a>⁠, upon which it went vi­ral and was ‘liked’ by Elon Musk, fur­ther dri­ving vi­ral­ity (19k re­shares, 65k likes, 1.29m watches as of 2019-03-22).<a href="https://www.gwern.net/Faces#fnref8" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn8">8</a></div><div class="sidenote" id="sn10" style="top: 6227px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn10" title="Link to footnote 10" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Cu­ri­ous­ly, the ben­e­fit of many more FC lay­ers than usual may have been stum­bled across be­fore: Illustration<span class="smallcaps-auto">GAN</span> found that adding some FC lay­ers seemed to help their <span class="smallcaps-auto">DCGAN</span> gen­er­ate anime faces, and when I &amp; <a href="https://github.com/FeepingCreature" class="no-popup">Feep­ingCrea­ture</a> ex­per­i­mented with adding 2–4 FC lay­ers to <span class="smallcaps-auto">WGAN-GP</span> along Illustration<span class="smallcaps-auto">GAN</span>’s lines, it <em>did</em> help our lack­lus­ter re­sults, and at the time <a href="https://github.com/martinarjovsky/WassersteinGAN/issues/2#issuecomment-278710552" class="no-popup">I spec­u­lated</a> that “the ful­ly-con­nected lay­ers are trans­form­ing the la­tent-<em>z</em>/<wbr>noise into a sort of global tem­plate which the sub­se­quent con­vo­lu­tion lay­ers can then fill in more lo­cal­ly.” But we never dreamed of go­ing as deep as 8!<a href="https://www.gwern.net/Faces#fnref10" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn10">10</a></div><div class="sidenote cut-off" id="sn12" style="top: 6530px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn12" title="Link to footnote 12" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">The la­tent em­bed­ding <em>z</em> is usu­ally gen­er­ated in about the sim­plest pos­si­ble way: draws from the Nor­mal dis­tri­b­u­tion, 𝒩(0,1). A Uni­for­m(−1,1) is some­times used in­stead. There is no good jus­ti­fi­ca­tion for this and some rea­son to think this can be bad (how does a <span class="smallcaps-auto">GAN</span> eas­ily map a dis­crete or bi­nary la­tent fac­tor, such as the pres­ence or ab­sence of the left ear, onto a Nor­mal vari­able?).</p>
<p class="cyxy-trs-source">The <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=26,pdf" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=26,pdf" data-attribute-title="Appendix E: Choosing Latent Spaces (Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=26,pdf )">Big<span class="smallcaps-auto">GAN</span> pa­per ex­plores al­ter­na­tives</a>⁠, find­ing im­prove­ments in train­ing time and/<wbr>or fi­nal qual­ity from us­ing in­stead (in as­cend­ing or­der): a Nor­mal + bi­nary Bernoulli (<em>p</em> = 0.5; per­sonal com­mu­ni­ca­tion, Brock) vari­able, a bi­nary (Bernoul­li), and a <a href="https://en.wikipedia.org/wiki/Rectified_Gaussian_distribution" class="docMetadata has-annotation spawns-popup">Rec­ti­fied Gauss­ian</a> (some­times called a “cen­sored nor­mal” even though that sounds like a <a href="https://en.wikipedia.org/wiki/Truncated_normal_distribution" class="docMetadata has-annotation spawns-popup">trun­cated nor­mal dis­tri­b­u­tion</a> rather than the rec­ti­fied one). The rec­ti­fied Gauss­ian dis­tri­b­u­tion “out­per­forms 𝒩(0,1)(in terms of IS) by 15–20% and tends to re­quire fewer it­er­a­tions.”</p>
<p class="cyxy-trs-source">The down­side is that the <a href="https://www.gwern.net/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=4" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=4" data-attribute-title="Section 3.1: Trading Off Variety And Fidelity With The Truncation Trick (Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=4 )">“trun­ca­tion trick”</a>⁠, which yields even larger av­er­age im­prove­ments in im­age qual­ity (at the ex­pense of di­ver­si­ty) does­n’t quite ap­ply, and the rec­ti­fied Gauss­ian sans trun­ca­tion pro­duced sim­i­lar re­sults as the Nor­mal+trun­ca­tion, so Big<span class="smallcaps-auto">GAN</span> re­verted to the de­fault Nor­mal dis­tri­b­u­tion+trun­ca­tion (per­sonal com­mu­ni­ca­tion).</p>
<p class="cyxy-trs-source">The trun­ca­tion trick ei­ther di­rectly ap­plies to some of the other dis­tri­b­u­tions, par­tic­u­larly the Rec­ti­fied Gaus­sian, or could eas­ily be adapt­ed—­pos­si­bly yield­ing an im­prove­ment over ei­ther ap­proach. The Rec­ti­fied Gauss­ian can be trun­cated just like the de­fault Nor­mals can. And for the Bernoul­li, one could de­crease <em>p</em> dur­ing the gen­er­a­tion, or what is prob­a­bly equiv­a­lent, re-sam­ple when­ever the vari­ance (ie squared sum) of all the Bernoulli la­tent vari­ables ex­ceeds a cer­tain con­stant. (With <em>p</em> = 0.5, a la­tent vec­tor of 512 Bernouil­lis would on av­er­age all sum up to sim­ply 0.5 × 512 = 256, with the 2.5%–97.5% quan­tiles be­ing 234–278, so a ‘trun­ca­tion trick’ here might be throw­ing out every vec­tor with a sum above, say, the 80% quan­tile of 266.)</p>
<p class="cyxy-trs-source">One also won­ders about vec­tors which draw from mul­ti­ple dis­tri­b­u­tions rather than just one. Could the Style<span class="smallcaps-auto">GAN</span> 8-FC-layer learned-la­ten­t-vari­able be re­verse-engi­neered? Per­haps the first layer or two merely con­verts the nor­mal in­put into a more use­ful dis­tri­b­u­tion &amp; pa­ra­me­ter­s/<wbr>­train­ing could be saved or in­sight gained by im­i­tat­ing that.<a href="https://www.gwern.net/Faces#fnref12" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn12">12</a></div><div class="sidenote" id="sn14" style="top: 7219px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn14" title="Link to footnote 14" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">For more on the choice of con­vo­lu­tion lay­er­s/<wbr>k­er­nel sizes, see Karpa­thy’s 2015 notes for <a href="http://cs231n.github.io/convolutional-networks/" class="no-popup">“CS231n: Con­vo­lu­tional Neural Net­works for Vi­sual Recog­ni­tion”</a>⁠, or take a look at these <a href="https://github.com/vdumoulin/conv_arithmetic" class="no-popup">Con­vo­lu­tion an­i­ma­tions</a> &amp; Yang’s in­ter­ac­tive <a href="https://ezyang.github.io/convolution-visualizer/index.html" class="no-popup">“Con­vo­lu­tion Vi­su­al­izer”</a>⁠.<a href="https://www.gwern.net/Faces#fnref14" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn14">14</a></div><div class="sidenote" id="sn16" style="top: 9676px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn16" title="Link to footnote 16" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">A pos­si­ble al­ter­na­tive is <a href="https://github.com/xinntao/ESRGAN" class="no-popup"><span class="smallcaps-auto">ESRGAN</span></a> (<a href="https://arxiv.org/abs/1809.00219" id="wang-et-al-2018-9" class="docMetadata has-annotation spawns-popup" data-attribute-title="ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks">Wang&nbsp;et&nbsp;al&nbsp;2018</a>).<a href="https://www.gwern.net/Faces#fnref16" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn16">16</a></div><div class="sidenote" id="sn18" style="top: 12283px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn18" title="Link to footnote 18" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><span class="smallcaps-auto">CATS</span> offer an amus­ing in­stance of the dan­gers of data aug­men­ta­tion: Pro<span class="smallcaps-auto">GAN</span> used hor­i­zon­tal flip­ping/<wbr>mir­ror­ing for every­thing, be­cause why not? This led to strange Cyril­lic text cap­tions show­ing up in the gen­er­ated cat im­ages. Why not Latin al­pha­bet cap­tions? Be­cause every cat im­age was be­ing shown <em>mir­rored</em> as well as nor­mal­ly! For Style<span class="smallcaps-auto">GAN</span>, mir­ror­ing was dis­abled, so now the lol­cat cap­tions are rec­og­niz­ably Latin al­pha­bet­i­cal, and even <em>al­most</em> Eng­lish words. This demon­strates that even datasets where left­/<wbr>right does­n’t seem to mat­ter, like cat pho­tos, can sur­prise you.<a href="https://www.gwern.net/Faces#fnref18" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn18">18</a></div><div class="sidenote" id="sn20" style="top: 13226px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn20" title="Link to footnote 20" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">I re­gard this as a flaw in Style<span class="smallcaps-auto">GAN</span> &amp; TF in gen­er­al. Com­put­ers are more than fast enough to load &amp; process im­ages asyn­chro­nously us­ing a few worker threads, and work­ing with a di­rec­tory of im­ages (rather than a spe­cial bi­nary for­mat 10–20× larg­er) avoids im­pos­ing se­ri­ous bur­dens on the user &amp; hard dri­ve. Py­Torch <span class="smallcaps-auto">GAN</span>s al­most al­ways avoid this mis­take, and are much more pleas­ant to work with as one can freely mod­ify the dataset be­tween (and even dur­ing) runs.<a href="https://www.gwern.net/Faces#fnref20" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn20">20</a></div><div class="sidenote" id="sn22" style="top: 13489px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn22" title="Link to footnote 22" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This may be why some peo­ple re­port that Style<span class="smallcaps-auto">GAN</span> just crashes for them &amp; they can’t fig­ure out why. They should try chang­ing their dataset <span class="smallcaps-auto">JPG</span> ↔︎ <span class="smallcaps-auto">PNG</span>.<a href="https://www.gwern.net/Faces#fnref22" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn22">22</a></div><div class="sidenote" id="sn24" style="top: 17382px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn24" title="Link to footnote 24" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">I would de­scribe the dis­tinc­tions as: Soft­ware 0.0 was im­per­a­tive pro­gram­ming for ham­mer­ing out clock­work mech­a­nism; Soft­ware 1.0 was de­clar­a­tive pro­gram­ming with spec­i­fi­ca­tion of pol­i­cy; and Soft­ware 2.0 is deep learn­ing by gar­den­ing loss func­tions (with every­thing else, from model arch to which dat­a­points to la­bel ide­ally learned end-to-end). Con­tin­u­ing the the­me, we might say that di­a­logue with mod­els, like <a href="https://www.gwern.net/GPT-3#prompts-as-programming" id="gwern-gpt-3-prompts-as-programming" class="link-local docMetadata has-annotation spawns-popup">“prompt pro­gram­ming”</a>⁠, are “Soft­ware 3.0”…<a href="https://www.gwern.net/Faces#fnref24" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn24">24</a></div><div class="sidenote" id="sn26" style="top: 18177px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn26" title="Link to footnote 26" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Note: If you use a differ­ent com­mand to re­size, check it thor­ough­ly. With Im­ageMag­ick, if you use the <code>^</code> op­er­a­tor like <code>-resize 512x512^</code>, you will not get ex­actly 512×512px im­ages as you need; while if you use the <code>!</code> op­er­a­tor like <code>-resize 512x512!</code>, the im­ages will be ex­actly 512×512px but the as­pect ra­tios will dis­torted to make im­ages fit, and this may con­fuse any­thing you are train­ing by in­tro­duc­ing un­nec­es­sary mean­ing­less dis­tor­tions &amp; will make any gen­er­ated im­ages look bad.<a href="https://www.gwern.net/Faces#fnref26" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn26">26</a></div><div class="sidenote" id="sn28" style="top: 23830px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn28" title="Link to footnote 28" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/www/old.reddit.com/d1f03c07a54ed8ab2baf22b1ae6782640f9964f0.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/ecji6v/removing_blob_artifact_from_stylegan_generations/" data-attribute-title="Removing blob artifact from StyleGAN generations without retraining. Inspired by StyleGAN2 (Original URL: https://old.reddit.com/r/MachineLearning/comments/ecji6v/removing_blob_artifact_from_stylegan_generations/ )">Stas Pod­gorskiy</a> has demon­strated that the <a href="https://www.gwern.net/Faces#stylegan-2" class="link-self identifier-link-up has-content spawns-popup">Style<span class="smallcaps-auto">GAN</span> 2</a> cor­rec­tion can be re­verse-engi­neered and ap­plied back to Style<span class="smallcaps-auto">GAN</span> 1 gen­er­a­tors if nec­es­sary.<a href="https://www.gwern.net/Faces#fnref28" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn28">28</a></div><div class="sidenote" id="sn30" style="top: 25602px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn30" title="Link to footnote 30" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">No mini­batches are used, so this is much slower than nec­es­sary.<a href="https://www.gwern.net/Faces#fnref30" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn30">30</a></div><div class="sidenote" id="sn32" style="top: 32378px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn32" title="Link to footnote 32" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Keep this change in mind if you run into er­rors like <code>ValueError: Cannot feed value of shape (1024,) for Tensor 'G/dlatent_avg/new_value:0', which has shape '(512,)'</code> try­ing to reuse the mod­el. Try in­crease the size of the <code>latent_size</code> and <code>dlatent_size</code> from 512 to 1024 in the <code>networks_stylegan2.py</code> con­fig.<a href="https://www.gwern.net/Faces#fnref32" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn32">32</a></div><div class="sidenote" id="sn34" style="top: 34690px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn34" title="Link to footnote 34" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">The ques­tion is not whether one is to start with an ini­tial­iza­tion at all, but whether to start with one which does every­thing poor­ly, or one which does a few sim­i­lar things well. Sim­i­lar­ly, from a Bayesian sta­tis­tics per­spec­tive, the ques­tion of what <a href="https://en.wikipedia.org/wiki/Prior_probability" class="docMetadata has-annotation spawns-popup">prior</a> to use is one that every­one faces; how­ev­er, many ap­proaches sweep it un­der the rug and effec­tively as­sume a de­fault flat prior that is con­sis­tently bad and op­ti­mal for no mean­ing­ful prob­lem ever.<a href="https://www.gwern.net/Faces#fnref34" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn34">34</a></div><div class="sidenote" id="sn36" style="top: 36959px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn36" title="Link to footnote 36" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">There are more real Asuka im­ages than Holo to be­gin with, but there is no par­tic­u­lar rea­son for the 10× data aug­men­ta­tion com­pared to the Holo’s 3×—the data aug­men­ta­tions were just done at differ­ent times and hap­pened to have less or more aug­men­ta­tions en­abled.<a href="https://www.gwern.net/Faces#fnref36" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn36">36</a></div><div class="sidenote" id="sn38" style="top: 45084px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn38" title="Link to footnote 38" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">It turns out that this la­tent vec­tor trick <em>does</em> work. In­trigu­ing­ly, it works even bet­ter to do ‘model av­er­ag­ing’ or ‘model blend­ing’ (<a href="https://www.gwern.net/docs/www/www.justinpinkney.com/e28f418fd89a348f1e659c67cec5bb17d92d3894.html" id="pinkney-2020" class="docMetadata localArchive has-annotation spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.justinpinkney.com/stylegan-network-blending/" data-attribute-title="(Original URL: https://www.justinpinkney.com/stylegan-network-blending/ )">“Style<span class="smallcaps-auto">GAN</span> net­work blend­ing”</a>⁠/ <a href="https://arxiv.org/abs/2010.05334" id="pinkney-adler-2020-0" class="docMetadata has-annotation spawns-popup">“Res­o­lu­tion De­pen­dent <span class="smallcaps-auto">GAN</span> In­ter­po­la­tion for Con­trol­lable Im­age Syn­the­sis Be­tween Do­mains”</a>⁠, Pinkney &amp; Adler 2020): re­train model A on dataset B, and then take a weighted av­er­age of the 2 mod­els (you av­er­age them, pa­ra­me­ter by pa­ra­me­ter, and re­mark­ably, that Just Works, or you can swap out lay­ers be­tween mod­el­s), and then you can cre­ate faces which are ar­bi­trar­ily in be­tween A and B. So for ex­am­ple, you can <a href="https://www.youtube.com/watch?v=W-VRdE7zagw" class="has-content spawns-popup">blend <span class="smallcaps-auto">FFHQ</span>/Western-animation faces</a> (<a href="https://colab.research.google.com/drive/1tputbmA9EaXs9HL9iO21g7xN7jz_Xrko" title="&#39;Network blending in StyleGAN: Swapping layers between two models in StyleGAN gives some interesting results. You need a base model and a second model which has been fine-tuned from the base.&#39;, Buntworthy" class="no-popup">Co­lab note­book</a>), <a href="https://nitter.cc/Buntworthy/status/1296208260421361665" title="I just tried my StyleGAN layer swapping method the other way round to what I&#39;d been doing before. So making the ukiyo-e model human (rather than the other way around) and I love the results!" class="no-popup"><em>ukiy­o-e</em>/<span class="smallcaps-auto">FFHQ</span> faces</a>⁠, <a href="https://nitter.cc/arfafax/status/1297681537337446400" title="Combining my cross-model interpolation with @Buntworthy&#39;s layer swapping idea. Here the different resolution layers are being interpolated at different rates between furry, FFHQ, and @KitsuneKey&#39;s foxes. p0 is 4x4 and 8x8, p1 is 16x16 to 128x128, and p2 is 256x256 to 512x512." class="no-popup">furries/foxes/<span class="smallcaps-auto">FFHQ</span> faces</a>⁠, or even <a href="https://nitter.cc/arfafax/status/1296084902928986113" title="Cross-model interpolations are one of those neat hidden features that arise from transfer learning. Here I&#39;m interpolating between 5 StyleGAN2 models: furry, FFHQ, anime, ponies, and @KitsuneKey&#39;s fox model. All were trained off the same base model, which makes blending possible." class="no-popup">furries/foxes/<span class="smallcaps-auto">FFHQ</span>/anime/ponies</a>⁠.<a href="https://www.gwern.net/Faces#fnref38" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn38">38</a></div><div class="sidenote" id="sn40" style="top: 47376px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn40" title="Link to footnote 40" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">There is for other ar­chi­tec­tures like flow-based ones such as Glow, and this is one of their ben­e­fit­s–while the re­quire­ment to be made out of build­ing blocks which <em>can</em> be run back­wards &amp; for­wards equally well, to be ‘in­vert­ible’, is cur­rently ex­tremely ex­pen­sive and the re­sults not com­pet­i­tive ei­ther in fi­nal im­age qual­ity or com­pute re­quire­ments, the in­vert­ibil­ity means that en­cod­ing an ar­bi­trary real im­age to get its in­ferred la­tents Just Works™ and one can eas­ily morph be­tween 2 ar­bi­trary im­ages, or en­code an ar­bi­trary im­age &amp; edit it in the la­tent space to do things like ad­d/<wbr>re­move glasses from a face or cre­ate an op­po­sites-ex ver­sion.<a href="https://www.gwern.net/Faces#fnref40" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn40">40</a></div><div class="sidenote" id="sn42" style="top: 48067px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn42" title="Link to footnote 42" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><span class="smallcaps-auto">SGD</span> is com­mon, but a sec­ond-order al­go­rithm like <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" class="docMetadata has-annotation spawns-popup">Lim­it­ed-mem­ory <span class="smallcaps-auto">BFGS</span></a> is often used in these ap­pli­ca­tions in or­der to run as few it­er­a­tions as pos­si­ble.<a href="https://www.gwern.net/Faces#fnref42" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn42">42</a></div><div class="sidenote" id="sn44" style="top: 48625px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn44" title="Link to footnote 44" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Flow mod­els have other ad­van­tages, mostly stem­ming from the max­i­mum like­li­hood train­ing ob­jec­tive. Since the im­age can be prop­a­gated back­wards and for­wards loss­less­ly, in­stead of be­ing lim­ited to gen­er­at­ing ran­dom sam­ples like a <span class="smallcaps-auto">GAN</span>, it’s pos­si­ble to cal­cu­late the ex­act prob­a­bil­ity of an im­age, en­abling max­i­mum like­li­hood as a loss to op­ti­mize, and drop­ping the Dis­crim­i­na­tor en­tire­ly. With no <span class="smallcaps-auto">GAN</span> dy­nam­ics, there’s no worry about weird train­ing dy­nam­ics, and the like­li­hood loss also for­bids ‘mode drop­ping’: the flow model can’t sim­ply con­spire with a Dis­crim­i­na­tor to for­get pos­si­ble im­ages.<a href="https://www.gwern.net/Faces#fnref44" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn44">44</a></div><div class="sidenote" id="sn46" style="top: 51503px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn46" title="Link to footnote 46" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Back­up-backup mir­ror: <code>rsync rsync://78.46.86.149:873/biggan/2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl.xz ./</code><a href="https://www.gwern.net/Faces#fnref46" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn46">46</a></div></div><div id="sidenote-column-right" class="footnotes" style=""><div class="sidenote" id="sn1" style="top: 1792px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn1" title="Link to footnote 1" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source cyxy-trs-source-ted">Turns out that when train­ing goes <em>re­ally</em> wrong, you can crash many <span class="smallcaps-auto">GAN</span> im­ple­men­ta­tions with ei­ther a seg­fault, in­te­ger over­flow, or di­vi­sion by zero er­ror.<a href="https://www.gwern.net/Faces#fnref1" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p><p class="cyxy-trs-source cyxy-trs-target" contenteditable="true">事实证明，当培训出现真正的错误时，您可以使许多 GAN 实现崩溃，这些实现可能是 segfault、整数溢出或零错误除法。Something</p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn1">1</a></div><div class="sidenote" id="sn3" style="top: 2142px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn3" title="Link to footnote 3" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Be sure to check out <a href="https://artbreeder.com/" id="simon-2019" class="docMetadata has-annotation spawns-popup">Art­breeder</a>⁠.<a href="https://www.gwern.net/Faces#fnref3" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn3">3</a></div><div class="sidenote" id="sn5" style="top: 2492px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn5" title="Link to footnote 5" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://www.gwern.net/docs/anime/2015-saito.pdf" id="masaki-matsui-2015" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;Illustration2Vec: A Semantic Vector Representation of Illustrations&#39;, Saito &amp; Matsui"><code>illustration2vec</code></a> is an old &amp; small <span class="smallcaps-auto">CNN</span> trained to pre­dict a few -booru tags on anime im­ages, and so pro­vides an em­bed­ding—but not a good one. The lack of a good em­bed­ding is the ma­jor lim­i­ta­tion for anime deep learn­ing as of Feb­ru­ary 2019. (<a href="https://www.gwern.net/docs/www/old.reddit.com/584c6c362de1ea72bc8962de25d364f366b0a301.html" class="localArchive has-content spawns-popup" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/" data-attribute-title="(Original URL: https://old.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/ )">Deep­Dan­booru</a>⁠, while per­form­ing well ap­par­ent­ly, has not yet been used for em­bed­dings.) An em­bed­ding is nec­es­sary for text → im­age <span class="smallcaps-auto">GAN</span>s, im­age searches &amp; near­est-neigh­bor checks of over­fit­ting, <span class="smallcaps-auto">FID</span> er­rors for ob­jec­tively com­par­ing <span class="smallcaps-auto">GAN</span>s, mini­batch dis­crim­i­na­tion to help the D/<wbr>pro­vide an aux­il­iary loss to sta­bi­lize learn­ing, anime style trans­fer (both for its own sake &amp; for cre­at­ing a ‘StyleDan­booru2018’ to re­duce tex­ture cheat­ing), en­cod­ing into <span class="smallcaps-auto">GAN</span> la­tent spaces for ma­nip­u­la­tion, data clean­ing (to de­tect anom­alous dat­a­points like failed face crop­s), per­cep­tual losses for en­coders or as an ad­di­tional aux­il­iary loss/<wbr>pre­train­ing (like <a href="https://www.gwern.net/docs/www/www.fast.ai/51f7b97683992e9c1476a63377ea01b9648f9db0.html" id="antic-2019" class="docMetadata localArchive has-annotation spawns-popup" rel="archived alternate nofollow" data-url-original="https://www.fast.ai/2019/05/03/decrappify/" data-attribute-title="Decrappification, DeOldification, and Super Resolution (Original URL: https://www.fast.ai/2019/05/03/decrappify/ )">“No<span class="smallcaps-auto">GAN</span>”</a>⁠, which trains a Gen­er­a­tor on a per­cep­tual loss and does <span class="smallcaps-auto">GAN</span> train­ing only for fine­tun­ing) etc. A good tag­ger is also a good start­ing point for do­ing pix­el-level se­man­tic seg­men­ta­tion (via “weak su­per­vi­sion”), which meta­data is key for train­ing some­thing like Nvidi­a’s <a href="https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/" class="no-popup">Gau<span class="smallcaps-auto">GAN</span></a> suc­ces­sor to pix2pix (<a href="https://arxiv.org/abs/1903.07291" id="park-et-al-2019-3" class="docMetadata has-annotation spawns-popup" data-attribute-title="Semantic Image Synthesis with Spatially-Adaptive Normalization">Park&nbsp;et&nbsp;al&nbsp;2019</a>⁠; <a href="https://github.com/NVlabs/SPADE" class="no-popup">source</a>).<a href="https://www.gwern.net/Faces#fnref5" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn5">5</a></div><div class="sidenote" id="sn7" style="top: 4028px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn7" title="Link to footnote 7" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://arxiv.org/abs/1904.06991" id="kynkäänniemi-et-al-2019-4" class="docMetadata has-annotation spawns-popup" data-attribute-title="Improved Precision and Recall Metric for Assessing Generative Models">Kynkään­niemi&nbsp;et&nbsp;al&nbsp;2019</a> ob­serves (§4 “Us­ing pre­ci­sion and re­call to an­a­lyze and im­prove Style<span class="smallcaps-auto">GAN</span>”) that Style<span class="smallcaps-auto">GAN</span> with pro­gres­sive grow­ing dis­abled <em>does</em> work but at some cost to pre­ci­sion/<wbr>re­call qual­ity met­rics; whether this re­flects in­fe­rior per­for­mance on a given train­ing bud­get or an in­her­ent limit—Big<span class="smallcaps-auto">GAN</span> and other self­-at­ten­tion-us­ing <span class="smallcaps-auto">GAN</span>s do not use pro­gres­sive grow­ing at all, sug­gest­ing it is not <em>truly</em> nec­es­sary—is not in­ves­ti­gat­ed. In De­cem­ber 2019, Style<span class="smallcaps-auto">GAN</span> 2 suc­cess­fully dropped pro­gres­sive grow­ing en­tirely at mod­est per­for­mance cost.<a href="https://www.gwern.net/Faces#fnref7" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn7">7</a></div><div class="sidenote" id="sn9" style="top: 5763px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn9" title="Link to footnote 9" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Google Co­lab is a free ser­vice in­cludes free <span class="smallcaps-auto">GPU</span> time (up to 12 hours on a small <span class="smallcaps-auto">GPU</span>). Es­pe­cially for peo­ple who do not have a rea­son­ably ca­pa­ble <span class="smallcaps-auto">GPU</span> on their per­sonal com­put­ers (such as all Ap­ple users) or do not want to en­gage in the ad­mit­ted has­sle of rent­ing a real cloud <span class="smallcaps-auto">GPU</span> in­stance, Co­lab can be a great way to play with a pre­trained mod­el, like gen­er­at­ing <span class="smallcaps-auto">GPT-2-117M</span> text com­ple­tions or Style<span class="smallcaps-auto">GAN</span> in­ter­po­la­tion videos, or pro­to­type on tiny prob­lems.</p>
<p class="cyxy-trs-source">How­ev­er, it is a bad idea to try to train real mod­els, like 512–1024px Style<span class="smallcaps-auto">GAN</span>s, on a Co­lab in­stance as the <span class="smallcaps-auto">GPU</span>s are low <span class="smallcaps-auto">VRAM</span>, far slower (6 hours per Style<span class="smallcaps-auto">GAN</span> tick­!), un­wieldy to work with (as one must save snap­shots con­stantly to restart when the ses­sion runs out), does­n’t have a real com­mand-line, etc. Co­lab is just barely ad­e­quate for per­haps 1 or 2 ticks of trans­fer learn­ing, but not more. If you har­bor greater am­bi­tions but still refuse to spend any money (rather than time), Kag­gle has a sim­i­lar ser­vice with P100 <span class="smallcaps-auto">GPU</span> slices rather than K80s. Oth­er­wise, one needs to get ac­cess to real <span class="smallcaps-auto">GPU</span>s.<a href="https://www.gwern.net/Faces#fnref9" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn9">9</a></div><div class="sidenote" id="sn11" style="top: 6437px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn11" title="Link to footnote 11" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">The Pro<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span> code­base <a href="https://nitter.cc/davidstap/status/1120667403837423616" class="no-popup">re­port­edly does work</a> with con­di­tion­ing, but none of the pa­pers re­port on this func­tion­al­ity and I have not used it my­self.<a href="https://www.gwern.net/Faces#fnref11" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn11">11</a></div><div class="sidenote" id="sn13" style="top: 6600px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn13" title="Link to footnote 13" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Which raises the ques­tion: if you added any or all of those fea­tures, would Style<span class="smallcaps-auto">GAN</span> be­come that much bet­ter? Un­for­tu­nate­ly, while the­o­rists &amp; prac­ti­tion­ers have had many ideas, so far the­ory has proven more fe­cund than fa­tidi­cal and the large-s­cale <span class="smallcaps-auto">GAN</span> ex­per­i­ments nec­es­sary to truly test the sug­ges­tions are too ex­pen­sive for most. Half of these sug­ges­tions are great ideas—but which half?<a href="https://www.gwern.net/Faces#fnref13" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn13">13</a></div><div class="sidenote" id="sn15" style="top: 8772px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn15" title="Link to footnote 15" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This ob­ser­va­tions ap­ply only to the Gen­er­a­tor in <span class="smallcaps-auto">GAN</span>s (which is what we pri­mar­ily care about); cu­ri­ous­ly, there’s some rea­son to think that <span class="smallcaps-auto">GAN</span> Dis­crim­i­na­tors are in fact mostly mem­o­riz­ing (<a href="https://www.gwern.net/Faces#discriminator-ranking" class="link-self identifier-link-up has-content spawns-popup">see later</a>).<a href="https://www.gwern.net/Faces#fnref15" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn15">15</a></div><div class="sidenote" id="sn17" style="top: 12418px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn17" title="Link to footnote 17" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Based on eye­balling the ‘cat’ bar graph in Fig­ure 3 of <a href="https://arxiv.org/abs/1506.03365" id="yu-et-al-2015-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop">Yu&nbsp;et&nbsp;al&nbsp;2015</a>⁠.<a href="https://www.gwern.net/Faces#fnref17" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn17">17</a></div><div class="sidenote" id="sn19" style="top: 12561px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn19" title="Link to footnote 19" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">I es­ti­mated the to­tal cost us­ing <span class="smallcaps-auto">AWS</span> EC2 pre­emptible hourly costs on 2019-03-15 as fol­lows:</p>
<ul>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">1 <span class="smallcaps-auto cyxy-trs-source">GPU</span>: <code>p2.xlarge</code> in­stance in <code>us-east-2a</code>, Half of a K80 (12GB <span class="smallcaps-auto cyxy-trs-source">VRAM</span>): $0.3235/<wbr>hour</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">2 <span class="smallcaps-auto cyxy-trs-source">GPU</span>s: NA—there is no <a href="https://aws.amazon.com/ec2/instance-types/p2/" class="no-popup cyxy-trs-source">P2 in­stance</a> with 2 <span class="smallcaps-auto cyxy-trs-source">GPU</span>s, only 1/<wbr>8/<wbr>16</li>
<li class="cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source cyxy-trs-source">8 <span class="smallcaps-auto cyxy-trs-source">GPU</span>s: <code>p2.8xlarge</code> in <code>us-east-2a</code>, 8 halves of K80s (12GB <span class="smallcaps-auto cyxy-trs-source">VRAM</span> each): $2.160/<wbr>hour</li>
</ul>
<p class="cyxy-trs-source">As usu­al, there is sub­lin­ear scal­ing, and larger in­stances cost dis­pro­por­tion­ately more, be­cause one is pay­ing for faster wall­clock train­ing (time is valu­able) and for not hav­ing to cre­ate a dis­trib­uted in­fra­struc­ture which can ex­ploit the cheap single-<span class="smallcaps-auto">GPU</span> in­stances.</p>
<p class="cyxy-trs-source">This cost es­ti­mate does not count ad­di­tional costs like hard drive space. In ad­di­tion to the dataset size (the Style<span class="smallcaps-auto">GAN</span> data en­cod­ing is ~18× larger than the raw data size, so a 10GB folder of im­ages → 200GB of <code>.tfrecords</code>), you would need at least 100GB <span class="smallcaps-auto">HDD</span> (50GB for the OS, and 50GB for check­points/<wbr>im­ages/<wbr>etc to avoid crashes from run­ning out of space).<a href="https://www.gwern.net/Faces#fnref19" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn19">19</a></div><div class="sidenote" id="sn21" style="top: 13411px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn21" title="Link to footnote 21" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">For ex­am­ple, my Dan­booru2018 anime por­trait dataset is 16GB, but the Style<span class="smallcaps-auto">GAN</span> en­coded dataset is 296GB.<a href="https://www.gwern.net/Faces#fnref21" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn21">21</a></div><div class="sidenote hidden" id="sn23"><div class="sidenote-outer-wrapper"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn23" title="Link to footnote 23" class="footnote-self-link no-popup">&nbsp;</a><p>That is, in train­ing G, the G’s fake im­ages must be aug­mented be­fore be­ing passed to the D for rat­ing; and in train­ing D, both real &amp; fake im­ages must be aug­mented the same way be­fore be­ing passed to D. Pre­vi­ous­ly, all <span class="smallcaps-auto">GAN</span> re­searchers ap­pear to have as­sumed that one should only aug­ment real im­ages be­fore pass­ing to D dur­ing D train­ing, which con­ve­niently can be done at dataset cre­ation; un­for­tu­nate­ly, this hid­den as­sump­tion turns out to be about the most harm­ful way pos­si­ble!<a href="https://www.gwern.net/Faces#fnref23" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn23">23</a></div><div class="sidenote" id="sn25" style="top: 17767px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn25" title="Link to footnote 25" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">But you may not want to–re­mem­ber the lol­cat cap­tions!<a href="https://www.gwern.net/Faces#fnref25" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn25">25</a></div><div class="sidenote" id="sn27" style="top: 19607px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn27" title="Link to footnote 27" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">If you are us­ing Python 2, you will get <code>print</code> syn­tax er­ror mes­sages; if you are us­ing Python 3–3.6, you will get ‘type hint’ er­rors.<a href="https://www.gwern.net/Faces#fnref27" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn27">27</a></div><div class="sidenote" id="sn29" style="top: 25275px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn29" title="Link to footnote 29" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This makes it con­form to a trun­cated nor­mal dis­tri­b­u­tion; why trun­cated rather than rec­ti­fied/<wbr>win­sorized at a max like 0.5 or 1.0 in­stead? Be­cause then many, pos­si­bly most, of the la­tent vari­ables would all be at the max, in­stead of smoothly spread out over the per­mit­ted range.<a href="https://www.gwern.net/Faces#fnref29" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn29">29</a></div><div class="sidenote" id="sn31" style="top: 29915px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn31" title="Link to footnote 31" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Sim­ply en­cod­ing each pos­si­ble tag as a one-hot cat­e­gor­i­cal vari­able would scale poor­ly: in the worst case, Dan­booru2020 has &gt;434,000 pos­si­ble tags. If that was passed into a ful­ly-con­nected lay­er, which out­put a 1024-long em­bed­ding, then that would use up 434,000 × 1,024 = <em>444 mil­lion</em> pa­ra­me­ters! The em­bed­ding would be larger than the ac­tual Style<span class="smallcaps-auto">GAN</span> mod­el, and ac­cord­ingly ex­pen­sive. <span class="smallcaps-auto">RNN</span>s his­tor­i­cally are com­monly used to con­vert text in­puts to an em­bed­ding for a <span class="smallcaps-auto">CNN</span> to process, but they are finicky and hard to work with. <a href="https://en.wikipedia.org/wiki/Word2vec" class="docMetadata has-annotation spawns-popup">Word2vec</a> dos­n’t work be­cause, as the name sug­gests, it only con­verts a sin­gle tag/<wbr>­word at a time into an em­bed­ding; doc2vec is its equiv­a­lent for se­quences of text. If we were do­ing it in 2021, we would prob­a­bly just throw a Trans­former at it (at­ten­tion is all you need!) with a win­dow of 512 to­kens or some­thing. (You don’t need that many tags, and it’s un­likely that a fea­si­ble model would make good use of ~100 tags any­way.)<a href="https://www.gwern.net/Faces#fnref31" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn31">31</a></div><div class="sidenote" id="sn33" style="top: 33220px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn33" title="Link to footnote 33" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Mov­ing self­-at­ten­tion around in Big<span class="smallcaps-auto">GAN</span> also makes sur­pris­ingly lit­tle differ­ence. We dis­cussed it with Big<span class="smallcaps-auto">GAN</span>’s Brock, and he noted that self­-at­ten­tion was ex­pen­sive &amp; never seemed to be as im­por­tant to Big<span class="smallcaps-auto">GAN</span> as one would as­sume (com­pared to other im­prove­ments like the or­thog­o­nal reg­u­lar­iza­tion, large mod­els, and large mini­batch­es). Given ex­am­ples like <a href="https://openai.com/blog/jukebox/" id="dhariwal-et-al-2020" class="docMetadata has-annotation spawns-popup" data-attribute-title="We&#39;re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We&#39;re releasing the model weights and code, along with a tool to explore the generated samples.">Juke­box</a>⁠, <a href="https://compvis.github.io/taming-transformers/" id="esser-et-al-2020" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;VQGAN: Taming Transformers for High-Resolution Image Synthesis&#39;, Esser et al 2020"><span class="smallcaps-auto">VQGAN</span></a>⁠, <a href="https://arxiv.org/abs/2009.04433" id="han-et-al-2020-9" class="docMetadata has-annotation spawns-popup" data-attribute-title="&#39;not-so-BigGAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution&#39;, Han et al 2020">not-so-BIg<span class="smallcaps-auto">GAN</span></a>⁠, or <a href="https://openai.com/blog/dall-e/" id="ramesh-et-al-2021" class="docMetadata has-annotation spawns-popup"><span class="smallcaps-auto">DALL·E</span></a>⁠, I sus­pect that the ben­e­fits of self­-at­ten­tion may be rel­a­tively min­i­mal at the raw pixel lev­el, and bet­ter fo­cused on the ‘se­man­tic level’ in some sense, such as in pro­cess­ing the la­tent vec­tor or <span class="smallcaps-auto">VQ-VAE</span> to­kens.<a href="https://www.gwern.net/Faces#fnref33" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn33">33</a></div><div class="sidenote" id="sn35" style="top: 34950px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn35" title="Link to footnote 35" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><span class="smallcaps-auto">ADA</span>/Style<span class="smallcaps-auto">GAN</span>3 is re­port­edly much more sam­ple-effi­cient and re­duces the need for trans­fer learn­ing: <a href="https://arxiv.org/abs/2006.06676#nvidia" id="karras-et-al-2020-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="ADA/StyleGAN3: Training Generative Adversarial Networks with Limited Data">Kar­ras&nbsp;et&nbsp;al&nbsp;2020</a>⁠. But if a rel­e­vant model is avail­able, it should still be used. Back­port­ing the <span class="smallcaps-auto">ADA</span> data aug­men­ta­tion trick to Style<span class="smallcaps-auto">GAN</span>1–2 will be a ma­jor up­grade.<a href="https://www.gwern.net/Faces#fnref35" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn35">35</a></div><div class="sidenote" id="sn37" style="top: 42876px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn37" title="Link to footnote 37" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">A fa­mous ex­am­ple is char­ac­ter de­signer <a href="https://www.gwern.net/images/eva/1996-sadamoto-howtodrawshinjinadia.jpg" class="invertible-auto invertible has-content spawns-popup" data-image-height="488" data-image-width="492">Yoshiyuki Sadamoto demon­strat­ing how to</a> turn <a href="https://en.wikipedia.org/wiki/List_of_Nadia%3A_The_Secret_of_Blue_Water_characters#Nadia" class="docMetadata has-annotation spawns-popup">Na­dia</a> (<a href="https://en.wikipedia.org/wiki/Nadia%3A_The_Secret_of_Blue_Water" class="docMetadata has-annotation spawns-popup"><em>Na­dia: The Se­cret of Blue Wa­ter</em></a>) into <a href="https://en.wikipedia.org/wiki/Shinji_Ikari" class="docMetadata has-annotation spawns-popup">Shinji Ikari</a> (<em>Evan­ge­lion</em>).<a href="https://www.gwern.net/Faces#fnref37" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn37">37</a></div><div class="sidenote" id="sn39" style="top: 46493px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn39" title="Link to footnote 39" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">In ret­ro­spect, this should­n’t’ve sur­prised me.<a href="https://www.gwern.net/Faces#fnref39" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn39">39</a></div><div class="sidenote" id="sn41" style="top: 48127px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn41" title="Link to footnote 41" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">This fi­nal ap­proach is, in­ter­est­ing­ly, the his­tor­i­cal rea­son back­prop­a­ga­tion was in­vent­ed: it cor­re­sponds to <em>plan­ning in a model</em>. For ex­am­ple, in plan­ning the flight path of an air­plane (<a href="https://www.gwern.net/docs/statistics/decision/1960-kelley.pdf" id="kelley-1960" class="docMetadata has-annotation spawns-popup" data-attribute-title="Gradient Theory of Optimal Flight Paths">Kel­ley 1960</a>⁠/ <a href="https://www.gwern.net/docs/ai/1962-bryson.pdf" id="bryson-denham-1962" class="docMetadata has-annotation spawns-popup" data-attribute-title="A Steepest-Ascent Method for Solving Optimum Programming Problems">Bryson &amp; Den­ham 1962</a>): the des­ti­na­tion or ‘out­put’ is fixed, the aero­dy­nam­ic­s+­geog­ra­phy or ‘model pa­ra­me­ters’ are also fixed, and the ques­tion is what ac­tions de­ter­min­ing a flight path will re­duce the loss func­tion of time or fuel spent. One starts with a ran­dom set of ac­tions pick­ing a ran­dom flight path, runs it for­ward through the en­vi­ron­ment mod­el, gets a fi­nal time/<wbr>­fuel spent, and then back­prop­a­gates through the model to get the gra­di­ents for the flight path, ad­just­ing the flight path to­wards a new set of ac­tions which will slightly re­duce the time/<wbr>­fuel spent; the new ac­tions are used to plan out the flight to get a new loss, and so on, un­til a lo­cal min­i­mum of the ac­tions has been found. This works with non-s­to­chas­tic prob­lems; for sto­chas­tic ones where the path can’t be guar­an­teed to be ex­e­cut­ed, “mod­el-pre­dic­tive con­trol” can be used to re­plan at every step and ex­e­cute ad­just­ments as nec­es­sary. An­other in­ter­est­ing use of back­prop­a­ga­tion for out­puts is <a href="https://arxiv.org/abs/1906.06565" id="zhang-et-al-2019-6" class="docMetadata has-annotation spawns-popup" data-attribute-title="Deep Set Prediction Networks">Zhang&nbsp;et&nbsp;al&nbsp;2019</a> which tack­les the long-s­tand­ing prob­lem of how to get NNs to out­put <em>sets</em> rather than list out­puts by gen­er­at­ing a pos­si­ble set out­put &amp; re­fin­ing it via back­prop­a­ga­tion.<a href="https://www.gwern.net/Faces#fnref41" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn41">41</a></div><div class="sidenote" id="sn43" style="top: 48879.3px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn43" title="Link to footnote 43" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source"><a href="https://arxiv.org/abs/1907.07171" id="jahanian-et-al-2019-7" class="docMetadata has-annotation spawns-popup" data-attribute-title="On the &#39;steerability&#39; of generative adversarial networks">Ja­han­ian&nbsp;et&nbsp;al&nbsp;2019</a> shows that Big<span class="smallcaps-auto">GAN</span>/Style<span class="smallcaps-auto">GAN</span> la­tent em­bed­dings can also go be­yond what one might ex­pect, to in­clude zooms, trans­la­tions, and other trans­forms.<a href="https://www.gwern.net/Faces#fnref43" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn43">43</a></div><div class="sidenote" id="sn45" style="top: 51380px;"><div class="sidenote-outer-wrapper" style="max-height: 600px;"><div class="sidenote-inner-wrapper"><a href="https://www.gwern.net/Faces#fn45" title="Link to footnote 45" class="footnote-self-link no-popup">&nbsp;</a><p class="cyxy-trs-source">Style<span class="smallcaps-auto">GAN</span> 2 is more com­pu­ta­tion­ally ex­pen­sive but Kar­ras et al op­ti­mized the code­base to make up for it, keep­ing to­tal com­pute con­stant.<a href="https://www.gwern.net/Faces#fnref45" class="footnote-back spawns-popup" role="doc-backlink">↩︎</a></p></div></div><a class="sidenote-self-link no-popup" href="https://www.gwern.net/Faces#sn45">45</a></div></div></div>

        <div id="navigation">
          <a id="navigation-previous" rel="prev" href="https://www.gwern.net/Traffic" title="Link to previous page by topic, ‘/Traffic’">​<svg><use href="/static/img/icons/sequential-nav-icons-arabesque.svg#direction"></use></svg></a><a id="navigation-center" title="Return to top of current page" href="https://www.gwern.net/Faces#top"><svg><use href="/static/img/icons/sequential-nav-icons-arabesque.svg#center"></use></svg></a><a id="navigation-next" rel="next" href="https://www.gwern.net/AB-testing" title="Link to next page by topic, ‘/AB-testing’"><svg><use href="/static/img/icons/sequential-nav-icons-arabesque.svg#direction"></use></svg></a>
        </div>

        <div id="footer">
          <p class="cyxy-trs-source"><a href="https://docs.google.com/forms/d/17cNf0ohMHKeTwmUM_V_hDVkirjsf8HLMWsJco8gt2Eg/viewform" title="Google Docs web form for submitting anonymized feedback to Gwern Branwen"><em>Submit Anonymous Feedback</em></a></p>
        </div>

      </article>
    </main>

        <!-- Load the rest of the CSS. Note that we do not need any JS hacks here: HTML allows <link> stylesheets in the body: https://html.spec.whatwg.org/multipage/links.html#body-ok https://jakearchibald.com/2016/link-in-body/ -->
    <link rel="stylesheet" type="text/css" href="./一个StyleGAN动漫脸编辑详细教程_files/fonts.css">
    <link rel="stylesheet" type="text/css" href="./一个StyleGAN动漫脸编辑详细教程_files/default.css">
    <link rel="stylesheet" type="text/css" href="./一个StyleGAN动漫脸编辑详细教程_files/links.css">

	<!-- JS library for spawning popups -->
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/popups.js.下载" defer=""></script>
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/popins.js.下载" defer=""></script>
    <!-- JS library for annotating hyperlinks with introduction/summary from various sources; not lazy-loaded as (almost) all pages have popups -->
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/annotations.js.下载" defer=""></script>
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/extracts.js.下载" defer=""></script>
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/extracts-options.js.下载" defer=""></script>

    <!-- Stuff we don’t need on /index gets conditionally loaded: -->
    

    <!-- JS library for clickable/zoomable images, for images larger than can be displayed in -->
    <!-- the body: http://share.obormot.net/misc/gwern/image-focus.js http://share.obormot.net/misc/gwern/image-focus.css -->
    <!-- not lazy-loaded due to various bugs -->
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/image-focus.js.下载" defer=""></script>

    <!-- sort tables: http://tablesorter.com/docs/ ; requires JQuery, which has been prepended to avoid multiple JS page loads & race conditions -->
	<script>
	if (document.querySelector("table")) {
		var js = document.createElement("script");
		js.type = "text/javascript";
		js.src = "/static/js/tablesorter.js?v=1604173830";
		document.body.appendChild(js);
	}
	</script><script type="text/javascript" src="./一个StyleGAN动漫脸编辑详细教程_files/tablesorter.js.下载"></script>

    

    <!-- Misc JS: 1. Collapsed sections: insert a toggle to expand them; 2. cover source-code block listings too (for large R sections especially); 3. type setting stuff -->
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/typography.js.下载" defer=""></script>
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/rewrite.js.下载" defer=""></script>
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/collapse.js.下载" defer=""></script>

    <!-- Dark Mode theme switcher (lets users toggle between regular black-on-white CSS, and dark mode) -->
    <script src="./一个StyleGAN动漫脸编辑详细教程_files/darkmode.js.下载" defer=""></script>


    

    <script defer="" id="googleAnalytics" src="./一个StyleGAN动漫脸编辑详细教程_files/js" onload="GW.googleAnalyticsLoaded = true;"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-18912926-1');
    </script>

    <!-- Lazy load Disqus comments: define a function, check for IntersectionObserver support, & load once user hits sections at bottom of pages; lazy load the tablesorter, as tables are on only a very small subset of pages -->
    <div id="comments">
      <div id="disqus_thread"></div>
      <script>
        function dynamicallyLoadScript(url) {
            var script = document.createElement("script");
            script.src = url;
            document.head.appendChild(script);
        }

       var lazyLoadObserver = function(f, selectors) {
           if (('IntersectionObserver' in window) &&
               ('IntersectionObserverEntry' in window) &&
               ('intersectionRatio' in window.IntersectionObserverEntry.prototype)) {
                var intersectionObserver = new IntersectionObserver(function(entries) {
                     if (entries[0].intersectionRatio <= 0) return;

                     f();
               }, { rootMargin: '140%' } );

               var target = document.querySelector(selectors);
               if (!(target === null)) { intersectionObserver.observe(target); }
           }
           else { f(); }
       }

        var disqus_shortname = ''; // declare variable globally as a way to make sure insertDisqus runs once and only once
        var insertDisqus = function() {
        if (disqus_shortname == '') {
           disqus_shortname = 'gwern';
           (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
           })();
        }
       }
      // 1. Load Disqus comments
      lazyLoadObserver(insertDisqus, '#disqus_thread');
      </script>

      <noscript><p>Enable JavaScript for Disqus comments</p></noscript>
    </div>
    

  

<div id="ui-elements-container"><div id="back-to-top" class="hidden"><a href="https://www.gwern.net/Faces#top" tabindex="-1" title="Back to top"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M6.1 422.3l209.4-209.4c4.7-4.7 12.3-4.7 17 0l209.4 209.4c4.7 4.7 4.7 12.3 0 17l-19.8 19.8c-4.7 4.7-12.3 4.7-17 0L224 278.4 42.9 459.1c-4.7 4.7-12.3 4.7-17 0L6.1 439.3c-4.7-4.7-4.7-12.3 0-17zm0-143l19.8 19.8c4.7 4.7 12.3 4.7 17 0L224 118.4l181.1 180.7c4.7 4.7 12.3 4.7 17 0l19.8-19.8c4.7-4.7 4.7-12.3 0-17L232.5 52.9c-4.7-4.7-12.3-4.7-17 0L6.1 262.3c-4.7 4.7-4.7 12.3 0 17z"></path></svg></a></div><div id="image-focus-overlay"><div class="help-overlay hidden">
		<p class="slideshow-help-text"><strong>Arrow keys:</strong> Next/previous image</p>
		<p><strong>Escape</strong> or <strong>click</strong>: Hide zoomed image</p>
		<p><strong>Space bar:</strong> Reset image size &amp; position</p>
		<p><strong>Scroll</strong> to zoom in/out</p>
		<p>(When zoomed in, <strong>drag</strong> to pan; <br><strong>double-click</strong> to close)</p>
	</div>
	<div class="image-number hidden" data-number-of-images="44"></div>
	<div class="slideshow-buttons">
		<button type="button" class="slideshow-button previous hidden" tabindex="-1" title="Previous image">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512">
				<path d="M34.52 239.03L228.87 44.69c9.37-9.37 24.57-9.37 33.94 0l22.67 22.67c9.36 9.36 9.37 24.52.04 33.9L131.49 256l154.02 154.75c9.34 9.38 9.32 24.54-.04 33.9l-22.67 22.67c-9.37 9.37-24.57 9.37-33.94 0L34.52 272.97c-9.37-9.37-9.37-24.57 0-33.94z"></path>
			</svg>
		</button>
		<button type="button" class="slideshow-button next hidden" tabindex="-1" title="Next image">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512">
				<path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"></path>
			</svg>
		</button>
	</div>
	<div class="caption hidden"></div></div><div id="mode-selector"><button type="button" class="select-mode-auto selected cyxy-trs-source" tabindex="-1" data-name="auto" title="Set light or dark mode automatically, according to system-wide setting (Win: Start → Personalization → Colors; Mac: Apple → System-Preferences → General → Appearance; iOS: Settings → Display-and-Brightness; Android: Settings → Display)" disabled="">Auto<font class="cyxy-trs-target"> 自动</font></button><button type="button" class="select-mode-light active cyxy-trs-source" tabindex="-1" data-name="light" title="Light mode at all times (black-on-white)">Light<font class="cyxy-trs-target"> 灯光</font></button><button type="button" class="select-mode-dark cyxy-trs-source" tabindex="-1" data-name="dark" title="Dark mode at all times (inverted: white-on-black)">Dark<font class="cyxy-trs-target"> 黑暗</font></button></div></div><div id="annotations-workspace" style="display:none;"></div><div class="cyxy-target-popup cyxy-trs-source" style="display: none;"><div style="margin: auto"><img id="cyxy-popup-left-slide" src="./一个StyleGAN动漫脸编辑详细教程_files/left-slide.png" style="display: none;"> <div id="cyxy-popup-userinfo"><img id="cyxy-popup-avatar" src="./一个StyleGAN动漫脸编辑详细教程_files/xiaoyilogo.jpg"> <div id="cyxy-popup-name-time"> <span id="cyxy-popup-name">彩云小译</span> <span id="cyxy-popup-time">刚刚</span></div></div><div id="cyxy-popup-favour"><img id="cyxy-popup-favour-img" src="./一个StyleGAN动漫脸编辑详细教程_files/favour.png"><span id="cyxy-popup-favour-num">0</span></div><img id="cyxy-popup-right-slide" src="./一个StyleGAN动漫脸编辑详细教程_files/right-slide.png" style="display: none;"></div></div><div class="cyxy-personal cyxy-trs-source"><a href="https://fanyi.caiyunapp.com/user/center/" target="_blank"><img class="cyxy-favorite-btn" src="./一个StyleGAN动漫脸编辑详细教程_files/g"></a></div><div class="cyxy-function cyxy-trs-source"><img class="cyxy-favorite-btn" src="./一个StyleGAN动漫脸编辑详细教程_files/fanyi-btn-hover.png"></div><div class="cyxy-favorite"><img class="cyxy-favorite-btn" id="cyxyFavoriteBtn" src="./一个StyleGAN动漫脸编辑详细教程_files/favorite-btn.png"></div><div class="cyxy-footer cyxy-trs-source" style="opacity: 0.88; display: none;"><div class="cyxy-footer-p">本网页由彩云小译<font id="cyxy-footer-translator"></font>翻译<div></div></div></div><iframe src="./一个StyleGAN动漫脸编辑详细教程_files/web_translate_data_stat.html" style="display: none;"></iframe><div class="cyxy-video-trans" style="filter: none;"><img class="cyxy-video-trans-btn" src="chrome-extension://jmpepeebcbihafjjadogphmbgiffiajh/images/fanyi-btn-subtitle.png"></div><div id="substr" class="cyxy-substr cyxy-no-trs" title="重复点缩小小译字幕" style="position: fixed; left: 48px; bottom: 10px; height: 11rem; width: 90%; padding-left: 20px; padding-right: 20px; cursor: grab; z-index: 300000; background: rgba(0, 0, 0, 0.8); color: white; transform: scale(1);"><div style="margin-top: 12px; width: 100%;"><img id="logo" src="chrome-extension://jmpepeebcbihafjjadogphmbgiffiajh/images/logo.png" style="border-radius: 50%; float: left; width: 21px !important; height: 21px !important; min-width: unset !important;"><a id="cy_link" class="menu_ele" style="font-family: PingFangSC-Regular; font-size: 14px; float: left; margin-left: 10px; color: white;">彩云小译</a><div id="vh_div" class="menu_ele" style="margin-right: 10px; margin-bottom: 10px; margin-left: 10px; display: inline; margin-top: 4px !important;"><hr style="border-top: none; border-right: none; border-bottom: none; border-image: initial; border-left: 2px solid rgb(255, 255, 255); height: 13px; width: 2px; color: rgb(255, 255, 255); float: left; margin-left: 14px; margin-top: 4px; display: inline; margin-bottom: -10px;"></div><div id="lang_menu" style="margin-left: -5px; display: inline;"><div id="lang_radio" style="display: inline;"><input id="zh_radio" type="radio" name="lang" value="zh" class="lang_radio"><img class="menu_ele lang_radio_img" id="zh_radio_img" src="chrome-extension://jmpepeebcbihafjjadogphmbgiffiajh/images/grey_radio.png" style="filter: brightness(0) invert(1);"><label id="zh_label" class="menu_ele" style="font-family: PingFangSC-Regular; font-size: 14px; color: rgb(255, 255, 255); text-align: right; margin-right: 12px;"> 中 &gt;&gt; 英</label><input id="en_radio" type="radio" name="lang" value="en" class="lang_radio"><img class="menu_ele lang_radio_img" id="en_radio_img" src="chrome-extension://jmpepeebcbihafjjadogphmbgiffiajh/images/green_radio.png" style="filter: none;"><label id="en_label" class="menu_ele" style="font-family: PingFangSC-Regular; font-size: 14px; color: rgb(255, 255, 255); text-align: right; margin-right: 12px;"> 英 &gt;&gt; 中</label><input id="ja-JP_radio" type="radio" name="lang" value="ja-JP" class="lang_radio"><img class="menu_ele lang_radio_img" id="ja-JP_radio_img" src="chrome-extension://jmpepeebcbihafjjadogphmbgiffiajh/images/grey_radio.png" style="filter: brightness(0) invert(1);"><label id="ja-JP_label" class="menu_ele" style="font-family: PingFangSC-Regular; font-size: 14px; color: rgb(255, 255, 255); text-align: right; margin-right: 12px;"> 日 &gt;&gt; 中</label></div><button class="Btn menu_ele" id="toggle_btn" style="background-color: rgba(0, 0, 0, 0); color: rgb(255, 255, 255); border: 1px solid rgb(255, 255, 255); border-radius: 3px; display: inline-block; margin-left: 4px; font-size: 0.8rem; cursor: pointer; height: 1.6rem !important; padding: 0rem 0.3rem !important;">停止</button><button class="menu_ele" id="switch_track_btn" style="background-color: rgba(0, 0, 0, 0); color: rgb(255, 255, 255); border: 1px solid rgb(255, 255, 255); border-radius: 3px; display: inline-block; margin-left: 12px; font-size: 0.8rem; cursor: pointer; height: 1.6rem !important; padding: 0rem 0.3rem !important;" value="bilingual">双语</button></div><button id="rwd_btn" title="隐藏菜单栏" style="display: inline; margin-right: -3px; background-color: rgba(0, 0, 0, 0); color: rgb(255, 255, 255); border: none; height: 2rem; margin-left: 4px; margin-top: -4px; padding: 4px; cursor: pointer; box-shadow: none;">◃</button></div><div class="reclog" style="font-size: 2rem;"><div style="font-size: 2rem; display: block; line-height: 34px;" class="reclogOrg"></div><div style="font-size: 2rem; display: block; line-height: 34px;" class="reclogTgt"></div></div></div></body><div id="popup-container" class="popup-container" style="z-index: 10001;"></div></html>